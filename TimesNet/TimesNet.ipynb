{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyLpndVh_KmV",
        "outputId": "b49bb006-00d1-4cc3-a2f8-37c5811fcba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Time-Series-Library'...\n",
            "remote: Enumerating objects: 2023, done.\u001b[K\n",
            "remote: Counting objects: 100% (1211/1211), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 2023 (delta 1083), reused 1066 (delta 1066), pack-reused 812 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2023/2023), 78.27 MiB | 14.33 MiB/s, done.\n",
            "Resolving deltas: 100% (1406/1406), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/thuml/Time-Series-Library.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15_7u9Xk_gKW",
        "outputId": "62bc19bc-6327-4d5b-8cee-94b899bed97c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Time-Series-Library\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Time-Series-Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hu7VuTFquUhl"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./dataset/weather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWLp6KNiP-7F"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./dataset/electricity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqj6rxnKowmX"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./dataset/exchange_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81s4_LRxqIip"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./dataset/illness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02k25yK5tfN7"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./dataset/traffic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xNL-wEByFa6"
      },
      "outputs": [],
      "source": [
        "# Add the CSV document of data to the corresponding path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE4u2wLLAAbH",
        "outputId": "e609f8c4-8460-4c03-bab9-80be318c52d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/lucidrains/product-key-memory.git\n",
            "  Cloning https://github.com/lucidrains/product-key-memory.git to /tmp/pip-req-build-jrks2b4q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lucidrains/product-key-memory.git /tmp/pip-req-build-jrks2b4q\n",
            "  Resolved https://github.com/lucidrains/product-key-memory.git to commit e96826c8608a8f0fc64a6e5ea41d5ac1b406ab5c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colt5-attention>=0.10.14 (from product_key_memory==0.2.11)\n",
            "  Using cached CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\n",
            "Requirement already satisfied: einops>=0.6 in /usr/local/lib/python3.11/dist-packages (from product_key_memory==0.2.11) (0.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from product_key_memory==0.2.11) (2.6.0+cu124)\n",
            "Collecting local-attention>=1.8.6 (from colt5-attention>=0.10.14->product_key_memory==0.2.11)\n",
            "  Using cached local_attention-1.11.1-py3-none-any.whl.metadata (907 bytes)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from colt5-attention>=0.10.14->product_key_memory==0.2.11) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->product_key_memory==0.2.11) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->product_key_memory==0.2.11) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->product_key_memory==0.2.11) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->product_key_memory==0.2.11) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->product_key_memory==0.2.11) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->product_key_memory==0.2.11)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->product_key_memory==0.2.11)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->product_key_memory==0.2.11)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->product_key_memory==0.2.11)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->product_key_memory==0.2.11)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->product_key_memory==0.2.11)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->product_key_memory==0.2.11)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->product_key_memory==0.2.11)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->product_key_memory==0.2.11)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->product_key_memory==0.2.11) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->product_key_memory==0.2.11) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->product_key_memory==0.2.11) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->product_key_memory==0.2.11)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->product_key_memory==0.2.11) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->product_key_memory==0.2.11) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->product_key_memory==0.2.11) (1.3.0)\n",
            "Collecting hyper-connections>=0.1.8 (from local-attention>=1.8.6->colt5-attention>=0.10.14->product_key_memory==0.2.11)\n",
            "  Using cached hyper_connections-0.1.15-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->product_key_memory==0.2.11) (3.0.2)\n",
            "Using cached CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading local_attention-1.11.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading hyper_connections-0.1.15-py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: product_key_memory\n",
            "  Building wheel for product_key_memory (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for product_key_memory: filename=product_key_memory-0.2.11-py3-none-any.whl size=6455 sha256=46998b9424dca65b648e2e59b20a672e98337392185b4537882280f93417e38c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4hv26qsi/wheels/87/3f/34/a625fc77ba45644a6a8d8ec41c30c34e15282d6a14f66c7d9a\n",
            "Successfully built product_key_memory\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, hyper-connections, local-attention, colt5-attention, product_key_memory\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed colt5-attention-0.11.1 hyper-connections-0.1.15 local-attention-1.11.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 product_key_memory-0.2.11\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/lucidrains/product-key-memory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcFOleTSAuu4",
        "outputId": "cb532391-3b2f-4249-e560-11229a8b5bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch==1.13.0\n",
            "  Downloading torch-1.13.0-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==1.13.0) (4.13.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.45.1)\n",
            "Downloading torch-1.13.0-cp311-cp311-manylinux1_x86_64.whl (890.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.2/890.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "hyper-connections 0.1.15 requires torch>=2.3, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 1.13.0 which is incompatible.\n",
            "accelerate 1.5.2 requires torch>=2.0.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65bO3kLEpKoP"
      },
      "outputs": [],
      "source": [
        "# To resolve version conflicts, modify requirements.txt, remove the installation of torch and pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dtoxE2hQ_icq",
        "outputId": "c4f2edc7-2103-4134-db0f-f78555be6521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops==0.8.0 (from -r requirements.txt (line 1))\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting local-attention==1.9.14 (from -r requirements.txt (line 2))\n",
            "  Downloading local_attention-1.9.14-py3-none-any.whl.metadata (683 bytes)\n",
            "Collecting matplotlib==3.7.0 (from -r requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 4))\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting patool==1.12 (from -r requirements.txt (line 6))\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting reformer-pytorch==1.4.4 (from -r requirements.txt (line 7))\n",
            "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting scikit-learn==1.2.2 (from -r requirements.txt (line 8))\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.10.1 (from -r requirements.txt (line 9))\n",
            "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sktime==0.16.1 (from -r requirements.txt (line 10))\n",
            "  Downloading sktime-0.16.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting sympy==1.11.1 (from -r requirements.txt (line 11))\n",
            "  Downloading sympy-1.11.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tqdm==4.64.1 (from -r requirements.txt (line 13))\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyWavelets (from -r requirements.txt (line 14))\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from local-attention==1.9.14->-r requirements.txt (line 2)) (1.13.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (2.8.2)\n",
            "Collecting axial-positional-embedding>=0.1.0 (from reformer-pytorch==1.4.4->-r requirements.txt (line 7))\n",
            "  Downloading axial_positional_embedding-0.3.12-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: product-key-memory in /usr/local/lib/python3.11/dist-packages (from reformer-pytorch==1.4.4->-r requirements.txt (line 7)) (0.2.11)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->-r requirements.txt (line 8)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->-r requirements.txt (line 8)) (3.6.0)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from sktime==0.16.1->-r requirements.txt (line 10)) (1.2.18)\n",
            "Collecting pandas<1.6.0,>=1.1.0 (from sktime==0.16.1->-r requirements.txt (line 10))\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy==1.11.1->-r requirements.txt (line 11)) (1.3.0)\n",
            "Collecting torch (from local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->sktime==0.16.1->-r requirements.txt (line 10)) (1.17.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<1.6.0,>=1.1.0->sktime==0.16.1->-r requirements.txt (line 10)) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.0->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->local-attention==1.9.14->-r requirements.txt (line 2)) (3.2.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting triton==3.1.0 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting torch (from local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading torch-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch->local-attention==1.9.14->-r requirements.txt (line 2))\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: colt5-attention>=0.10.14 in /usr/local/lib/python3.11/dist-packages (from product-key-memory->reformer-pytorch==1.4.4->-r requirements.txt (line 7)) (0.11.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->local-attention==1.9.14->-r requirements.txt (line 2)) (3.0.2)\n",
            "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading local_attention-1.9.14-py3-none-any.whl (9.0 kB)\n",
            "Downloading matplotlib-3.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
            "Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sktime-0.16.1-py3-none-any.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading axial_positional_embedding-0.3.12-py3-none-any.whl (6.7 kB)\n",
            "Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool, triton, tqdm, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, einops, scipy, PyWavelets, pandas, nvidia-cusolver-cu12, torch, scikit-learn, matplotlib, sktime, local-attention, axial-positional-embedding, reformer-pytorch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.1\n",
            "    Uninstalling einops-0.8.1:\n",
            "      Successfully uninstalled einops-0.8.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.0\n",
            "    Uninstalling pandas-2.0.0:\n",
            "      Successfully uninstalled pandas-2.0.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0\n",
            "    Uninstalling torch-1.13.0:\n",
            "      Successfully uninstalled torch-1.13.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: local-attention\n",
            "    Found existing installation: local-attention 1.11.1\n",
            "    Uninstalling local-attention-1.11.1:\n",
            "      Successfully uninstalled local-attention-1.11.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.42.0 requires matplotlib>=3.7.1, but you have matplotlib 3.7.0 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "cvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "mizani 0.13.2 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.0 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyWavelets-1.8.0 axial-positional-embedding-0.3.12 einops-0.8.0 local-attention-1.9.14 matplotlib-3.7.0 numpy-1.23.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 pandas-1.5.3 patool-1.12 reformer-pytorch-1.4.4 scikit-learn-1.2.2 scipy-1.10.1 sktime-0.16.1 sympy-1.11.1 torch-2.4.1 tqdm-4.64.1 triton-3.0.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0224243f36094e6f9f06348fcb6e78d8",
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VOrD378skke",
        "outputId": "879a4e3d-6e77-42ed-caeb-38c8e17b2809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas==2.0.0\n",
            "  Using cached pandas-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.0) (2025.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.0) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.0) (1.17.0)\n",
            "Using cached pandas-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sktime 0.16.1 requires pandas<1.6.0,>=1.1.0, but you have pandas 2.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.0 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "bigframes 1.42.0 requires matplotlib>=3.7.1, but you have matplotlib 3.7.0 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 2.0.0 which is incompatible.\n",
            "mizani 0.13.2 requires pandas>=2.2.0, but you have pandas 2.0.0 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.0 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad5sDPSAhD2W",
        "outputId": "07e6bb56-10a9-4282-c812-223ea91eee5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8Zm-Itbkib_5",
        "outputId": "b6686857-967b-4f3c-cd15-adfade602c50"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Time-Series-Library'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdWLuGgyZ8Xd"
      },
      "source": [
        "### 2. TimesNet for Traffic data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB-9VsVy0BC-",
        "outputId": "0ba6e7ec-cc07-4a76-d704-c94b603dd065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           traffic_96_96       Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/traffic/  \n",
            "  Data Path:          traffic.csv         Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            96                  Label Len:          48                  \n",
            "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             862                 Dec In:             862                 \n",
            "  C Out:              862                 d model:            512                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               512                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_traffic_96_96_TimesNet_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 12089\n",
            "val 1661\n",
            "test 3413\n",
            "\titers: 100, epoch: 1 | loss: 0.3998465\n",
            "\tspeed: 0.6281s/iter; left time: 2312.1116s\n",
            "\titers: 200, epoch: 1 | loss: 0.2873707\n",
            "\tspeed: 0.6129s/iter; left time: 2194.6266s\n",
            "\titers: 300, epoch: 1 | loss: 0.2471017\n",
            "\tspeed: 0.6145s/iter; left time: 2139.0164s\n",
            "Epoch: 1 cost time: 233.92263007164001\n",
            "Epoch: 1, Steps: 378 | Train Loss: 0.3616796 Vali Loss: 0.4798301 Test Loss: 0.6137252\n",
            "Validation loss decreased (inf --> 0.479830).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2262584\n",
            "\tspeed: 1.5185s/iter; left time: 5015.4527s\n",
            "\titers: 200, epoch: 2 | loss: 0.2351930\n",
            "\tspeed: 0.6180s/iter; left time: 1979.4535s\n",
            "\titers: 300, epoch: 2 | loss: 0.2303922\n",
            "\tspeed: 0.6178s/iter; left time: 1916.9336s\n",
            "Epoch: 2 cost time: 233.82317876815796\n",
            "Epoch: 2, Steps: 378 | Train Loss: 0.2411839 Vali Loss: 0.4656692 Test Loss: 0.6006053\n",
            "Validation loss decreased (0.479830 --> 0.465669).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.2295238\n",
            "\tspeed: 1.5406s/iter; left time: 4506.3219s\n",
            "\titers: 200, epoch: 3 | loss: 0.2299184\n",
            "\tspeed: 0.6176s/iter; left time: 1744.6843s\n",
            "\titers: 300, epoch: 3 | loss: 0.2231877\n",
            "\tspeed: 0.6177s/iter; left time: 1683.1117s\n",
            "Epoch: 3 cost time: 233.94770765304565\n",
            "Epoch: 3, Steps: 378 | Train Loss: 0.2232737 Vali Loss: 0.4629903 Test Loss: 0.5998943\n",
            "Validation loss decreased (0.465669 --> 0.462990).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.2218765\n",
            "\tspeed: 1.5428s/iter; left time: 3929.4367s\n",
            "\titers: 200, epoch: 4 | loss: 0.2143145\n",
            "\tspeed: 0.6168s/iter; left time: 1509.3452s\n",
            "\titers: 300, epoch: 4 | loss: 0.2307456\n",
            "\tspeed: 0.6173s/iter; left time: 1448.8377s\n",
            "Epoch: 4 cost time: 233.81650757789612\n",
            "Epoch: 4, Steps: 378 | Train Loss: 0.2156901 Vali Loss: 0.4543354 Test Loss: 0.5918268\n",
            "Validation loss decreased (0.462990 --> 0.454335).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.2224984\n",
            "\tspeed: 1.5440s/iter; left time: 3348.9396s\n",
            "\titers: 200, epoch: 5 | loss: 0.2055461\n",
            "\tspeed: 0.6172s/iter; left time: 1276.9117s\n",
            "\titers: 300, epoch: 5 | loss: 0.2019865\n",
            "\tspeed: 0.6171s/iter; left time: 1215.1215s\n",
            "Epoch: 5 cost time: 233.82802844047546\n",
            "Epoch: 5, Steps: 378 | Train Loss: 0.2114457 Vali Loss: 0.4513545 Test Loss: 0.5899158\n",
            "Validation loss decreased (0.454335 --> 0.451355).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.2168064\n",
            "\tspeed: 1.5413s/iter; left time: 2760.4213s\n",
            "\titers: 200, epoch: 6 | loss: 0.2037589\n",
            "\tspeed: 0.6164s/iter; left time: 1042.3523s\n",
            "\titers: 300, epoch: 6 | loss: 0.1891956\n",
            "\tspeed: 0.6164s/iter; left time: 980.6928s\n",
            "Epoch: 6 cost time: 233.53782391548157\n",
            "Epoch: 6, Steps: 378 | Train Loss: 0.2091635 Vali Loss: 0.4491518 Test Loss: 0.5887166\n",
            "Validation loss decreased (0.451355 --> 0.449152).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.1999130\n",
            "\tspeed: 1.5354s/iter; left time: 2169.4525s\n",
            "\titers: 200, epoch: 7 | loss: 0.2229567\n",
            "\tspeed: 0.6162s/iter; left time: 809.1268s\n",
            "\titers: 300, epoch: 7 | loss: 0.2082802\n",
            "\tspeed: 0.6165s/iter; left time: 747.8657s\n",
            "Epoch: 7 cost time: 233.43249773979187\n",
            "Epoch: 7, Steps: 378 | Train Loss: 0.2079191 Vali Loss: 0.4488319 Test Loss: 0.5887920\n",
            "Validation loss decreased (0.449152 --> 0.448832).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 0.2118640\n",
            "\tspeed: 1.5364s/iter; left time: 1590.1273s\n",
            "\titers: 200, epoch: 8 | loss: 0.1909814\n",
            "\tspeed: 0.6160s/iter; left time: 575.9692s\n",
            "\titers: 300, epoch: 8 | loss: 0.2114917\n",
            "\tspeed: 0.6164s/iter; left time: 514.6920s\n",
            "Epoch: 8 cost time: 233.4502124786377\n",
            "Epoch: 8, Steps: 378 | Train Loss: 0.2071896 Vali Loss: 0.4489344 Test Loss: 0.5892323\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 7.8125e-07\n",
            "\titers: 100, epoch: 9 | loss: 0.2218572\n",
            "\tspeed: 1.5179s/iter; left time: 997.2507s\n",
            "\titers: 200, epoch: 9 | loss: 0.2076623\n",
            "\tspeed: 0.6167s/iter; left time: 343.4802s\n",
            "\titers: 300, epoch: 9 | loss: 0.2078961\n",
            "\tspeed: 0.6167s/iter; left time: 281.8488s\n",
            "Epoch: 9 cost time: 233.62260055541992\n",
            "Epoch: 9, Steps: 378 | Train Loss: 0.2068522 Vali Loss: 0.4496692 Test Loss: 0.5895615\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 3.90625e-07\n",
            "\titers: 100, epoch: 10 | loss: 0.2238641\n",
            "\tspeed: 1.5195s/iter; left time: 423.9387s\n",
            "\titers: 200, epoch: 10 | loss: 0.2027675\n",
            "\tspeed: 0.6161s/iter; left time: 110.2867s\n",
            "\titers: 300, epoch: 10 | loss: 0.2083316\n",
            "\tspeed: 0.6158s/iter; left time: 48.6485s\n",
            "Epoch: 10 cost time: 233.3812973499298\n",
            "Epoch: 10, Steps: 378 | Train Loss: 0.2066354 Vali Loss: 0.4497057 Test Loss: 0.5899957\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_traffic_96_96_TimesNet_custom_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 3413\n",
            "test shape: (3413, 96, 862) (3413, 96, 862)\n",
            "test shape: (3413, 96, 862) (3413, 96, 862)\n",
            "mse:0.589649498462677, mae:0.31423404812812805, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "  --task_name long_term_forecast \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/traffic/ \\\n",
        "  --data_path traffic.csv \\\n",
        "  --model_id traffic_96_96 \\\n",
        "  --model $model_name \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --seq_len 96 \\\n",
        "  --label_len 48 \\\n",
        "  --pred_len 96 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 3 \\\n",
        "  --enc_in 862 \\\n",
        "  --dec_in 862 \\\n",
        "  --c_out 862 \\\n",
        "  --d_model 512 \\\n",
        "  --d_ff 512 \\\n",
        "  --top_k 5 \\\n",
        "  --des 'Exp' \\\n",
        "  --itr 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDNjmuifj8Gz",
        "outputId": "ba70b76d-4572-48eb-be3d-5bc1201fc15c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           traffic_720_96      Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/traffic/  \n",
            "  Data Path:          traffic.csv         Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            720                 Label Len:          48                  \n",
            "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             862                 Dec In:             862                 \n",
            "  C Out:              862                 d model:            512                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               512                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_traffic_720_96_TimesNet_custom_ftM_sl720_ll48_pl96_dm512_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 11465\n",
            "val 1661\n",
            "test 3413\n",
            "\titers: 100, epoch: 1 | loss: 0.3378698\n",
            "\tspeed: 2.3545s/iter; left time: 8219.6524s\n",
            "\titers: 200, epoch: 1 | loss: 0.2431625\n",
            "\tspeed: 2.3291s/iter; left time: 7897.9221s\n",
            "\titers: 300, epoch: 1 | loss: 0.2468068\n",
            "\tspeed: 2.3318s/iter; left time: 7673.8931s\n",
            "Epoch: 1 cost time: 838.8247458934784\n",
            "Epoch: 1, Steps: 359 | Train Loss: 0.3319087 Vali Loss: 0.4583628 Test Loss: 0.6278962\n",
            "Validation loss decreased (inf --> 0.458363).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2454628\n",
            "\tspeed: 5.3078s/iter; left time: 16623.9671s\n",
            "\titers: 200, epoch: 2 | loss: 0.2300736\n",
            "\tspeed: 2.3053s/iter; left time: 6989.5227s\n",
            "\titers: 300, epoch: 2 | loss: 0.2164520\n",
            "\tspeed: 2.3045s/iter; left time: 6756.9385s\n",
            "Epoch: 2 cost time: 828.5080015659332\n",
            "Epoch: 2, Steps: 359 | Train Loss: 0.2289698 Vali Loss: 0.4311110 Test Loss: 0.5945419\n",
            "Validation loss decreased (0.458363 --> 0.431111).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.2007313\n",
            "\tspeed: 5.2979s/iter; left time: 14691.1604s\n",
            "\titers: 200, epoch: 3 | loss: 0.2056759\n",
            "\tspeed: 2.3137s/iter; left time: 6184.6228s\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "  --task_name long_term_forecast \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/traffic/ \\\n",
        "  --data_path traffic.csv \\\n",
        "  --model_id traffic_720_96 \\\n",
        "  --model $model_name \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --seq_len 720 \\\n",
        "  --label_len 48 \\\n",
        "  --pred_len 96 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 3 \\\n",
        "  --enc_in 862 \\\n",
        "  --dec_in 862 \\\n",
        "  --c_out 862 \\\n",
        "  --d_model 512 \\\n",
        "  --d_ff 512 \\\n",
        "  --top_k 5 \\\n",
        "  --des 'Exp' \\\n",
        "  --itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K619I_VA4330"
      },
      "source": [
        "### 3. TimesNet for Illness data:\n",
        "\n",
        "patience=3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEgQcXubj8N1",
        "outputId": "4a35d47a-566d-42fe-8ad1-728c08dcf5d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ili_60_24           Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/illness/  \n",
            "  Data Path:          national_illness.csvFeatures:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            60                  Label Len:          18                  \n",
            "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             7                   Dec In:             7                   \n",
            "  C Out:              7                   d model:            768                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               768                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ili_60_24_TimesNet_custom_ftM_sl60_ll18_pl24_dm768_nh8_el2_dl1_df768_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 593\n",
            "val 74\n",
            "test 170\n",
            "Epoch: 1 cost time: 16.14660620689392\n",
            "Epoch: 1, Steps: 19 | Train Loss: 0.6792097 Vali Loss: 0.2659407 Test Loss: 4.0083995\n",
            "Validation loss decreased (inf --> 0.265941).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 14.886169910430908\n",
            "Epoch: 2, Steps: 19 | Train Loss: 0.4659270 Vali Loss: 0.2741287 Test Loss: 5.3526797\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 14.83007550239563\n",
            "Epoch: 3, Steps: 19 | Train Loss: 0.3211348 Vali Loss: 0.2433264 Test Loss: 4.1268506\n",
            "Validation loss decreased (0.265941 --> 0.243326).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 14.868562698364258\n",
            "Epoch: 4, Steps: 19 | Train Loss: 0.2488428 Vali Loss: 0.2313475 Test Loss: 4.4763160\n",
            "Validation loss decreased (0.243326 --> 0.231347).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "Epoch: 5 cost time: 14.880343198776245\n",
            "Epoch: 5, Steps: 19 | Train Loss: 0.2120355 Vali Loss: 0.2060200 Test Loss: 4.6922584\n",
            "Validation loss decreased (0.231347 --> 0.206020).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "Epoch: 6 cost time: 14.900678396224976\n",
            "Epoch: 6, Steps: 19 | Train Loss: 0.2078197 Vali Loss: 0.2140998 Test Loss: 5.1297731\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 3.125e-06\n",
            "Epoch: 7 cost time: 14.860028982162476\n",
            "Epoch: 7, Steps: 19 | Train Loss: 0.1991849 Vali Loss: 0.1947522 Test Loss: 4.7663927\n",
            "Validation loss decreased (0.206020 --> 0.194752).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "Epoch: 8 cost time: 14.888328790664673\n",
            "Epoch: 8, Steps: 19 | Train Loss: 0.1906157 Vali Loss: 0.1995925 Test Loss: 4.9681630\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 7.8125e-07\n",
            "Epoch: 9 cost time: 14.894224166870117\n",
            "Epoch: 9, Steps: 19 | Train Loss: 0.1924124 Vali Loss: 0.2147715 Test Loss: 4.9077249\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 3.90625e-07\n",
            "Epoch: 10 cost time: 14.844995975494385\n",
            "Epoch: 10, Steps: 19 | Train Loss: 0.1855197 Vali Loss: 0.2118738 Test Loss: 4.8793635\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ili_60_24_TimesNet_custom_ftM_sl60_ll18_pl24_dm768_nh8_el2_dl1_df768_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 170\n",
            "test shape: (170, 24, 7) (170, 24, 7)\n",
            "test shape: (170, 24, 7) (170, 24, 7)\n",
            "mse:2.8450424671173096, mae:1.013441801071167, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "  --task_name long_term_forecast \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/illness/ \\\n",
        "  --data_path national_illness.csv \\\n",
        "  --model_id ili_60_24 \\\n",
        "  --model $model_name \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --seq_len 60 \\\n",
        "  --label_len 18 \\\n",
        "  --pred_len 24 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 3 \\\n",
        "  --enc_in 7 \\\n",
        "  --dec_in 7 \\\n",
        "  --c_out 7 \\\n",
        "  --d_model 768 \\\n",
        "  --d_ff 768 \\\n",
        "  --top_k 5 \\\n",
        "  --des 'Exp' \\\n",
        "  --itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbIauqhh15Y7"
      },
      "source": [
        "illness-patience=5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lru6AaXZ1z5a",
        "outputId": "7556b9f5-6729-4a6c-c746-ad65a9189302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ili_60_24           Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/illness/  \n",
            "  Data Path:          national_illness.csvFeatures:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            60                  Label Len:          18                  \n",
            "  Pred Len:           24                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             7                   Dec In:             7                   \n",
            "  C Out:              7                   d model:            768                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               768                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       100                 Batch Size:         32                  \n",
            "  Patience:           5                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ili_60_24_TimesNet_custom_ftM_sl60_ll18_pl24_dm768_nh8_el2_dl1_df768_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 593\n",
            "val 74\n",
            "test 170\n",
            "Epoch: 1 cost time: 16.174363613128662\n",
            "Epoch: 1, Steps: 19 | Train Loss: 0.6792097 Vali Loss: 0.2659407 Test Loss: 4.0083995\n",
            "Validation loss decreased (inf --> 0.265941).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 14.947072505950928\n",
            "Epoch: 2, Steps: 19 | Train Loss: 0.4659270 Vali Loss: 0.2741287 Test Loss: 5.3526797\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 14.85745882987976\n",
            "Epoch: 3, Steps: 19 | Train Loss: 0.3211348 Vali Loss: 0.2433264 Test Loss: 4.1268506\n",
            "Validation loss decreased (0.265941 --> 0.243326).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 14.90254545211792\n",
            "Epoch: 4, Steps: 19 | Train Loss: 0.2488428 Vali Loss: 0.2313475 Test Loss: 4.4763160\n",
            "Validation loss decreased (0.243326 --> 0.231347).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "Epoch: 5 cost time: 14.897240161895752\n",
            "Epoch: 5, Steps: 19 | Train Loss: 0.2120355 Vali Loss: 0.2060200 Test Loss: 4.6922584\n",
            "Validation loss decreased (0.231347 --> 0.206020).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "Epoch: 6 cost time: 14.918991804122925\n",
            "Epoch: 6, Steps: 19 | Train Loss: 0.2078197 Vali Loss: 0.2140998 Test Loss: 5.1297731\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 3.125e-06\n",
            "Epoch: 7 cost time: 14.87598729133606\n",
            "Epoch: 7, Steps: 19 | Train Loss: 0.1991849 Vali Loss: 0.1947522 Test Loss: 4.7663927\n",
            "Validation loss decreased (0.206020 --> 0.194752).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "Epoch: 8 cost time: 14.907493114471436\n",
            "Epoch: 8, Steps: 19 | Train Loss: 0.1906157 Vali Loss: 0.1995925 Test Loss: 4.9681630\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 7.8125e-07\n",
            "Epoch: 9 cost time: 14.89844822883606\n",
            "Epoch: 9, Steps: 19 | Train Loss: 0.1924124 Vali Loss: 0.2147715 Test Loss: 4.9077249\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Updating learning rate to 3.90625e-07\n",
            "Epoch: 10 cost time: 14.882509469985962\n",
            "Epoch: 10, Steps: 19 | Train Loss: 0.1855197 Vali Loss: 0.2118738 Test Loss: 4.8793635\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Updating learning rate to 1.953125e-07\n",
            "Epoch: 11 cost time: 14.88539743423462\n",
            "Epoch: 11, Steps: 19 | Train Loss: 0.1893175 Vali Loss: 0.1918047 Test Loss: 4.8867636\n",
            "Validation loss decreased (0.194752 --> 0.191805).  Saving model ...\n",
            "Updating learning rate to 9.765625e-08\n",
            "Epoch: 12 cost time: 14.890422105789185\n",
            "Epoch: 12, Steps: 19 | Train Loss: 0.1819940 Vali Loss: 0.1977398 Test Loss: 4.8850608\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 4.8828125e-08\n",
            "Epoch: 13 cost time: 14.875897645950317\n",
            "Epoch: 13, Steps: 19 | Train Loss: 0.1897494 Vali Loss: 0.1976084 Test Loss: 4.8875413\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Updating learning rate to 2.44140625e-08\n",
            "Epoch: 14 cost time: 14.878101348876953\n",
            "Epoch: 14, Steps: 19 | Train Loss: 0.1865825 Vali Loss: 0.2072454 Test Loss: 4.8908629\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Updating learning rate to 1.220703125e-08\n",
            "Epoch: 15 cost time: 14.883919715881348\n",
            "Epoch: 15, Steps: 19 | Train Loss: 0.1944824 Vali Loss: 0.1991968 Test Loss: 4.8927798\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Updating learning rate to 6.103515625e-09\n",
            "Epoch: 16 cost time: 14.8916597366333\n",
            "Epoch: 16, Steps: 19 | Train Loss: 0.1903779 Vali Loss: 0.2045888 Test Loss: 4.8926558\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ili_60_24_TimesNet_custom_ftM_sl60_ll18_pl24_dm768_nh8_el2_dl1_df768_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 170\n",
            "test shape: (170, 24, 7) (170, 24, 7)\n",
            "test shape: (170, 24, 7) (170, 24, 7)\n",
            "mse:2.8802952766418457, mae:1.013312816619873, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/illness/ \\\n",
        "--data_path national_illness.csv \\\n",
        "--model_id ili_60_24 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 60 \\\n",
        "--label_len 18 \\\n",
        "--pred_len 24 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 7 \\\n",
        "--dec_in 7 \\\n",
        "--c_out 7 \\\n",
        "--d_model 768 \\\n",
        "--d_ff 768 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1 \\\n",
        "--patience 5 \\\n",
        "--train_epochs 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMls2h7GaGlY"
      },
      "source": [
        "illness: seq_len=60, pred_len=36"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FIbpUcJaG0L",
        "outputId": "9fd2e396-15bf-4ce0-c69f-7fdab90acc36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ili_60_36           Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/illness/  \n",
            "  Data Path:          national_illness.csvFeatures:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            60                  Label Len:          18                  \n",
            "  Pred Len:           36                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             7                   Dec In:             7                   \n",
            "  C Out:              7                   d model:            768                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               768                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ili_60_36_TimesNet_custom_ftM_sl60_ll18_pl36_dm768_nh8_el2_dl1_df768_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 581\n",
            "val 62\n",
            "test 158\n",
            "Epoch: 1 cost time: 17.349912643432617\n",
            "Epoch: 1, Steps: 19 | Train Loss: 0.7052360 Vali Loss: 0.3482271 Test Loss: 2.7827797\n",
            "Validation loss decreased (inf --> 0.348227).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 15.59572148323059\n",
            "Epoch: 2, Steps: 19 | Train Loss: 0.5167018 Vali Loss: 0.3405162 Test Loss: 2.3368483\n",
            "Validation loss decreased (0.348227 --> 0.340516).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 15.625304698944092\n",
            "Epoch: 3, Steps: 19 | Train Loss: 0.3860019 Vali Loss: 0.2920122 Test Loss: 2.1174576\n",
            "Validation loss decreased (0.340516 --> 0.292012).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 15.58038878440857\n",
            "Epoch: 4, Steps: 19 | Train Loss: 0.3055953 Vali Loss: 0.3126647 Test Loss: 2.3574109\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "Epoch: 5 cost time: 15.611979722976685\n",
            "Epoch: 5, Steps: 19 | Train Loss: 0.2664719 Vali Loss: 0.2703794 Test Loss: 1.9165962\n",
            "Validation loss decreased (0.292012 --> 0.270379).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "Epoch: 6 cost time: 15.606512546539307\n",
            "Epoch: 6, Steps: 19 | Train Loss: 0.2614125 Vali Loss: 0.2612621 Test Loss: 2.0419452\n",
            "Validation loss decreased (0.270379 --> 0.261262).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "Epoch: 7 cost time: 15.658090353012085\n",
            "Epoch: 7, Steps: 19 | Train Loss: 0.2441622 Vali Loss: 0.2770921 Test Loss: 1.9912472\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.5625e-06\n",
            "Epoch: 8 cost time: 15.575188159942627\n",
            "Epoch: 8, Steps: 19 | Train Loss: 0.2616622 Vali Loss: 0.2743013 Test Loss: 1.9943793\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 7.8125e-07\n",
            "Epoch: 9 cost time: 15.582322359085083\n",
            "Epoch: 9, Steps: 19 | Train Loss: 0.2401450 Vali Loss: 0.2908647 Test Loss: 2.0066619\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ili_60_36_TimesNet_custom_ftM_sl60_ll18_pl36_dm768_nh8_el2_dl1_df768_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 158\n",
            "test shape: (158, 36, 7) (158, 36, 7)\n",
            "test shape: (158, 36, 7) (158, 36, 7)\n",
            "mse:2.0176351070404053, mae:0.9250882267951965, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "  --task_name long_term_forecast \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/illness/ \\\n",
        "  --data_path national_illness.csv \\\n",
        "  --model_id ili_60_36 \\\n",
        "  --model $model_name \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --seq_len 60 \\\n",
        "  --label_len 18 \\\n",
        "  --pred_len 36 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 3 \\\n",
        "  --enc_in 7 \\\n",
        "  --dec_in 7 \\\n",
        "  --c_out 7 \\\n",
        "  --d_model 768 \\\n",
        "  --d_ff 768 \\\n",
        "  --top_k 5 \\\n",
        "  --des 'Exp' \\\n",
        "  --itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDVTTUFpaHDz"
      },
      "source": [
        "illness: seq_len=60, pred_len=48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0QYFuysaHZc",
        "outputId": "133561c4-0087-4814-cf48-67e8455acf63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ili_60_48           Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/illness/  \n",
            "  Data Path:          national_illness.csvFeatures:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            60                  Label Len:          18                  \n",
            "  Pred Len:           48                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             7                   Dec In:             7                   \n",
            "  C Out:              7                   d model:            768                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               768                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ili_60_48_TimesNet_custom_ftM_sl60_ll18_pl48_dm768_nh8_el2_dl1_df768_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 569\n",
            "val 50\n",
            "test 146\n",
            "Epoch: 1 cost time: 17.62415385246277\n",
            "Epoch: 1, Steps: 18 | Train Loss: 0.8568566 Vali Loss: 0.3541532 Test Loss: 2.8240936\n",
            "Validation loss decreased (inf --> 0.354153).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 16.20590877532959\n",
            "Epoch: 2, Steps: 18 | Train Loss: 0.5515494 Vali Loss: 0.3492058 Test Loss: 2.7760882\n",
            "Validation loss decreased (0.354153 --> 0.349206).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 16.246484518051147\n",
            "Epoch: 3, Steps: 18 | Train Loss: 0.4377386 Vali Loss: 0.3039026 Test Loss: 3.1865382\n",
            "Validation loss decreased (0.349206 --> 0.303903).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 16.29914093017578\n",
            "Epoch: 4, Steps: 18 | Train Loss: 0.3796469 Vali Loss: 0.2604377 Test Loss: 2.7457352\n",
            "Validation loss decreased (0.303903 --> 0.260438).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "Epoch: 5 cost time: 16.292518138885498\n",
            "Epoch: 5, Steps: 18 | Train Loss: 0.3471566 Vali Loss: 0.2684512 Test Loss: 2.6256664\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 6.25e-06\n",
            "Epoch: 6 cost time: 16.25690746307373\n",
            "Epoch: 6, Steps: 18 | Train Loss: 0.3281906 Vali Loss: 0.2640632 Test Loss: 2.6621437\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 3.125e-06\n",
            "Epoch: 7 cost time: 16.270318269729614\n",
            "Epoch: 7, Steps: 18 | Train Loss: 0.3211519 Vali Loss: 0.2903748 Test Loss: 2.6874919\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ili_60_48_TimesNet_custom_ftM_sl60_ll18_pl48_dm768_nh8_el2_dl1_df768_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 146\n",
            "test shape: (146, 48, 7) (146, 48, 7)\n",
            "test shape: (146, 48, 7) (146, 48, 7)\n",
            "mse:2.4417169094085693, mae:1.0359680652618408, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "  --task_name long_term_forecast \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/illness/ \\\n",
        "  --data_path national_illness.csv \\\n",
        "  --model_id ili_60_48 \\\n",
        "  --model $model_name \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --seq_len 60 \\\n",
        "  --label_len 18 \\\n",
        "  --pred_len 48 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 3 \\\n",
        "  --enc_in 7 \\\n",
        "  --dec_in 7 \\\n",
        "  --c_out 7 \\\n",
        "  --d_model 768 \\\n",
        "  --d_ff 768 \\\n",
        "  --top_k 5 \\\n",
        "  --des 'Exp' \\\n",
        "  --itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1ij2PrDaHuz"
      },
      "source": [
        "illness: seq_len=60, pred_len=60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNTmVkyXaH6f",
        "outputId": "814bd962-6ff6-40aa-cbe1-9b691d9f9bee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ili_60_60           Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/illness/  \n",
            "  Data Path:          national_illness.csvFeatures:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            60                  Label Len:          18                  \n",
            "  Pred Len:           60                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             7                   Dec In:             7                   \n",
            "  C Out:              7                   d model:            768                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               768                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ili_60_60_TimesNet_custom_ftM_sl60_ll18_pl60_dm768_nh8_el2_dl1_df768_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 557\n",
            "val 38\n",
            "test 134\n",
            "Epoch: 1 cost time: 18.288870811462402\n",
            "Epoch: 1, Steps: 18 | Train Loss: 0.9042352 Vali Loss: 0.4420364 Test Loss: 3.1462529\n",
            "Validation loss decreased (inf --> 0.442036).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 16.88152503967285\n",
            "Epoch: 2, Steps: 18 | Train Loss: 0.5956994 Vali Loss: 0.2899522 Test Loss: 2.9288776\n",
            "Validation loss decreased (0.442036 --> 0.289952).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 16.90672492980957\n",
            "Epoch: 3, Steps: 18 | Train Loss: 0.4938617 Vali Loss: 0.3266423 Test Loss: 2.9041882\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 16.908344268798828\n",
            "Epoch: 4, Steps: 18 | Train Loss: 0.4320814 Vali Loss: 0.2571126 Test Loss: 2.6582172\n",
            "Validation loss decreased (0.289952 --> 0.257113).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "Epoch: 5 cost time: 16.91866111755371\n",
            "Epoch: 5, Steps: 18 | Train Loss: 0.4096192 Vali Loss: 0.2660117 Test Loss: 2.6605675\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 6.25e-06\n",
            "Epoch: 6 cost time: 16.918392658233643\n",
            "Epoch: 6, Steps: 18 | Train Loss: 0.3821692 Vali Loss: 0.2725539 Test Loss: 2.6513562\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 3.125e-06\n",
            "Epoch: 7 cost time: 16.88044309616089\n",
            "Epoch: 7, Steps: 18 | Train Loss: 0.3775792 Vali Loss: 0.2627943 Test Loss: 2.6514153\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ili_60_60_TimesNet_custom_ftM_sl60_ll18_pl60_dm768_nh8_el2_dl1_df768_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 134\n",
            "test shape: (134, 60, 7) (134, 60, 7)\n",
            "test shape: (134, 60, 7) (134, 60, 7)\n",
            "mse:2.2745463848114014, mae:0.9863921403884888, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "  --task_name long_term_forecast \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/illness/ \\\n",
        "  --data_path national_illness.csv \\\n",
        "  --model_id ili_60_60 \\\n",
        "  --model $model_name \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --seq_len 60 \\\n",
        "  --label_len 18 \\\n",
        "  --pred_len 60 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 3 \\\n",
        "  --enc_in 7 \\\n",
        "  --dec_in 7 \\\n",
        "  --c_out 7 \\\n",
        "  --d_model 768 \\\n",
        "  --d_ff 768 \\\n",
        "  --top_k 5 \\\n",
        "  --des 'Exp' \\\n",
        "  --itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9cao-R13c7z"
      },
      "source": [
        "### 4. TimesNet for Exchange_rate data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjUWLbv5aV6i",
        "outputId": "11a6c321-8655-4795-f2f5-25482d7b2f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           Exchange_336_96     Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/exchange_rate/\n",
            "  Data Path:          exchange_rate.csv   Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            336                 Label Len:          48                  \n",
            "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             8                   Dec In:             8                   \n",
            "  C Out:              8                   d model:            64                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               64                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_Exchange_336_96_TimesNet_custom_ftM_sl336_ll48_pl96_dm64_nh8_el2_dl1_df64_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 4880\n",
            "val 665\n",
            "test 1422\n",
            "\titers: 100, epoch: 1 | loss: 0.1553365\n",
            "\tspeed: 0.0779s/iter; left time: 111.4709s\n",
            "Epoch: 1 cost time: 11.199787855148315\n",
            "Epoch: 1, Steps: 153 | Train Loss: 0.1946701 Vali Loss: 0.1976587 Test Loss: 0.1662691\n",
            "Validation loss decreased (inf --> 0.197659).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.0652482\n",
            "\tspeed: 0.1188s/iter; left time: 151.8379s\n",
            "Epoch: 2 cost time: 9.72550344467163\n",
            "Epoch: 2, Steps: 153 | Train Loss: 0.0964331 Vali Loss: 0.1938489 Test Loss: 0.2057583\n",
            "Validation loss decreased (0.197659 --> 0.193849).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.0696306\n",
            "\tspeed: 0.1180s/iter; left time: 132.7768s\n",
            "Epoch: 3 cost time: 9.722339391708374\n",
            "Epoch: 3, Steps: 153 | Train Loss: 0.0732315 Vali Loss: 0.1951841 Test Loss: 0.2145694\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.0476996\n",
            "\tspeed: 0.1170s/iter; left time: 113.7069s\n",
            "Epoch: 4 cost time: 9.716987133026123\n",
            "Epoch: 4, Steps: 153 | Train Loss: 0.0639318 Vali Loss: 0.1919467 Test Loss: 0.2127161\n",
            "Validation loss decreased (0.193849 --> 0.191947).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.0427143\n",
            "\tspeed: 0.1179s/iter; left time: 96.5537s\n",
            "Epoch: 5 cost time: 9.744905948638916\n",
            "Epoch: 5, Steps: 153 | Train Loss: 0.0598218 Vali Loss: 0.1908709 Test Loss: 0.2177295\n",
            "Validation loss decreased (0.191947 --> 0.190871).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.0525627\n",
            "\tspeed: 0.1180s/iter; left time: 78.5547s\n",
            "Epoch: 6 cost time: 9.749067783355713\n",
            "Epoch: 6, Steps: 153 | Train Loss: 0.0578885 Vali Loss: 0.1921718 Test Loss: 0.2147168\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.0424072\n",
            "\tspeed: 0.1173s/iter; left time: 60.1952s\n",
            "Epoch: 7 cost time: 9.726699113845825\n",
            "Epoch: 7, Steps: 153 | Train Loss: 0.0570917 Vali Loss: 0.1902304 Test Loss: 0.2171758\n",
            "Validation loss decreased (0.190871 --> 0.190230).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 0.0449286\n",
            "\tspeed: 0.1179s/iter; left time: 42.4474s\n",
            "Epoch: 8 cost time: 9.757519960403442\n",
            "Epoch: 8, Steps: 153 | Train Loss: 0.0566090 Vali Loss: 0.1913188 Test Loss: 0.2173222\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 7.8125e-07\n",
            "\titers: 100, epoch: 9 | loss: 0.0623191\n",
            "\tspeed: 0.1178s/iter; left time: 24.3816s\n",
            "Epoch: 9 cost time: 9.773288249969482\n",
            "Epoch: 9, Steps: 153 | Train Loss: 0.0561432 Vali Loss: 0.1913179 Test Loss: 0.2166301\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 3.90625e-07\n",
            "\titers: 100, epoch: 10 | loss: 0.0436563\n",
            "\tspeed: 0.1174s/iter; left time: 6.3408s\n",
            "Epoch: 10 cost time: 9.74209213256836\n",
            "Epoch: 10, Steps: 153 | Train Loss: 0.0560811 Vali Loss: 0.1908838 Test Loss: 0.2165323\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_Exchange_336_96_TimesNet_custom_ftM_sl336_ll48_pl96_dm64_nh8_el2_dl1_df64_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 1422\n",
            "test shape: (1422, 96, 8) (1422, 96, 8)\n",
            "test shape: (1422, 96, 8) (1422, 96, 8)\n",
            "mse:0.21770934760570526, mae:0.3431643545627594, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/exchange_rate/ \\\n",
        "--data_path exchange_rate.csv \\\n",
        "--model_id Exchange_336_96 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 336 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 96 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 8 \\\n",
        "--dec_in 8 \\\n",
        "--c_out 8 \\\n",
        "--d_model 64 \\\n",
        "--d_ff 64 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTICVDmFgQMr"
      },
      "source": [
        "Exchange_336_96, patience=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_ROfQIIgQZo",
        "outputId": "21dda7df-bc42-4782-dd83-be62fd1e14d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           Exchange_336_96     Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/exchange_rate/\n",
            "  Data Path:          exchange_rate.csv   Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            336                 Label Len:          48                  \n",
            "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             8                   Dec In:             8                   \n",
            "  C Out:              8                   d model:            64                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               64                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       100                 Batch Size:         32                  \n",
            "  Patience:           5                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_Exchange_336_96_TimesNet_custom_ftM_sl336_ll48_pl96_dm64_nh8_el2_dl1_df64_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 4880\n",
            "val 665\n",
            "test 1422\n",
            "\titers: 100, epoch: 1 | loss: 0.1553365\n",
            "\tspeed: 0.0791s/iter; left time: 1202.4162s\n",
            "Epoch: 1 cost time: 11.28965449333191\n",
            "Epoch: 1, Steps: 153 | Train Loss: 0.1946701 Vali Loss: 0.1976587 Test Loss: 0.1662691\n",
            "Validation loss decreased (inf --> 0.197659).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.0652482\n",
            "\tspeed: 0.1178s/iter; left time: 1772.8194s\n",
            "Epoch: 2 cost time: 9.662077903747559\n",
            "Epoch: 2, Steps: 153 | Train Loss: 0.0964331 Vali Loss: 0.1938489 Test Loss: 0.2057583\n",
            "Validation loss decreased (0.197659 --> 0.193849).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.0696306\n",
            "\tspeed: 0.1168s/iter; left time: 1740.3204s\n",
            "Epoch: 3 cost time: 9.668354511260986\n",
            "Epoch: 3, Steps: 153 | Train Loss: 0.0732315 Vali Loss: 0.1951841 Test Loss: 0.2145694\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.0476996\n",
            "\tspeed: 0.1162s/iter; left time: 1712.2860s\n",
            "Epoch: 4 cost time: 9.66441011428833\n",
            "Epoch: 4, Steps: 153 | Train Loss: 0.0639318 Vali Loss: 0.1919467 Test Loss: 0.2127161\n",
            "Validation loss decreased (0.193849 --> 0.191947).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.0427143\n",
            "\tspeed: 0.1173s/iter; left time: 1710.6184s\n",
            "Epoch: 5 cost time: 9.695487022399902\n",
            "Epoch: 5, Steps: 153 | Train Loss: 0.0598218 Vali Loss: 0.1908709 Test Loss: 0.2177295\n",
            "Validation loss decreased (0.191947 --> 0.190871).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.0525627\n",
            "\tspeed: 0.1172s/iter; left time: 1692.3617s\n",
            "Epoch: 6 cost time: 9.711372375488281\n",
            "Epoch: 6, Steps: 153 | Train Loss: 0.0578885 Vali Loss: 0.1921718 Test Loss: 0.2147168\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.0424072\n",
            "\tspeed: 0.1161s/iter; left time: 1658.7114s\n",
            "Epoch: 7 cost time: 9.677215337753296\n",
            "Epoch: 7, Steps: 153 | Train Loss: 0.0570917 Vali Loss: 0.1902304 Test Loss: 0.2171758\n",
            "Validation loss decreased (0.190871 --> 0.190230).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 0.0449286\n",
            "\tspeed: 0.1170s/iter; left time: 1652.8304s\n",
            "Epoch: 8 cost time: 9.688150882720947\n",
            "Epoch: 8, Steps: 153 | Train Loss: 0.0566090 Vali Loss: 0.1913188 Test Loss: 0.2173222\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Updating learning rate to 7.8125e-07\n",
            "\titers: 100, epoch: 9 | loss: 0.0623191\n",
            "\tspeed: 0.1168s/iter; left time: 1632.6908s\n",
            "Epoch: 9 cost time: 9.682359218597412\n",
            "Epoch: 9, Steps: 153 | Train Loss: 0.0561432 Vali Loss: 0.1913179 Test Loss: 0.2166301\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Updating learning rate to 3.90625e-07\n",
            "\titers: 100, epoch: 10 | loss: 0.0436563\n",
            "\tspeed: 0.1163s/iter; left time: 1607.3820s\n",
            "Epoch: 10 cost time: 9.667794227600098\n",
            "Epoch: 10, Steps: 153 | Train Loss: 0.0560811 Vali Loss: 0.1908838 Test Loss: 0.2165323\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Updating learning rate to 1.953125e-07\n",
            "\titers: 100, epoch: 11 | loss: 0.0751348\n",
            "\tspeed: 0.1163s/iter; left time: 1589.4236s\n",
            "Epoch: 11 cost time: 9.672275304794312\n",
            "Epoch: 11, Steps: 153 | Train Loss: 0.0559686 Vali Loss: 0.1910088 Test Loss: 0.2165850\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Updating learning rate to 9.765625e-08\n",
            "\titers: 100, epoch: 12 | loss: 0.0882711\n",
            "\tspeed: 0.1161s/iter; left time: 1569.5132s\n",
            "Epoch: 12 cost time: 9.695113182067871\n",
            "Epoch: 12, Steps: 153 | Train Loss: 0.0559175 Vali Loss: 0.1907920 Test Loss: 0.2165865\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_Exchange_336_96_TimesNet_custom_ftM_sl336_ll48_pl96_dm64_nh8_el2_dl1_df64_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 1422\n",
            "test shape: (1422, 96, 8) (1422, 96, 8)\n",
            "test shape: (1422, 96, 8) (1422, 96, 8)\n",
            "mse:0.21770934760570526, mae:0.3431643545627594, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/exchange_rate/ \\\n",
        "--data_path exchange_rate.csv \\\n",
        "--model_id Exchange_336_96 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 336 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 96 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 8 \\\n",
        "--dec_in 8 \\\n",
        "--c_out 8 \\\n",
        "--d_model 64 \\\n",
        "--d_ff 64 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1 \\\n",
        "--patience 5\\\n",
        "--train_epochs 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-a9OkRrgi7A"
      },
      "source": [
        "Exchange_336_192, patience=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djBBj61TgjGM",
        "outputId": "3d3bee50-ab0a-444c-b6c2-382e12286245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           Exchange_336_192    Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/exchange_rate/\n",
            "  Data Path:          exchange_rate.csv   Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            336                 Label Len:          48                  \n",
            "  Pred Len:           192                 Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             8                   Dec In:             8                   \n",
            "  C Out:              8                   d model:            64                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               64                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_Exchange_336_192_TimesNet_custom_ftM_sl336_ll48_pl192_dm64_nh8_el2_dl1_df64_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 4784\n",
            "val 569\n",
            "test 1326\n",
            "\titers: 100, epoch: 1 | loss: 0.1580918\n",
            "\tspeed: 0.0886s/iter; left time: 124.0596s\n",
            "Epoch: 1 cost time: 12.566560983657837\n",
            "Epoch: 1, Steps: 150 | Train Loss: 0.2578326 Vali Loss: 0.3769172 Test Loss: 0.2979923\n",
            "Validation loss decreased (inf --> 0.376917).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.1079533\n",
            "\tspeed: 0.1342s/iter; left time: 167.8580s\n",
            "Epoch: 2 cost time: 11.074349164962769\n",
            "Epoch: 2, Steps: 150 | Train Loss: 0.1289636 Vali Loss: 0.4510225 Test Loss: 0.3351032\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.0733059\n",
            "\tspeed: 0.1333s/iter; left time: 146.7382s\n",
            "Epoch: 3 cost time: 11.076980590820312\n",
            "Epoch: 3, Steps: 150 | Train Loss: 0.0976677 Vali Loss: 0.4706113 Test Loss: 0.3604669\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1118469\n",
            "\tspeed: 0.1329s/iter; left time: 126.3758s\n",
            "Epoch: 4 cost time: 11.049692630767822\n",
            "Epoch: 4, Steps: 150 | Train Loss: 0.0862105 Vali Loss: 0.4664409 Test Loss: 0.3613500\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_Exchange_336_192_TimesNet_custom_ftM_sl336_ll48_pl192_dm64_nh8_el2_dl1_df64_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 1326\n",
            "test shape: (1326, 192, 8) (1326, 192, 8)\n",
            "test shape: (1326, 192, 8) (1326, 192, 8)\n",
            "mse:0.2990022897720337, mae:0.41073596477508545, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/exchange_rate/ \\\n",
        "--data_path exchange_rate.csv \\\n",
        "--model_id Exchange_336_192 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 336 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 192 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 8 \\\n",
        "--dec_in 8 \\\n",
        "--c_out 8 \\\n",
        "--d_model 64 \\\n",
        "--d_ff 64 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-Pvbl6egj9K"
      },
      "source": [
        "Exchange_336_192, patience=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19a73-KqgkIH",
        "outputId": "50c40ea1-a423-4a8d-ac93-ab384eaf29f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           Exchange_336_336    Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/exchange_rate/\n",
            "  Data Path:          exchange_rate.csv   Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            336                 Label Len:          48                  \n",
            "  Pred Len:           336                 Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             8                   Dec In:             8                   \n",
            "  C Out:              8                   d model:            64                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               64                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_Exchange_336_336_TimesNet_custom_ftM_sl336_ll48_pl336_dm64_nh8_el2_dl1_df64_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 4640\n",
            "val 425\n",
            "test 1182\n",
            "\titers: 100, epoch: 1 | loss: 0.2342948\n",
            "\tspeed: 0.1017s/iter; left time: 137.4545s\n",
            "Epoch: 1 cost time: 14.585669755935669\n",
            "Epoch: 1, Steps: 145 | Train Loss: 0.4187573 Vali Loss: 0.7552990 Test Loss: 0.5118735\n",
            "Validation loss decreased (inf --> 0.755299).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2599529\n",
            "\tspeed: 0.1543s/iter; left time: 186.1218s\n",
            "Epoch: 2 cost time: 12.4048912525177\n",
            "Epoch: 2, Steps: 145 | Train Loss: 0.2246582 Vali Loss: 1.0012354 Test Loss: 0.5419341\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.1875928\n",
            "\tspeed: 0.1482s/iter; left time: 157.2458s\n",
            "Epoch: 3 cost time: 12.39585256576538\n",
            "Epoch: 3, Steps: 145 | Train Loss: 0.1705284 Vali Loss: 1.0427405 Test Loss: 0.5788783\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1526706\n",
            "\tspeed: 0.1480s/iter; left time: 135.6064s\n",
            "Epoch: 4 cost time: 12.385175704956055\n",
            "Epoch: 4, Steps: 145 | Train Loss: 0.1521332 Vali Loss: 1.0896388 Test Loss: 0.5868288\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_Exchange_336_336_TimesNet_custom_ftM_sl336_ll48_pl336_dm64_nh8_el2_dl1_df64_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 1182\n",
            "test shape: (1182, 336, 8) (1182, 336, 8)\n",
            "test shape: (1182, 336, 8) (1182, 336, 8)\n",
            "mse:0.5122362375259399, mae:0.5438117384910583, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/exchange_rate/ \\\n",
        "--data_path exchange_rate.csv \\\n",
        "--model_id Exchange_336_336 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 336 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 336 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 8 \\\n",
        "--dec_in 8 \\\n",
        "--c_out 8 \\\n",
        "--d_model 64 \\\n",
        "--d_ff 64 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoEHDYDbgkQJ"
      },
      "source": [
        "Exchange_336_720, patience=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4gcdQsGgkX-",
        "outputId": "8a513386-3a4d-492f-b9c9-e2717ea206e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           Exchange_336_720    Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/exchange_rate/\n",
            "  Data Path:          exchange_rate.csv   Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            336                 Label Len:          48                  \n",
            "  Pred Len:           720                 Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             8                   Dec In:             8                   \n",
            "  C Out:              8                   d model:            64                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               64                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_Exchange_336_720_TimesNet_custom_ftM_sl336_ll48_pl720_dm64_nh8_el2_dl1_df64_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 4256\n",
            "val 41\n",
            "test 798\n",
            "\titers: 100, epoch: 1 | loss: 0.5640702\n",
            "\tspeed: 0.1449s/iter; left time: 178.4091s\n",
            "Epoch: 1 cost time: 18.699584245681763\n",
            "Epoch: 1, Steps: 133 | Train Loss: 0.6729256 Vali Loss: 1.1608055 Test Loss: 1.6235268\n",
            "Validation loss decreased (inf --> 1.160805).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.4026890\n",
            "\tspeed: 0.1875s/iter; left time: 205.8987s\n",
            "Epoch: 2 cost time: 16.849945783615112\n",
            "Epoch: 2, Steps: 133 | Train Loss: 0.4029445 Vali Loss: 0.7540535 Test Loss: 1.6209247\n",
            "Validation loss decreased (1.160805 --> 0.754053).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.3401450\n",
            "\tspeed: 0.1880s/iter; left time: 181.4175s\n",
            "Epoch: 3 cost time: 16.822481632232666\n",
            "Epoch: 3, Steps: 133 | Train Loss: 0.3266514 Vali Loss: 0.6982387 Test Loss: 1.6527807\n",
            "Validation loss decreased (0.754053 --> 0.698239).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.3712290\n",
            "\tspeed: 0.1877s/iter; left time: 156.1276s\n",
            "Epoch: 4 cost time: 16.82059073448181\n",
            "Epoch: 4, Steps: 133 | Train Loss: 0.2995431 Vali Loss: 0.6985564 Test Loss: 1.7308465\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.2968048\n",
            "\tspeed: 0.1871s/iter; left time: 130.7573s\n",
            "Epoch: 5 cost time: 16.831820726394653\n",
            "Epoch: 5, Steps: 133 | Train Loss: 0.2872143 Vali Loss: 0.7133813 Test Loss: 1.7343521\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.2862076\n",
            "\tspeed: 0.1870s/iter; left time: 105.8402s\n",
            "Epoch: 6 cost time: 16.814727306365967\n",
            "Epoch: 6, Steps: 133 | Train Loss: 0.2813213 Vali Loss: 0.7050886 Test Loss: 1.7146068\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_Exchange_336_720_TimesNet_custom_ftM_sl336_ll48_pl720_dm64_nh8_el2_dl1_df64_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 798\n",
            "test shape: (798, 720, 8) (798, 720, 8)\n",
            "test shape: (798, 720, 8) (798, 720, 8)\n",
            "mse:1.6555826663970947, mae:0.9688942432403564, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/exchange_rate/ \\\n",
        "--data_path exchange_rate.csv \\\n",
        "--model_id Exchange_336_720 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 336 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 720 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 8 \\\n",
        "--dec_in 8 \\\n",
        "--c_out 8 \\\n",
        "--d_model 64 \\\n",
        "--d_ff 64 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usKcWgKv3qSv"
      },
      "source": [
        "### 4. TimesNet for Electricity data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ECL_336_96:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzdTLGcqRQTu",
        "outputId": "f513dbc0-555f-494b-95c0-24f205665ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ECL_336_96          Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/electricity/\n",
            "  Data Path:          electricity.csv     Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            336                 Label Len:          48                  \n",
            "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             321                 Dec In:             321                 \n",
            "  C Out:              321                 d model:            256                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               512                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ECL_336_96_TimesNet_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 17981\n",
            "val 2537\n",
            "test 5165\n",
            "\titers: 100, epoch: 1 | loss: 0.2566556\n",
            "\tspeed: 0.6864s/iter; left time: 3789.4679s\n",
            "\titers: 200, epoch: 1 | loss: 0.2341645\n",
            "\tspeed: 0.6704s/iter; left time: 3634.3443s\n",
            "\titers: 300, epoch: 1 | loss: 0.2421463\n",
            "\tspeed: 0.6767s/iter; left time: 3600.9337s\n",
            "\titers: 400, epoch: 1 | loss: 0.2489232\n",
            "\tspeed: 0.6872s/iter; left time: 3587.6285s\n",
            "\titers: 500, epoch: 1 | loss: 0.2079205\n",
            "\tspeed: 0.6871s/iter; left time: 3518.8046s\n",
            "Epoch: 1 cost time: 383.6704614162445\n",
            "Epoch: 1, Steps: 562 | Train Loss: 0.2517665 Vali Loss: 0.2013169 Test Loss: 0.2354123\n",
            "Validation loss decreased (inf --> 0.201317).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.1829885\n",
            "\tspeed: 1.7433s/iter; left time: 8645.0169s\n",
            "\titers: 200, epoch: 2 | loss: 0.1993950\n",
            "\tspeed: 0.6865s/iter; left time: 3335.6189s\n",
            "\titers: 300, epoch: 2 | loss: 0.1612469\n",
            "\tspeed: 0.6862s/iter; left time: 3265.6342s\n",
            "\titers: 400, epoch: 2 | loss: 0.1327665\n",
            "\tspeed: 0.6849s/iter; left time: 3191.1158s\n",
            "\titers: 500, epoch: 2 | loss: 0.1378581\n",
            "\tspeed: 0.6832s/iter; left time: 3114.8501s\n",
            "Epoch: 2 cost time: 385.44676995277405\n",
            "Epoch: 2, Steps: 562 | Train Loss: 0.1640746 Vali Loss: 0.1533587 Test Loss: 0.1733946\n",
            "Validation loss decreased (0.201317 --> 0.153359).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.1114018\n",
            "\tspeed: 1.7454s/iter; left time: 7674.4867s\n",
            "\titers: 200, epoch: 3 | loss: 0.1199458\n",
            "\tspeed: 0.6817s/iter; left time: 2929.4055s\n",
            "\titers: 300, epoch: 3 | loss: 0.1266291\n",
            "\tspeed: 0.6816s/iter; left time: 2860.8846s\n",
            "\titers: 400, epoch: 3 | loss: 0.1134786\n",
            "\tspeed: 0.6821s/iter; left time: 2794.4682s\n",
            "\titers: 500, epoch: 3 | loss: 0.1056433\n",
            "\tspeed: 0.6826s/iter; left time: 2728.3442s\n",
            "Epoch: 3 cost time: 383.74390387535095\n",
            "Epoch: 3, Steps: 562 | Train Loss: 0.1194713 Vali Loss: 0.1458488 Test Loss: 0.1733555\n",
            "Validation loss decreased (0.153359 --> 0.145849).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.0954116\n",
            "\tspeed: 1.7457s/iter; left time: 6694.7097s\n",
            "\titers: 200, epoch: 4 | loss: 0.1006940\n",
            "\tspeed: 0.6838s/iter; left time: 2553.8085s\n",
            "\titers: 300, epoch: 4 | loss: 0.1018417\n",
            "\tspeed: 0.6837s/iter; left time: 2485.3808s\n",
            "\titers: 400, epoch: 4 | loss: 0.1027281\n",
            "\tspeed: 0.6835s/iter; left time: 2416.1821s\n",
            "\titers: 500, epoch: 4 | loss: 0.0975029\n",
            "\tspeed: 0.6834s/iter; left time: 2347.6440s\n",
            "Epoch: 4 cost time: 384.5639417171478\n",
            "Epoch: 4, Steps: 562 | Train Loss: 0.1016116 Vali Loss: 0.1520222 Test Loss: 0.1790085\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.0893329\n",
            "\tspeed: 1.7302s/iter; left time: 5662.8103s\n",
            "\titers: 200, epoch: 5 | loss: 0.0988015\n",
            "\tspeed: 0.6841s/iter; left time: 2170.5556s\n",
            "\titers: 300, epoch: 5 | loss: 0.1000666\n",
            "\tspeed: 0.6841s/iter; left time: 2102.3369s\n",
            "\titers: 400, epoch: 5 | loss: 0.0951150\n",
            "\tspeed: 0.6843s/iter; left time: 2034.5629s\n",
            "\titers: 500, epoch: 5 | loss: 0.0911860\n",
            "\tspeed: 0.6842s/iter; left time: 1965.6900s\n",
            "Epoch: 5 cost time: 384.7502717971802\n",
            "Epoch: 5, Steps: 562 | Train Loss: 0.0943241 Vali Loss: 0.1554569 Test Loss: 0.1825308\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.0926965\n",
            "\tspeed: 1.7346s/iter; left time: 4702.3685s\n",
            "\titers: 200, epoch: 6 | loss: 0.0928172\n",
            "\tspeed: 0.6845s/iter; left time: 1787.1457s\n",
            "\titers: 300, epoch: 6 | loss: 0.0895590\n",
            "\tspeed: 0.6843s/iter; left time: 1718.3644s\n",
            "\titers: 400, epoch: 6 | loss: 0.0850607\n",
            "\tspeed: 0.6845s/iter; left time: 1650.3142s\n",
            "\titers: 500, epoch: 6 | loss: 0.0913087\n",
            "\tspeed: 0.6842s/iter; left time: 1581.2851s\n",
            "Epoch: 6 cost time: 384.9902517795563\n",
            "Epoch: 6, Steps: 562 | Train Loss: 0.0905324 Vali Loss: 0.1574639 Test Loss: 0.1833369\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ECL_336_96_TimesNet_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 5165\n",
            "test shape: (5165, 96, 321) (5165, 96, 321)\n",
            "test shape: (5165, 96, 321) (5165, 96, 321)\n",
            "mse:0.17317576706409454, mae:0.2788081467151642, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/electricity/ \\\n",
        "--data_path electricity.csv \\\n",
        "--model_id ECL_336_96 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 336 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 96 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 321 \\\n",
        "--dec_in 321 \\\n",
        "--c_out 321 \\\n",
        "--d_model 256 \\\n",
        "--d_ff 512 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ECL_96_96:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjUfRq9FWlR1",
        "outputId": "2d62ed9d-4e5b-480d-fc34-234a5caa5b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ECL_96_96           Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/electricity/\n",
            "  Data Path:          electricity.csv     Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            96                  Label Len:          48                  \n",
            "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             321                 Dec In:             321                 \n",
            "  C Out:              321                 d model:            256                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               512                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ECL_96_96_TimesNet_custom_ftM_sl96_ll48_pl96_dm256_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 11113\n",
            "val 1579\n",
            "test 3366\n",
            "\titers: 100, epoch: 1 | loss: 0.2541153\n",
            "\tspeed: 0.3604s/iter; left time: 1218.3867s\n",
            "\titers: 200, epoch: 1 | loss: 0.2262035\n",
            "\tspeed: 0.3500s/iter; left time: 1148.5097s\n",
            "\titers: 300, epoch: 1 | loss: 0.2502774\n",
            "\tspeed: 0.3518s/iter; left time: 1119.1669s\n",
            "Epoch: 1 cost time: 123.31561517715454\n",
            "Epoch: 1, Steps: 348 | Train Loss: 0.2909905 Vali Loss: 0.2506628 Test Loss: nan\n",
            "Validation loss decreased (inf --> 0.250663).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.1741367\n",
            "\tspeed: 0.7595s/iter; left time: 2303.6795s\n",
            "\titers: 200, epoch: 2 | loss: 0.1484365\n",
            "\tspeed: 0.3561s/iter; left time: 1044.5729s\n",
            "\titers: 300, epoch: 2 | loss: 0.1634210\n",
            "\tspeed: 0.3581s/iter; left time: 1014.5575s\n",
            "Epoch: 2 cost time: 124.42245888710022\n",
            "Epoch: 2, Steps: 348 | Train Loss: 0.1765286 Vali Loss: 0.1912982 Test Loss: nan\n",
            "Validation loss decreased (0.250663 --> 0.191298).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.1531984\n",
            "\tspeed: 0.7754s/iter; left time: 2081.9233s\n",
            "\titers: 200, epoch: 3 | loss: 0.1470868\n",
            "\tspeed: 0.3587s/iter; left time: 927.2915s\n",
            "\titers: 300, epoch: 3 | loss: 0.1334618\n",
            "\tspeed: 0.3592s/iter; left time: 892.4879s\n",
            "Epoch: 3 cost time: 125.04024291038513\n",
            "Epoch: 3, Steps: 348 | Train Loss: 0.1525108 Vali Loss: 0.1831054 Test Loss: nan\n",
            "Validation loss decreased (0.191298 --> 0.183105).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1473742\n",
            "\tspeed: 0.7756s/iter; left time: 1812.4874s\n",
            "\titers: 200, epoch: 4 | loss: 0.1511223\n",
            "\tspeed: 0.3584s/iter; left time: 801.8504s\n",
            "\titers: 300, epoch: 4 | loss: 0.1479071\n",
            "\tspeed: 0.3596s/iter; left time: 768.3877s\n",
            "Epoch: 4 cost time: 125.15334486961365\n",
            "Epoch: 4, Steps: 348 | Train Loss: 0.1416131 Vali Loss: 0.1818560 Test Loss: nan\n",
            "Validation loss decreased (0.183105 --> 0.181856).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.1409852\n",
            "\tspeed: 0.7795s/iter; left time: 1550.4619s\n",
            "\titers: 200, epoch: 5 | loss: 0.1564492\n",
            "\tspeed: 0.3611s/iter; left time: 682.1471s\n",
            "\titers: 300, epoch: 5 | loss: 0.1390697\n",
            "\tspeed: 0.3618s/iter; left time: 647.3211s\n",
            "Epoch: 5 cost time: 125.94394111633301\n",
            "Epoch: 5, Steps: 348 | Train Loss: 0.1359823 Vali Loss: 0.1820420 Test Loss: nan\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.1392290\n",
            "\tspeed: 0.7665s/iter; left time: 1257.8281s\n",
            "\titers: 200, epoch: 6 | loss: 0.1356699\n",
            "\tspeed: 0.3619s/iter; left time: 557.7227s\n",
            "\titers: 300, epoch: 6 | loss: 0.1344196\n",
            "\tspeed: 0.3616s/iter; left time: 521.0243s\n",
            "Epoch: 6 cost time: 126.06605410575867\n",
            "Epoch: 6, Steps: 348 | Train Loss: 0.1331460 Vali Loss: 0.1834558 Test Loss: nan\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.1341782\n",
            "\tspeed: 0.7679s/iter; left time: 992.8324s\n",
            "\titers: 200, epoch: 7 | loss: 0.1199708\n",
            "\tspeed: 0.3622s/iter; left time: 432.0851s\n",
            "\titers: 300, epoch: 7 | loss: 0.1350248\n",
            "\tspeed: 0.3621s/iter; left time: 395.7351s\n",
            "Epoch: 7 cost time: 126.18393182754517\n",
            "Epoch: 7, Steps: 348 | Train Loss: 0.1317753 Vali Loss: 0.1830969 Test Loss: nan\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ECL_96_96_TimesNet_custom_ftM_sl96_ll48_pl96_dm256_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 5165\n",
            "test shape: (5165, 96, 321) (5165, 96, 321)\n",
            "test shape: (5165, 96, 321) (5165, 96, 321)\n",
            "mse:0.18992894887924194, mae:0.2915608584880829, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/electricity/ \\\n",
        "--data_path electricity.csv \\\n",
        "--model_id ECL_96_96 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 96 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 96 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 321 \\\n",
        "--dec_in 321 \\\n",
        "--c_out 321 \\\n",
        "--d_model 256 \\\n",
        "--d_ff 512 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOWqVfbjmOLz"
      },
      "source": [
        "ECL_96_96, patience 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfQVR8mymOUY",
        "outputId": "98876332-92e5-41f3-ddaf-ce3161da2d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ECL_96_96           Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/electricity/\n",
            "  Data Path:          electricity.csv     Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            96                  Label Len:          48                  \n",
            "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             321                 Dec In:             321                 \n",
            "  C Out:              321                 d model:            256                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               512                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       100                 Batch Size:         32                  \n",
            "  Patience:           10                  Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ECL_96_96_TimesNet_custom_ftM_sl96_ll48_pl96_dm256_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 18221\n",
            "val 2537\n",
            "test 5165\n",
            "\titers: 100, epoch: 1 | loss: 0.2653269\n",
            "\tspeed: 0.3650s/iter; left time: 20771.0486s\n",
            "\titers: 200, epoch: 1 | loss: 0.2559776\n",
            "\tspeed: 0.3546s/iter; left time: 20143.6554s\n",
            "\titers: 300, epoch: 1 | loss: 0.2416122\n",
            "\tspeed: 0.3579s/iter; left time: 20290.7530s\n",
            "\titers: 400, epoch: 1 | loss: 0.1932897\n",
            "\tspeed: 0.3583s/iter; left time: 20281.1965s\n",
            "\titers: 500, epoch: 1 | loss: 0.1738660\n",
            "\tspeed: 0.3587s/iter; left time: 20268.2724s\n",
            "Epoch: 1 cost time: 204.82433247566223\n",
            "Epoch: 1, Steps: 570 | Train Loss: 0.2494229 Vali Loss: 0.1593949 Test Loss: 0.1920154\n",
            "Validation loss decreased (inf --> 0.159395).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.1694502\n",
            "\tspeed: 0.9669s/iter; left time: 54464.3211s\n",
            "\titers: 200, epoch: 2 | loss: 0.1542802\n",
            "\tspeed: 0.3642s/iter; left time: 20477.6114s\n",
            "\titers: 300, epoch: 2 | loss: 0.1571235\n",
            "\tspeed: 0.3649s/iter; left time: 20484.7220s\n",
            "\titers: 400, epoch: 2 | loss: 0.1343965\n",
            "\tspeed: 0.3653s/iter; left time: 20465.7473s\n",
            "\titers: 500, epoch: 2 | loss: 0.1451518\n",
            "\tspeed: 0.3669s/iter; left time: 20522.1951s\n",
            "Epoch: 2 cost time: 208.38863015174866\n",
            "Epoch: 2, Steps: 570 | Train Loss: 0.1563056 Vali Loss: 0.1481453 Test Loss: 0.1741794\n",
            "Validation loss decreased (0.159395 --> 0.148145).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.1362669\n",
            "\tspeed: 1.0031s/iter; left time: 55931.3793s\n",
            "\titers: 200, epoch: 3 | loss: 0.1348895\n",
            "\tspeed: 0.3679s/iter; left time: 20476.0525s\n",
            "\titers: 300, epoch: 3 | loss: 0.1402179\n",
            "\tspeed: 0.3683s/iter; left time: 20463.4898s\n",
            "\titers: 400, epoch: 3 | loss: 0.1256068\n",
            "\tspeed: 0.3683s/iter; left time: 20426.3821s\n",
            "\titers: 500, epoch: 3 | loss: 0.1358616\n",
            "\tspeed: 0.3683s/iter; left time: 20387.4999s\n",
            "Epoch: 3 cost time: 210.12183594703674\n",
            "Epoch: 3, Steps: 570 | Train Loss: 0.1332906 Vali Loss: 0.1436218 Test Loss: 0.1696815\n",
            "Validation loss decreased (0.148145 --> 0.143622).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1222565\n",
            "\tspeed: 1.0040s/iter; left time: 55411.6079s\n",
            "\titers: 200, epoch: 4 | loss: 0.1333790\n",
            "\tspeed: 0.3678s/iter; left time: 20264.9228s\n",
            "\titers: 300, epoch: 4 | loss: 0.1199404\n",
            "\tspeed: 0.3682s/iter; left time: 20250.0520s\n",
            "\titers: 400, epoch: 4 | loss: 0.1219073\n",
            "\tspeed: 0.3684s/iter; left time: 20219.1233s\n",
            "\titers: 500, epoch: 4 | loss: 0.1208902\n",
            "\tspeed: 0.3683s/iter; left time: 20180.9414s\n",
            "Epoch: 4 cost time: 210.15065002441406\n",
            "Epoch: 4, Steps: 570 | Train Loss: 0.1227615 Vali Loss: 0.1393410 Test Loss: 0.1688930\n",
            "Validation loss decreased (0.143622 --> 0.139341).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.1161903\n",
            "\tspeed: 1.0036s/iter; left time: 54816.9262s\n",
            "\titers: 200, epoch: 5 | loss: 0.1273292\n",
            "\tspeed: 0.3677s/iter; left time: 20047.0287s\n",
            "\titers: 300, epoch: 5 | loss: 0.1217965\n",
            "\tspeed: 0.3680s/iter; left time: 20026.5521s\n",
            "\titers: 400, epoch: 5 | loss: 0.1105436\n",
            "\tspeed: 0.3680s/iter; left time: 19989.9692s\n",
            "\titers: 500, epoch: 5 | loss: 0.1106758\n",
            "\tspeed: 0.3680s/iter; left time: 19955.7454s\n",
            "Epoch: 5 cost time: 209.99473094940186\n",
            "Epoch: 5, Steps: 570 | Train Loss: 0.1178091 Vali Loss: 0.1395363 Test Loss: 0.1691426\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.1153657\n",
            "\tspeed: 0.9895s/iter; left time: 53480.8361s\n",
            "\titers: 200, epoch: 6 | loss: 0.1127353\n",
            "\tspeed: 0.3684s/iter; left time: 19874.1238s\n",
            "\titers: 300, epoch: 6 | loss: 0.1128844\n",
            "\tspeed: 0.3686s/iter; left time: 19847.4334s\n",
            "\titers: 400, epoch: 6 | loss: 0.1078351\n",
            "\tspeed: 0.3686s/iter; left time: 19813.4811s\n",
            "\titers: 500, epoch: 6 | loss: 0.1124911\n",
            "\tspeed: 0.3683s/iter; left time: 19758.5704s\n",
            "Epoch: 6 cost time: 210.32843470573425\n",
            "Epoch: 6, Steps: 570 | Train Loss: 0.1153135 Vali Loss: 0.1381799 Test Loss: 0.1687135\n",
            "Validation loss decreased (0.139341 --> 0.138180).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.1230968\n",
            "\tspeed: 1.0109s/iter; left time: 54064.9391s\n",
            "\titers: 200, epoch: 7 | loss: 0.1093325\n",
            "\tspeed: 0.3683s/iter; left time: 19660.6057s\n",
            "\titers: 300, epoch: 7 | loss: 0.1163788\n",
            "\tspeed: 0.3695s/iter; left time: 19688.7642s\n",
            "\titers: 400, epoch: 7 | loss: 0.1181586\n",
            "\tspeed: 0.3700s/iter; left time: 19676.4122s\n",
            "\titers: 500, epoch: 7 | loss: 0.1170326\n",
            "\tspeed: 0.3699s/iter; left time: 19635.8339s\n",
            "Epoch: 7 cost time: 210.88658714294434\n",
            "Epoch: 7, Steps: 570 | Train Loss: 0.1140489 Vali Loss: 0.1382525 Test Loss: 0.1683975\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 0.1101843\n",
            "\tspeed: 0.9968s/iter; left time: 52741.6773s\n",
            "\titers: 200, epoch: 8 | loss: 0.1152554\n",
            "\tspeed: 0.3676s/iter; left time: 19415.4796s\n",
            "\titers: 300, epoch: 8 | loss: 0.1114998\n",
            "\tspeed: 0.3679s/iter; left time: 19392.9295s\n",
            "\titers: 400, epoch: 8 | loss: 0.1112634\n",
            "\tspeed: 0.3680s/iter; left time: 19359.6962s\n",
            "\titers: 500, epoch: 8 | loss: 0.1102992\n",
            "\tspeed: 0.3681s/iter; left time: 19330.9219s\n",
            "Epoch: 8 cost time: 210.03278732299805\n",
            "Epoch: 8, Steps: 570 | Train Loss: 0.1134199 Vali Loss: 0.1381758 Test Loss: 0.1683316\n",
            "Validation loss decreased (0.138180 --> 0.138176).  Saving model ...\n",
            "Updating learning rate to 7.8125e-07\n",
            "\titers: 100, epoch: 9 | loss: 0.1152867\n",
            "\tspeed: 1.0035s/iter; left time: 52523.2398s\n",
            "\titers: 200, epoch: 9 | loss: 0.1022545\n",
            "\tspeed: 0.3681s/iter; left time: 19229.3288s\n",
            "\titers: 300, epoch: 9 | loss: 0.1094079\n",
            "\tspeed: 0.3691s/iter; left time: 19244.4781s\n",
            "\titers: 400, epoch: 9 | loss: 0.1203028\n",
            "\tspeed: 0.3693s/iter; left time: 19216.4313s\n",
            "\titers: 500, epoch: 9 | loss: 0.1159585\n",
            "\tspeed: 0.3687s/iter; left time: 19153.0970s\n",
            "Epoch: 9 cost time: 210.51090216636658\n",
            "Epoch: 9, Steps: 570 | Train Loss: 0.1130235 Vali Loss: 0.1376381 Test Loss: 0.1684808\n",
            "Validation loss decreased (0.138176 --> 0.137638).  Saving model ...\n",
            "Updating learning rate to 3.90625e-07\n",
            "\titers: 100, epoch: 10 | loss: 0.1221848\n",
            "\tspeed: 1.0122s/iter; left time: 52400.1235s\n",
            "\titers: 200, epoch: 10 | loss: 0.1091121\n",
            "\tspeed: 0.3694s/iter; left time: 19088.9145s\n",
            "\titers: 300, epoch: 10 | loss: 0.1145994\n",
            "\tspeed: 0.3682s/iter; left time: 18989.2542s\n",
            "\titers: 400, epoch: 10 | loss: 0.1104067\n",
            "\tspeed: 0.3679s/iter; left time: 18938.0988s\n",
            "\titers: 500, epoch: 10 | loss: 0.1145881\n",
            "\tspeed: 0.3680s/iter; left time: 18905.4071s\n",
            "Epoch: 10 cost time: 210.420818567276\n",
            "Epoch: 10, Steps: 570 | Train Loss: 0.1128594 Vali Loss: 0.1377932 Test Loss: 0.1684427\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Updating learning rate to 1.953125e-07\n",
            "\titers: 100, epoch: 11 | loss: 0.1226717\n",
            "\tspeed: 0.9881s/iter; left time: 50593.5265s\n",
            "\titers: 200, epoch: 11 | loss: 0.1139578\n",
            "\tspeed: 0.3681s/iter; left time: 18808.7445s\n",
            "\titers: 300, epoch: 11 | loss: 0.1236443\n",
            "\tspeed: 0.3685s/iter; left time: 18793.8440s\n",
            "\titers: 400, epoch: 11 | loss: 0.1116050\n",
            "\tspeed: 0.3687s/iter; left time: 18767.2280s\n",
            "\titers: 500, epoch: 11 | loss: 0.1146673\n",
            "\tspeed: 0.3690s/iter; left time: 18745.0158s\n",
            "Epoch: 11 cost time: 210.28988933563232\n",
            "Epoch: 11, Steps: 570 | Train Loss: 0.1127350 Vali Loss: 0.1376709 Test Loss: 0.1683640\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Updating learning rate to 9.765625e-08\n",
            "\titers: 100, epoch: 12 | loss: 0.1073992\n",
            "\tspeed: 0.9886s/iter; left time: 50056.2806s\n",
            "\titers: 200, epoch: 12 | loss: 0.1209317\n",
            "\tspeed: 0.3680s/iter; left time: 18597.4617s\n",
            "\titers: 300, epoch: 12 | loss: 0.1236825\n",
            "\tspeed: 0.3686s/iter; left time: 18590.0389s\n",
            "\titers: 400, epoch: 12 | loss: 0.1140156\n",
            "\tspeed: 0.3688s/iter; left time: 18563.6856s\n",
            "\titers: 500, epoch: 12 | loss: 0.1072234\n",
            "\tspeed: 0.3689s/iter; left time: 18531.6812s\n",
            "Epoch: 12 cost time: 210.3363435268402\n",
            "Epoch: 12, Steps: 570 | Train Loss: 0.1127278 Vali Loss: 0.1377225 Test Loss: 0.1683714\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Updating learning rate to 4.8828125e-08\n",
            "\titers: 100, epoch: 13 | loss: 0.1176997\n",
            "\tspeed: 0.9906s/iter; left time: 49588.8380s\n",
            "\titers: 200, epoch: 13 | loss: 0.1167586\n",
            "\tspeed: 0.3680s/iter; left time: 18388.0479s\n",
            "\titers: 300, epoch: 13 | loss: 0.1047226\n",
            "\tspeed: 0.3684s/iter; left time: 18366.7788s\n",
            "\titers: 400, epoch: 13 | loss: 0.1141531\n",
            "\tspeed: 0.3686s/iter; left time: 18340.3810s\n",
            "\titers: 500, epoch: 13 | loss: 0.1044610\n",
            "\tspeed: 0.3684s/iter; left time: 18296.1863s\n",
            "Epoch: 13 cost time: 210.21588802337646\n",
            "Epoch: 13, Steps: 570 | Train Loss: 0.1126702 Vali Loss: 0.1377515 Test Loss: 0.1683950\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Updating learning rate to 2.44140625e-08\n",
            "\titers: 100, epoch: 14 | loss: 0.1172914\n",
            "\tspeed: 0.9916s/iter; left time: 49075.7350s\n",
            "\titers: 200, epoch: 14 | loss: 0.1151153\n",
            "\tspeed: 0.3686s/iter; left time: 18207.0170s\n",
            "\titers: 300, epoch: 14 | loss: 0.1141099\n",
            "\tspeed: 0.3687s/iter; left time: 18174.9562s\n",
            "\titers: 400, epoch: 14 | loss: 0.1119162\n",
            "\tspeed: 0.3688s/iter; left time: 18143.8234s\n",
            "\titers: 500, epoch: 14 | loss: 0.1182371\n",
            "\tspeed: 0.3687s/iter; left time: 18097.8742s\n",
            "Epoch: 14 cost time: 210.44497156143188\n",
            "Epoch: 14, Steps: 570 | Train Loss: 0.1126454 Vali Loss: 0.1376186 Test Loss: 0.1683776\n",
            "Validation loss decreased (0.137638 --> 0.137619).  Saving model ...\n",
            "Updating learning rate to 1.220703125e-08\n",
            "\titers: 100, epoch: 15 | loss: 0.1067808\n",
            "\tspeed: 1.0031s/iter; left time: 49073.0453s\n",
            "\titers: 200, epoch: 15 | loss: 0.1119256\n",
            "\tspeed: 0.3676s/iter; left time: 17948.2665s\n",
            "\titers: 300, epoch: 15 | loss: 0.1164876\n",
            "\tspeed: 0.3680s/iter; left time: 17926.8985s\n",
            "\titers: 400, epoch: 15 | loss: 0.1169975\n",
            "\tspeed: 0.3680s/iter; left time: 17891.7751s\n",
            "\titers: 500, epoch: 15 | loss: 0.1132468\n",
            "\tspeed: 0.3684s/iter; left time: 17874.3675s\n",
            "Epoch: 15 cost time: 210.06234765052795\n",
            "Epoch: 15, Steps: 570 | Train Loss: 0.1126543 Vali Loss: 0.1376398 Test Loss: 0.1683929\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Updating learning rate to 6.103515625e-09\n",
            "\titers: 100, epoch: 16 | loss: 0.1161690\n",
            "\tspeed: 0.9900s/iter; left time: 47867.0160s\n",
            "\titers: 200, epoch: 16 | loss: 0.1118213\n",
            "\tspeed: 0.3680s/iter; left time: 17754.2833s\n",
            "\titers: 300, epoch: 16 | loss: 0.1142063\n",
            "\tspeed: 0.3681s/iter; left time: 17723.6885s\n",
            "\titers: 400, epoch: 16 | loss: 0.1103093\n",
            "\tspeed: 0.3682s/iter; left time: 17694.6350s\n",
            "\titers: 500, epoch: 16 | loss: 0.1123350\n",
            "\tspeed: 0.3683s/iter; left time: 17659.2304s\n",
            "Epoch: 16 cost time: 210.10149478912354\n",
            "Epoch: 16, Steps: 570 | Train Loss: 0.1126686 Vali Loss: 0.1376963 Test Loss: 0.1683918\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Updating learning rate to 3.0517578125e-09\n",
            "\titers: 100, epoch: 17 | loss: 0.1110276\n",
            "\tspeed: 0.9890s/iter; left time: 47255.9107s\n",
            "\titers: 200, epoch: 17 | loss: 0.1217759\n",
            "\tspeed: 0.3681s/iter; left time: 17549.9058s\n",
            "\titers: 300, epoch: 17 | loss: 0.1085060\n",
            "\tspeed: 0.3685s/iter; left time: 17533.7825s\n",
            "\titers: 400, epoch: 17 | loss: 0.1193583\n",
            "\tspeed: 0.3685s/iter; left time: 17498.5662s\n",
            "\titers: 500, epoch: 17 | loss: 0.1130625\n",
            "\tspeed: 0.3683s/iter; left time: 17451.6393s\n",
            "Epoch: 17 cost time: 210.23950290679932\n",
            "Epoch: 17, Steps: 570 | Train Loss: 0.1126697 Vali Loss: 0.1379233 Test Loss: 0.1683882\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Updating learning rate to 1.52587890625e-09\n",
            "\titers: 100, epoch: 18 | loss: 0.1143085\n",
            "\tspeed: 0.9874s/iter; left time: 46616.9830s\n",
            "\titers: 200, epoch: 18 | loss: 0.1180645\n",
            "\tspeed: 0.3676s/iter; left time: 17316.8136s\n",
            "\titers: 300, epoch: 18 | loss: 0.1131662\n",
            "\tspeed: 0.3679s/iter; left time: 17295.6637s\n",
            "\titers: 400, epoch: 18 | loss: 0.1132098\n",
            "\tspeed: 0.3681s/iter; left time: 17269.3207s\n",
            "\titers: 500, epoch: 18 | loss: 0.1170704\n",
            "\tspeed: 0.3684s/iter; left time: 17244.5040s\n",
            "Epoch: 18 cost time: 209.98491072654724\n",
            "Epoch: 18, Steps: 570 | Train Loss: 0.1126667 Vali Loss: 0.1375412 Test Loss: 0.1683874\n",
            "Validation loss decreased (0.137619 --> 0.137541).  Saving model ...\n",
            "Updating learning rate to 7.62939453125e-10\n",
            "\titers: 100, epoch: 19 | loss: 0.1109809\n",
            "\tspeed: 1.0022s/iter; left time: 46744.5003s\n",
            "\titers: 200, epoch: 19 | loss: 0.1124296\n",
            "\tspeed: 0.3678s/iter; left time: 17117.3488s\n",
            "\titers: 300, epoch: 19 | loss: 0.1083658\n",
            "\tspeed: 0.3682s/iter; left time: 17098.6872s\n",
            "\titers: 400, epoch: 19 | loss: 0.1119808\n",
            "\tspeed: 0.3682s/iter; left time: 17062.3656s\n",
            "\titers: 500, epoch: 19 | loss: 0.1138300\n",
            "\tspeed: 0.3681s/iter; left time: 17020.3010s\n",
            "Epoch: 19 cost time: 210.04712986946106\n",
            "Epoch: 19, Steps: 570 | Train Loss: 0.1126642 Vali Loss: 0.1376432 Test Loss: 0.1683869\n",
            "EarlyStopping counter: 1 out of 10\n",
            "Updating learning rate to 3.814697265625e-10\n",
            "\titers: 100, epoch: 20 | loss: 0.1225517\n",
            "\tspeed: 0.9881s/iter; left time: 45521.0620s\n",
            "\titers: 200, epoch: 20 | loss: 0.1214189\n",
            "\tspeed: 0.3679s/iter; left time: 16911.6594s\n",
            "\titers: 300, epoch: 20 | loss: 0.1128660\n",
            "\tspeed: 0.3685s/iter; left time: 16901.5141s\n",
            "\titers: 400, epoch: 20 | loss: 0.1123948\n",
            "\tspeed: 0.3685s/iter; left time: 16864.6386s\n",
            "\titers: 500, epoch: 20 | loss: 0.1093838\n",
            "\tspeed: 0.3682s/iter; left time: 16816.3495s\n",
            "Epoch: 20 cost time: 210.15477466583252\n",
            "Epoch: 20, Steps: 570 | Train Loss: 0.1126585 Vali Loss: 0.1378235 Test Loss: 0.1683869\n",
            "EarlyStopping counter: 2 out of 10\n",
            "Updating learning rate to 1.9073486328125e-10\n",
            "\titers: 100, epoch: 21 | loss: 0.1110652\n",
            "\tspeed: 0.9895s/iter; left time: 45023.3866s\n",
            "\titers: 200, epoch: 21 | loss: 0.1204522\n",
            "\tspeed: 0.3677s/iter; left time: 16695.3869s\n",
            "\titers: 300, epoch: 21 | loss: 0.1067372\n",
            "\tspeed: 0.3677s/iter; left time: 16657.8752s\n",
            "\titers: 400, epoch: 21 | loss: 0.1106114\n",
            "\tspeed: 0.3680s/iter; left time: 16632.7345s\n",
            "\titers: 500, epoch: 21 | loss: 0.1158285\n",
            "\tspeed: 0.3679s/iter; left time: 16592.7427s\n",
            "Epoch: 21 cost time: 209.96368765830994\n",
            "Epoch: 21, Steps: 570 | Train Loss: 0.1126466 Vali Loss: 0.1376503 Test Loss: 0.1683869\n",
            "EarlyStopping counter: 3 out of 10\n",
            "Updating learning rate to 9.5367431640625e-11\n",
            "\titers: 100, epoch: 22 | loss: 0.1103288\n",
            "\tspeed: 0.9911s/iter; left time: 44529.8095s\n",
            "\titers: 200, epoch: 22 | loss: 0.1103727\n",
            "\tspeed: 0.3681s/iter; left time: 16503.2225s\n",
            "\titers: 300, epoch: 22 | loss: 0.1063565\n",
            "\tspeed: 0.3682s/iter; left time: 16468.3425s\n",
            "\titers: 400, epoch: 22 | loss: 0.1119013\n",
            "\tspeed: 0.3684s/iter; left time: 16439.8765s\n",
            "\titers: 500, epoch: 22 | loss: 0.1049497\n",
            "\tspeed: 0.3686s/iter; left time: 16412.9098s\n",
            "Epoch: 22 cost time: 210.21813321113586\n",
            "Epoch: 22, Steps: 570 | Train Loss: 0.1126643 Vali Loss: 0.1377194 Test Loss: 0.1683868\n",
            "EarlyStopping counter: 4 out of 10\n",
            "Updating learning rate to 4.76837158203125e-11\n",
            "\titers: 100, epoch: 23 | loss: 0.1124587\n",
            "\tspeed: 0.9924s/iter; left time: 44024.9376s\n",
            "\titers: 200, epoch: 23 | loss: 0.1062896\n",
            "\tspeed: 0.3683s/iter; left time: 16300.9429s\n",
            "\titers: 300, epoch: 23 | loss: 0.1163022\n",
            "\tspeed: 0.3685s/iter; left time: 16272.3892s\n",
            "\titers: 400, epoch: 23 | loss: 0.1163793\n",
            "\tspeed: 0.3686s/iter; left time: 16242.5456s\n",
            "\titers: 500, epoch: 23 | loss: 0.1119934\n",
            "\tspeed: 0.3686s/iter; left time: 16205.0978s\n",
            "Epoch: 23 cost time: 210.33158373832703\n",
            "Epoch: 23, Steps: 570 | Train Loss: 0.1126438 Vali Loss: 0.1377252 Test Loss: 0.1683869\n",
            "EarlyStopping counter: 5 out of 10\n",
            "Updating learning rate to 2.384185791015625e-11\n",
            "\titers: 100, epoch: 24 | loss: 0.1174557\n",
            "\tspeed: 0.9917s/iter; left time: 43426.5454s\n",
            "\titers: 200, epoch: 24 | loss: 0.1104584\n",
            "\tspeed: 0.3678s/iter; left time: 16069.0454s\n",
            "\titers: 300, epoch: 24 | loss: 0.1021308\n",
            "\tspeed: 0.3679s/iter; left time: 16037.8182s\n",
            "\titers: 400, epoch: 24 | loss: 0.1039160\n",
            "\tspeed: 0.3682s/iter; left time: 16014.6742s\n",
            "\titers: 500, epoch: 24 | loss: 0.1169273\n",
            "\tspeed: 0.3685s/iter; left time: 15987.4687s\n",
            "Epoch: 24 cost time: 210.13746643066406\n",
            "Epoch: 24, Steps: 570 | Train Loss: 0.1126317 Vali Loss: 0.1377086 Test Loss: 0.1683869\n",
            "EarlyStopping counter: 6 out of 10\n",
            "Updating learning rate to 1.1920928955078126e-11\n",
            "\titers: 100, epoch: 25 | loss: 0.1114229\n",
            "\tspeed: 0.9881s/iter; left time: 42705.6722s\n",
            "\titers: 200, epoch: 25 | loss: 0.1153754\n",
            "\tspeed: 0.3677s/iter; left time: 15857.1005s\n",
            "\titers: 300, epoch: 25 | loss: 0.1125069\n",
            "\tspeed: 0.3678s/iter; left time: 15824.2057s\n",
            "\titers: 400, epoch: 25 | loss: 0.1154702\n",
            "\tspeed: 0.3678s/iter; left time: 15788.0744s\n",
            "\titers: 500, epoch: 25 | loss: 0.1205157\n",
            "\tspeed: 0.3679s/iter; left time: 15753.2105s\n",
            "Epoch: 25 cost time: 209.90730142593384\n",
            "Epoch: 25, Steps: 570 | Train Loss: 0.1126390 Vali Loss: 0.1376899 Test Loss: 0.1683869\n",
            "EarlyStopping counter: 7 out of 10\n",
            "Updating learning rate to 5.960464477539063e-12\n",
            "\titers: 100, epoch: 26 | loss: 0.1114449\n",
            "\tspeed: 0.9872s/iter; left time: 42104.3516s\n",
            "\titers: 200, epoch: 26 | loss: 0.1107396\n",
            "\tspeed: 0.3677s/iter; left time: 15648.0953s\n",
            "\titers: 300, epoch: 26 | loss: 0.1096964\n",
            "\tspeed: 0.3679s/iter; left time: 15618.4630s\n",
            "\titers: 400, epoch: 26 | loss: 0.1115142\n",
            "\tspeed: 0.3682s/iter; left time: 15593.6975s\n",
            "\titers: 500, epoch: 26 | loss: 0.1074364\n",
            "\tspeed: 0.3682s/iter; left time: 15556.5339s\n",
            "Epoch: 26 cost time: 210.0079185962677\n",
            "Epoch: 26, Steps: 570 | Train Loss: 0.1126701 Vali Loss: 0.1377159 Test Loss: 0.1683869\n",
            "EarlyStopping counter: 8 out of 10\n",
            "Updating learning rate to 2.9802322387695314e-12\n",
            "\titers: 100, epoch: 27 | loss: 0.1158293\n",
            "\tspeed: 0.9890s/iter; left time: 41619.4937s\n",
            "\titers: 200, epoch: 27 | loss: 0.1102741\n",
            "\tspeed: 0.3684s/iter; left time: 15466.3785s\n",
            "\titers: 300, epoch: 27 | loss: 0.1244918\n",
            "\tspeed: 0.3683s/iter; left time: 15426.2815s\n",
            "\titers: 400, epoch: 27 | loss: 0.1125639\n",
            "\tspeed: 0.3683s/iter; left time: 15388.3656s\n",
            "\titers: 500, epoch: 27 | loss: 0.1067141\n",
            "\tspeed: 0.3684s/iter; left time: 15354.4841s\n",
            "Epoch: 27 cost time: 210.21715760231018\n",
            "Epoch: 27, Steps: 570 | Train Loss: 0.1126450 Vali Loss: 0.1375671 Test Loss: 0.1683868\n",
            "EarlyStopping counter: 9 out of 10\n",
            "Updating learning rate to 1.4901161193847657e-12\n",
            "\titers: 100, epoch: 28 | loss: 0.1181202\n",
            "\tspeed: 0.9883s/iter; left time: 41026.1242s\n",
            "\titers: 200, epoch: 28 | loss: 0.1110134\n",
            "\tspeed: 0.3676s/iter; left time: 15222.8408s\n",
            "\titers: 300, epoch: 28 | loss: 0.1114116\n",
            "\tspeed: 0.3680s/iter; left time: 15201.2360s\n",
            "\titers: 400, epoch: 28 | loss: 0.1154168\n",
            "\tspeed: 0.3681s/iter; left time: 15168.5498s\n",
            "\titers: 500, epoch: 28 | loss: 0.1149506\n",
            "\tspeed: 0.3680s/iter; left time: 15128.8822s\n",
            "Epoch: 28 cost time: 209.97873997688293\n",
            "Epoch: 28, Steps: 570 | Train Loss: 0.1126361 Vali Loss: 0.1376429 Test Loss: 0.1683868\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ECL_96_96_TimesNet_custom_ftM_sl96_ll48_pl96_dm256_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 5165\n",
            "test shape: (5165, 96, 321) (5165, 96, 321)\n",
            "test shape: (5165, 96, 321) (5165, 96, 321)\n",
            "mse:0.16775935888290405, mae:0.2715821862220764, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/electricity/ \\\n",
        "--data_path electricity.csv \\\n",
        "--model_id ECL_96_96 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 96 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 96 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 321 \\\n",
        "--dec_in 321 \\\n",
        "--c_out 321 \\\n",
        "--d_model 256 \\\n",
        "--d_ff 512 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1 \\\n",
        "--patience 10 \\\n",
        "--train_epochs 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBZZZEL2mOhL"
      },
      "source": [
        "ECL_96_192, patience=3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKjXIV77mOpf",
        "outputId": "bfacc062-cbd3-4779-9f9a-dab9046eb489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ECL_96_192          Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/electricity/\n",
            "  Data Path:          electricity.csv     Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            96                  Label Len:          48                  \n",
            "  Pred Len:           192                 Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             321                 Dec In:             321                 \n",
            "  C Out:              321                 d model:            256                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               512                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ECL_96_192_TimesNet_custom_ftM_sl96_ll48_pl192_dm256_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 18125\n",
            "val 2441\n",
            "test 5069\n",
            "\titers: 100, epoch: 1 | loss: 0.2712238\n",
            "\tspeed: 0.5146s/iter; left time: 2866.9633s\n",
            "\titers: 200, epoch: 1 | loss: 0.2551957\n",
            "\tspeed: 0.5058s/iter; left time: 2767.2791s\n",
            "\titers: 300, epoch: 1 | loss: 0.2419274\n",
            "\tspeed: 0.5063s/iter; left time: 2719.5079s\n",
            "\titers: 400, epoch: 1 | loss: 0.2345241\n",
            "\tspeed: 0.5063s/iter; left time: 2668.6870s\n",
            "\titers: 500, epoch: 1 | loss: 0.1930682\n",
            "\tspeed: 0.5077s/iter; left time: 2625.5297s\n",
            "Epoch: 1 cost time: 288.27970457077026\n",
            "Epoch: 1, Steps: 567 | Train Loss: 0.2619441 Vali Loss: 0.1695228 Test Loss: 0.2033561\n",
            "Validation loss decreased (inf --> 0.169523).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2027636\n",
            "\tspeed: 1.3211s/iter; left time: 6610.8813s\n",
            "\titers: 200, epoch: 2 | loss: 0.1701085\n",
            "\tspeed: 0.5062s/iter; left time: 2482.3663s\n",
            "\titers: 300, epoch: 2 | loss: 0.1583900\n",
            "\tspeed: 0.5056s/iter; left time: 2429.1039s\n",
            "\titers: 400, epoch: 2 | loss: 0.1606374\n",
            "\tspeed: 0.5065s/iter; left time: 2382.7517s\n",
            "\titers: 500, epoch: 2 | loss: 0.1471063\n",
            "\tspeed: 0.5072s/iter; left time: 2334.9626s\n",
            "Epoch: 2 cost time: 287.6318509578705\n",
            "Epoch: 2, Steps: 567 | Train Loss: 0.1719752 Vali Loss: 0.1615331 Test Loss: 0.1903022\n",
            "Validation loss decreased (0.169523 --> 0.161533).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.1538377\n",
            "\tspeed: 1.3426s/iter; left time: 5957.1059s\n",
            "\titers: 200, epoch: 3 | loss: 0.1501272\n",
            "\tspeed: 0.5068s/iter; left time: 2198.0496s\n",
            "\titers: 300, epoch: 3 | loss: 0.1460893\n",
            "\tspeed: 0.5063s/iter; left time: 2145.1635s\n",
            "\titers: 400, epoch: 3 | loss: 0.1359995\n",
            "\tspeed: 0.5067s/iter; left time: 2096.3128s\n",
            "\titers: 500, epoch: 3 | loss: 0.1469554\n",
            "\tspeed: 0.5070s/iter; left time: 2046.6671s\n",
            "Epoch: 3 cost time: 287.69653487205505\n",
            "Epoch: 3, Steps: 567 | Train Loss: 0.1478126 Vali Loss: 0.1577777 Test Loss: 0.1872087\n",
            "Validation loss decreased (0.161533 --> 0.157778).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1318374\n",
            "\tspeed: 1.3411s/iter; left time: 5189.8958s\n",
            "\titers: 200, epoch: 4 | loss: 0.1383816\n",
            "\tspeed: 0.5064s/iter; left time: 1909.0796s\n",
            "\titers: 300, epoch: 4 | loss: 0.1375586\n",
            "\tspeed: 0.5065s/iter; left time: 1859.0217s\n",
            "\titers: 400, epoch: 4 | loss: 0.1285629\n",
            "\tspeed: 0.5071s/iter; left time: 1810.3109s\n",
            "\titers: 500, epoch: 4 | loss: 0.1365178\n",
            "\tspeed: 0.5071s/iter; left time: 1759.5189s\n",
            "Epoch: 4 cost time: 287.625097990036\n",
            "Epoch: 4, Steps: 567 | Train Loss: 0.1343316 Vali Loss: 0.1547326 Test Loss: 0.1873983\n",
            "Validation loss decreased (0.157778 --> 0.154733).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.1339168\n",
            "\tspeed: 1.3398s/iter; left time: 4425.3273s\n",
            "\titers: 200, epoch: 5 | loss: 0.1285949\n",
            "\tspeed: 0.5065s/iter; left time: 1622.2593s\n",
            "\titers: 300, epoch: 5 | loss: 0.1427629\n",
            "\tspeed: 0.5067s/iter; left time: 1572.3307s\n",
            "\titers: 400, epoch: 5 | loss: 0.1254665\n",
            "\tspeed: 0.5068s/iter; left time: 1522.0542s\n",
            "\titers: 500, epoch: 5 | loss: 0.1245988\n",
            "\tspeed: 0.5070s/iter; left time: 1471.7482s\n",
            "Epoch: 5 cost time: 287.5664231777191\n",
            "Epoch: 5, Steps: 567 | Train Loss: 0.1283057 Vali Loss: 0.1519375 Test Loss: 0.1880008\n",
            "Validation loss decreased (0.154733 --> 0.151937).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "\titers: 100, epoch: 6 | loss: 0.1234002\n",
            "\tspeed: 1.3410s/iter; left time: 3669.1071s\n",
            "\titers: 200, epoch: 6 | loss: 0.1357462\n",
            "\tspeed: 0.5064s/iter; left time: 1334.8723s\n",
            "\titers: 300, epoch: 6 | loss: 0.1318861\n",
            "\tspeed: 0.5069s/iter; left time: 1285.4142s\n",
            "\titers: 400, epoch: 6 | loss: 0.1211293\n",
            "\tspeed: 0.5071s/iter; left time: 1235.4159s\n",
            "\titers: 500, epoch: 6 | loss: 0.1275074\n",
            "\tspeed: 0.5069s/iter; left time: 1184.0808s\n",
            "Epoch: 6 cost time: 287.63475584983826\n",
            "Epoch: 6, Steps: 567 | Train Loss: 0.1253237 Vali Loss: 0.1525514 Test Loss: 0.1865537\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 3.125e-06\n",
            "\titers: 100, epoch: 7 | loss: 0.1297500\n",
            "\tspeed: 1.3293s/iter; left time: 2883.1993s\n",
            "\titers: 200, epoch: 7 | loss: 0.1254395\n",
            "\tspeed: 0.5067s/iter; left time: 1048.4519s\n",
            "\titers: 300, epoch: 7 | loss: 0.1289703\n",
            "\tspeed: 0.5070s/iter; left time: 998.2099s\n",
            "\titers: 400, epoch: 7 | loss: 0.1273395\n",
            "\tspeed: 0.5066s/iter; left time: 946.9276s\n",
            "\titers: 500, epoch: 7 | loss: 0.1234713\n",
            "\tspeed: 0.5066s/iter; left time: 896.1951s\n",
            "Epoch: 7 cost time: 287.6058678627014\n",
            "Epoch: 7, Steps: 567 | Train Loss: 0.1238340 Vali Loss: 0.1525624 Test Loss: 0.1876934\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 1.5625e-06\n",
            "\titers: 100, epoch: 8 | loss: 0.1262856\n",
            "\tspeed: 1.3268s/iter; left time: 2125.5354s\n",
            "\titers: 200, epoch: 8 | loss: 0.1278474\n",
            "\tspeed: 0.5064s/iter; left time: 760.5554s\n",
            "\titers: 300, epoch: 8 | loss: 0.1285798\n",
            "\tspeed: 0.5068s/iter; left time: 710.5384s\n",
            "\titers: 400, epoch: 8 | loss: 0.1248858\n",
            "\tspeed: 0.5069s/iter; left time: 659.9226s\n",
            "\titers: 500, epoch: 8 | loss: 0.1147884\n",
            "\tspeed: 0.5068s/iter; left time: 609.1184s\n",
            "Epoch: 8 cost time: 287.6105728149414\n",
            "Epoch: 8, Steps: 567 | Train Loss: 0.1230067 Vali Loss: 0.1522655 Test Loss: 0.1866213\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ECL_96_192_TimesNet_custom_ftM_sl96_ll48_pl192_dm256_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 5069\n",
            "test shape: (5069, 192, 321) (5069, 192, 321)\n",
            "test shape: (5069, 192, 321) (5069, 192, 321)\n",
            "mse:0.18681877851486206, mae:0.2891916036605835, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/electricity/ \\\n",
        "--data_path electricity.csv \\\n",
        "--model_id ECL_96_192 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 96 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 192 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 321 \\\n",
        "--dec_in 321 \\\n",
        "--c_out 321 \\\n",
        "--d_model 256 \\\n",
        "--d_ff 512 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJMBWlNLmOyF"
      },
      "source": [
        "ECL_96_336, patience:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4tj9AsfmO64",
        "outputId": "8fcb8fed-a9d0-4e86-f79a-3851d2527bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           ECL_96_336          Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/electricity/\n",
            "  Data Path:          electricity.csv     Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            96                  Label Len:          48                  \n",
            "  Pred Len:           336                 Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             321                 Dec In:             321                 \n",
            "  C Out:              321                 d model:            256                 \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               512                 \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_ECL_96_336_TimesNet_custom_ftM_sl96_ll48_pl336_dm256_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 17981\n",
            "val 2297\n",
            "test 4925\n",
            "\titers: 100, epoch: 1 | loss: 0.2922420\n",
            "\tspeed: 0.6664s/iter; left time: 3679.4578s\n",
            "\titers: 200, epoch: 1 | loss: 0.2615663\n",
            "\tspeed: 0.6553s/iter; left time: 3552.5683s\n",
            "\titers: 300, epoch: 1 | loss: 0.2912578\n",
            "\tspeed: 0.6628s/iter; left time: 3526.5372s\n",
            "\titers: 400, epoch: 1 | loss: 0.2593476\n",
            "\tspeed: 0.6660s/iter; left time: 3477.0269s\n",
            "\titers: 500, epoch: 1 | loss: 0.2537765\n",
            "\tspeed: 0.6649s/iter; left time: 3404.7291s\n",
            "Epoch: 1 cost time: 372.8409175872803\n",
            "Epoch: 1, Steps: 562 | Train Loss: 0.2903795 Vali Loss: 0.2203805 Test Loss: 0.2583703\n",
            "Validation loss decreased (inf --> 0.220381).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2048471\n",
            "\tspeed: 1.6594s/iter; left time: 8229.0132s\n",
            "\titers: 200, epoch: 2 | loss: 0.1868569\n",
            "\tspeed: 0.6608s/iter; left time: 3210.9895s\n",
            "\titers: 300, epoch: 2 | loss: 0.2120987\n",
            "\tspeed: 0.6605s/iter; left time: 3143.3517s\n",
            "\titers: 400, epoch: 2 | loss: 0.1861483\n",
            "\tspeed: 0.6594s/iter; left time: 3072.3478s\n",
            "\titers: 500, epoch: 2 | loss: 0.1881417\n",
            "\tspeed: 0.6598s/iter; left time: 3008.1612s\n",
            "Epoch: 2 cost time: 371.54037714004517\n",
            "Epoch: 2, Steps: 562 | Train Loss: 0.2060564 Vali Loss: 0.1713649 Test Loss: 0.2042011\n",
            "Validation loss decreased (0.220381 --> 0.171365).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.1884960\n",
            "\tspeed: 1.6867s/iter; left time: 7416.2866s\n",
            "\titers: 200, epoch: 3 | loss: 0.1839538\n",
            "\tspeed: 0.6617s/iter; left time: 2843.4245s\n",
            "\titers: 300, epoch: 3 | loss: 0.1908700\n",
            "\tspeed: 0.6624s/iter; left time: 2780.1184s\n",
            "\titers: 400, epoch: 3 | loss: 0.1943572\n",
            "\tspeed: 0.6633s/iter; left time: 2717.5670s\n",
            "\titers: 500, epoch: 3 | loss: 0.2013750\n",
            "\tspeed: 0.6634s/iter; left time: 2651.4973s\n",
            "Epoch: 3 cost time: 372.896253824234\n",
            "Epoch: 3, Steps: 562 | Train Loss: 0.1762861 Vali Loss: 0.1737783 Test Loss: 0.2019271\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1520493\n",
            "\tspeed: 1.6791s/iter; left time: 6439.2854s\n",
            "\titers: 200, epoch: 4 | loss: 0.1802685\n",
            "\tspeed: 0.6630s/iter; left time: 2476.4892s\n",
            "\titers: 300, epoch: 4 | loss: 0.1525212\n",
            "\tspeed: 0.6618s/iter; left time: 2405.5074s\n",
            "\titers: 400, epoch: 4 | loss: 0.1588013\n",
            "\tspeed: 0.6608s/iter; left time: 2336.0264s\n",
            "\titers: 500, epoch: 4 | loss: 0.1678295\n",
            "\tspeed: 0.6612s/iter; left time: 2271.3863s\n",
            "Epoch: 4 cost time: 372.6153280735016\n",
            "Epoch: 4, Steps: 562 | Train Loss: 0.1606663 Vali Loss: 0.1780804 Test Loss: 0.2002770\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.1633541\n",
            "\tspeed: 1.6771s/iter; left time: 5489.1161s\n",
            "\titers: 200, epoch: 5 | loss: 0.1645648\n",
            "\tspeed: 0.6610s/iter; left time: 2097.4888s\n",
            "\titers: 300, epoch: 5 | loss: 0.1394878\n",
            "\tspeed: 0.6612s/iter; left time: 2031.7901s\n",
            "\titers: 400, epoch: 5 | loss: 0.1607495\n",
            "\tspeed: 0.6610s/iter; left time: 1965.0422s\n",
            "\titers: 500, epoch: 5 | loss: 0.1544665\n",
            "\tspeed: 0.6606s/iter; left time: 1897.8598s\n",
            "Epoch: 5 cost time: 372.20698285102844\n",
            "Epoch: 5, Steps: 562 | Train Loss: 0.1517408 Vali Loss: 0.1770101 Test Loss: 0.1988818\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_ECL_96_336_TimesNet_custom_ftM_sl96_ll48_pl336_dm256_nh8_el2_dl1_df512_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 4925\n",
            "test shape: (4925, 336, 321) (4925, 336, 321)\n",
            "test shape: (4925, 336, 321) (4925, 336, 321)\n",
            "mse:0.20409414172172546, mae:0.3056146204471588, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name='TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/electricity/ \\\n",
        "--data_path electricity.csv \\\n",
        "--model_id ECL_96_336 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 96 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 336 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 321 \\\n",
        "--dec_in 321 \\\n",
        "--c_out 321 \\\n",
        "--d_model 256 \\\n",
        "--d_ff 512 \\\n",
        "--top_k 5 \\\n",
        "--des 'Exp' \\\n",
        "--itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wriwu3s13vKd"
      },
      "source": [
        "### 5. TimesNet for Weather data："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "weather_336_96:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2E63skK4H1d",
        "outputId": "a5dc28d6-d540-4dea-a65f-ecab1e885ed0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Args in experiment:\n",
            "\u001b[1mBasic Config\u001b[0m\n",
            "  Task Name:          long_term_forecast  Is Training:        1                   \n",
            "  Model ID:           weather_336_96      Model:              TimesNet            \n",
            "\n",
            "\u001b[1mData Loader\u001b[0m\n",
            "  Data:               custom              Root Path:          ./dataset/weather/  \n",
            "  Data Path:          weather.csv         Features:           M                   \n",
            "  Target:             OT                  Freq:               h                   \n",
            "  Checkpoints:        ./checkpoints/      \n",
            "\n",
            "\u001b[1mForecasting Task\u001b[0m\n",
            "  Seq Len:            336                 Label Len:          48                  \n",
            "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
            "  Inverse:            0                   \n",
            "\n",
            "\u001b[1mModel Parameters\u001b[0m\n",
            "  Top k:              5                   Num Kernels:        6                   \n",
            "  Enc In:             21                  Dec In:             21                  \n",
            "  C Out:              21                  d model:            32                  \n",
            "  n heads:            8                   e layers:           2                   \n",
            "  d layers:           1                   d FF:               32                  \n",
            "  Moving Avg:         25                  Factor:             3                   \n",
            "  Distil:             1                   Dropout:            0.1                 \n",
            "  Embed:              timeF               Activation:         gelu                \n",
            "\n",
            "\u001b[1mRun Parameters\u001b[0m\n",
            "  Num Workers:        10                  Itr:                1                   \n",
            "  Train Epochs:       10                  Batch Size:         32                  \n",
            "  Patience:           3                   Learning Rate:      0.0001              \n",
            "  Des:                Exp                 Loss:               MSE                 \n",
            "  Lradj:              type1               Use Amp:            0                   \n",
            "\n",
            "\u001b[1mGPU\u001b[0m\n",
            "  Use GPU:            1                   GPU:                0                   \n",
            "  Use Multi GPU:      0                   Devices:            0,1,2,3             \n",
            "\n",
            "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
            "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
            "\n",
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : long_term_forecast_weather_336_96_TimesNet_custom_ftM_sl336_ll48_pl96_dm32_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 36456\n",
            "val 5175\n",
            "test 10444\n",
            "\titers: 100, epoch: 1 | loss: 0.3879917\n",
            "\tspeed: 0.9302s/iter; left time: 10511.9823s\n",
            "\titers: 200, epoch: 1 | loss: 0.8357340\n",
            "\tspeed: 1.0650s/iter; left time: 11928.8135s\n",
            "\titers: 300, epoch: 1 | loss: 0.4726094\n",
            "\tspeed: 1.0544s/iter; left time: 11704.4639s\n",
            "\titers: 400, epoch: 1 | loss: 0.3154809\n",
            "\tspeed: 1.0436s/iter; left time: 11480.5908s\n",
            "\titers: 500, epoch: 1 | loss: 0.2835390\n",
            "\tspeed: 1.0287s/iter; left time: 11214.1379s\n",
            "\titers: 600, epoch: 1 | loss: 0.3039151\n",
            "\tspeed: 1.0369s/iter; left time: 11199.1652s\n",
            "\titers: 700, epoch: 1 | loss: 0.3001208\n",
            "\tspeed: 1.0277s/iter; left time: 10996.8859s\n",
            "\titers: 800, epoch: 1 | loss: 0.4057226\n",
            "\tspeed: 1.0180s/iter; left time: 10791.3818s\n",
            "\titers: 900, epoch: 1 | loss: 0.2989587\n",
            "\tspeed: 1.0268s/iter; left time: 10782.7213s\n",
            "\titers: 1000, epoch: 1 | loss: 0.9941731\n",
            "\tspeed: 1.0257s/iter; left time: 10668.0296s\n",
            "\titers: 1100, epoch: 1 | loss: 0.4965881\n",
            "\tspeed: 1.0296s/iter; left time: 10605.3964s\n",
            "Epoch: 1 cost time: 1170.2681193351746\n",
            "Epoch: 1, Steps: 1140 | Train Loss: 0.4494959 Vali Loss: 0.4186044 Test Loss: 0.1677563\n",
            "Validation loss decreased (inf --> 0.418604).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.3142587\n",
            "\tspeed: 1.9384s/iter; left time: 19695.8864s\n",
            "\titers: 200, epoch: 2 | loss: 0.2875324\n",
            "\tspeed: 1.0355s/iter; left time: 10418.6115s\n",
            "\titers: 300, epoch: 2 | loss: 0.2295291\n",
            "\tspeed: 1.0350s/iter; left time: 10309.4556s\n",
            "\titers: 400, epoch: 2 | loss: 0.1728862\n",
            "\tspeed: 1.0354s/iter; left time: 10210.3163s\n",
            "\titers: 500, epoch: 2 | loss: 0.2212968\n",
            "\tspeed: 1.0371s/iter; left time: 10123.5736s\n",
            "\titers: 600, epoch: 2 | loss: 0.3156153\n",
            "\tspeed: 1.0390s/iter; left time: 10037.7871s\n",
            "\titers: 700, epoch: 2 | loss: 0.2746969\n",
            "\tspeed: 1.0385s/iter; left time: 9929.3290s\n",
            "\titers: 800, epoch: 2 | loss: 0.2293966\n",
            "\tspeed: 1.0419s/iter; left time: 9857.3906s\n",
            "\titers: 900, epoch: 2 | loss: 0.2715727\n",
            "\tspeed: 1.0413s/iter; left time: 9747.3907s\n",
            "\titers: 1000, epoch: 2 | loss: 0.1996463\n",
            "\tspeed: 1.0412s/iter; left time: 9642.6701s\n",
            "\titers: 1100, epoch: 2 | loss: 0.4188144\n",
            "\tspeed: 1.0418s/iter; left time: 9544.0230s\n",
            "Epoch: 2 cost time: 1183.5995044708252\n",
            "Epoch: 2, Steps: 1140 | Train Loss: 0.3626837 Vali Loss: 0.4092531 Test Loss: 0.1702716\n",
            "Validation loss decreased (0.418604 --> 0.409253).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.4080051\n",
            "\tspeed: 1.9559s/iter; left time: 17644.2103s\n",
            "\titers: 200, epoch: 3 | loss: 0.2118906\n",
            "\tspeed: 1.0462s/iter; left time: 9333.2143s\n",
            "\titers: 300, epoch: 3 | loss: 0.2166975\n",
            "\tspeed: 1.0478s/iter; left time: 9242.6746s\n",
            "\titers: 400, epoch: 3 | loss: 0.2050207\n",
            "\tspeed: 1.0497s/iter; left time: 9154.4220s\n",
            "\titers: 500, epoch: 3 | loss: 0.1925462\n",
            "\tspeed: 1.0484s/iter; left time: 9038.6467s\n",
            "\titers: 600, epoch: 3 | loss: 1.1631221\n",
            "\tspeed: 1.0494s/iter; left time: 8941.7307s\n",
            "\titers: 700, epoch: 3 | loss: 0.3303179\n",
            "\tspeed: 1.0517s/iter; left time: 8856.7639s\n",
            "\titers: 800, epoch: 3 | loss: 0.2045995\n",
            "\tspeed: 1.0471s/iter; left time: 8712.8381s\n",
            "\titers: 900, epoch: 3 | loss: 0.1786446\n",
            "\tspeed: 1.0484s/iter; left time: 8619.0816s\n",
            "\titers: 1000, epoch: 3 | loss: 0.1958671\n",
            "\tspeed: 1.0487s/iter; left time: 8516.6756s\n",
            "\titers: 1100, epoch: 3 | loss: 0.1988828\n",
            "\tspeed: 1.0499s/iter; left time: 8421.2811s\n",
            "Epoch: 3 cost time: 1194.9692220687866\n",
            "Epoch: 3, Steps: 1140 | Train Loss: 0.3330443 Vali Loss: 0.4205828 Test Loss: 0.1780589\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.2453482\n",
            "\tspeed: 1.9652s/iter; left time: 15487.7641s\n",
            "\titers: 200, epoch: 4 | loss: 0.1800856\n",
            "\tspeed: 1.0526s/iter; left time: 8190.0464s\n",
            "\titers: 300, epoch: 4 | loss: 0.1789191\n",
            "\tspeed: 1.0525s/iter; left time: 8084.2695s\n",
            "\titers: 400, epoch: 4 | loss: 0.2742552\n",
            "\tspeed: 1.0525s/iter; left time: 7978.9067s\n",
            "\titers: 500, epoch: 4 | loss: 0.1952053\n",
            "\tspeed: 1.0526s/iter; left time: 7874.5976s\n",
            "\titers: 600, epoch: 4 | loss: 0.2411260\n",
            "\tspeed: 1.0525s/iter; left time: 7768.3443s\n",
            "\titers: 700, epoch: 4 | loss: 0.1808450\n",
            "\tspeed: 1.0526s/iter; left time: 7663.9432s\n",
            "\titers: 800, epoch: 4 | loss: 0.4153814\n",
            "\tspeed: 1.0524s/iter; left time: 7556.9639s\n",
            "\titers: 900, epoch: 4 | loss: 0.2952526\n",
            "\tspeed: 1.0526s/iter; left time: 7453.5065s\n",
            "\titers: 1000, epoch: 4 | loss: 0.2296793\n",
            "\tspeed: 1.0528s/iter; left time: 7349.4105s\n",
            "\titers: 1100, epoch: 4 | loss: 0.1875924\n",
            "\tspeed: 1.0527s/iter; left time: 7243.3056s\n",
            "Epoch: 4 cost time: 1199.4287717342377\n",
            "Epoch: 4, Steps: 1140 | Train Loss: 0.3197069 Vali Loss: 0.4283780 Test Loss: 0.1831108\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.2326910\n",
            "\tspeed: 1.9656s/iter; left time: 13249.8593s\n",
            "\titers: 200, epoch: 5 | loss: 2.0997171\n",
            "\tspeed: 1.0528s/iter; left time: 6991.7161s\n",
            "\titers: 300, epoch: 5 | loss: 0.3257026\n",
            "\tspeed: 1.0526s/iter; left time: 6885.0463s\n",
            "\titers: 400, epoch: 5 | loss: 0.1521512\n",
            "\tspeed: 1.0527s/iter; left time: 6780.6065s\n",
            "\titers: 500, epoch: 5 | loss: 0.2070545\n",
            "\tspeed: 1.0523s/iter; left time: 6672.6452s\n",
            "\titers: 600, epoch: 5 | loss: 0.2178333\n",
            "\tspeed: 1.0529s/iter; left time: 6571.0665s\n",
            "\titers: 700, epoch: 5 | loss: 0.1459170\n",
            "\tspeed: 1.0525s/iter; left time: 6463.5315s\n",
            "\titers: 800, epoch: 5 | loss: 0.2249719\n",
            "\tspeed: 1.0527s/iter; left time: 6359.4606s\n",
            "\titers: 900, epoch: 5 | loss: 0.1789824\n",
            "\tspeed: 1.0525s/iter; left time: 6252.6785s\n",
            "\titers: 1000, epoch: 5 | loss: 0.2982723\n",
            "\tspeed: 1.0526s/iter; left time: 6148.2841s\n",
            "\titers: 1100, epoch: 5 | loss: 0.1688881\n",
            "\tspeed: 1.0528s/iter; left time: 6043.9707s\n",
            "Epoch: 5 cost time: 1199.5990571975708\n",
            "Epoch: 5, Steps: 1140 | Train Loss: 0.3133072 Vali Loss: 0.4338043 Test Loss: 0.1849373\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : long_term_forecast_weather_336_96_TimesNet_custom_ftM_sl336_ll48_pl96_dm32_nh8_el2_dl1_df32_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 10444\n",
            "test shape: (10444, 96, 21) (10444, 96, 21)\n",
            "test shape: (10444, 96, 21) (10444, 96, 21)\n",
            "mse:0.17047545313835144, mae:0.22755615413188934, dtw:Not calculated\n"
          ]
        }
      ],
      "source": [
        "model_name = 'TimesNet'\n",
        "\n",
        "!python -u run.py \\\n",
        "--task_name long_term_forecast \\\n",
        "--is_training 1 \\\n",
        "--root_path ./dataset/weather/ \\\n",
        "--data_path weather.csv \\\n",
        "--model_id weather_336_96 \\\n",
        "--model $model_name \\\n",
        "--data custom \\\n",
        "--features M \\\n",
        "--seq_len 336 \\\n",
        "--label_len 48 \\\n",
        "--pred_len 96 \\\n",
        "--e_layers 2 \\\n",
        "--d_layers 1 \\\n",
        "--factor 3 \\\n",
        "--enc_in 21 \\\n",
        "--dec_in 21 \\\n",
        "--c_out 21 \\\n",
        "--d_model 32 \\\n",
        "--d_ff 32 \\\n",
        "--top_k 5 \\\n",
        "--des Exp \\\n",
        "--itr 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Save the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoNXIMA_xfuP"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/Time-Series-Library /content/drive/MyDrive/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
