{"cells":[{"cell_type":"code","metadata":{"source_hash":"1acff9c4","execution_start":1744874894210,"execution_millis":87,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"9309bd6833ee4f53a13683869cd0aad9","deepnote_cell_type":"code"},"source":"%cd Time-Series-Library\n!pwd","block_group":"9309bd6833ee4f53a13683869cd0aad9","execution_count":1,"outputs":[{"name":"stdout","text":"/datasets/_deepnote_work/Time-Series-Library\n/datasets/_deepnote_work/Time-Series-Library\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/afcf53e2-b3b5-4eed-9855-19de880a9011","content_dependencies":{"codeHash":"1acff9c4","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"7958d4a9","execution_start":1744874894360,"execution_millis":49052,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"143cc7604bc04851bd20c648f7419797","deepnote_cell_type":"code"},"source":"!pip install -r requirements.txt","block_group":"c26c468e0e8c43fd8be7fd6b5a07f04b","execution_count":2,"outputs":[{"name":"stdout","text":"Collecting einops==0.8.0\n  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting local-attention==1.9.14\n  Downloading local_attention-1.9.14-py3-none-any.whl (9.0 kB)\nCollecting matplotlib==3.7.0\n  Downloading matplotlib-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting numpy==1.23.5\n  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas==1.5.3 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.5.3)\nCollecting patool==1.12\n  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting reformer-pytorch==1.4.4\n  Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\nCollecting scikit-learn==1.2.2\n  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scipy==1.10.1\n  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sktime==0.16.1\n  Downloading sktime-0.16.1-py3-none-any.whl (16.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sympy==1.11.1\n  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torch==1.7.1\n  Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tqdm==4.64.1\n  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting PyWavelets\n  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyparsing>=2.3.1\n  Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fonttools>=4.22.0\n  Downloading fonttools-4.57.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting kiwisolver>=1.0.1\n  Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting contourpy>=1.0.1\n  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: importlib-resources>=3.2.0 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (6.4.4)\nCollecting cycler>=0.10\n  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\nRequirement already satisfied: python-dateutil>=2.7 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (2.9.0.post0)\nRequirement already satisfied: packaging>=20.0 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (24.1)\nCollecting pillow>=6.2.0\n  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (4.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2024.1)\nCollecting axial-positional-embedding>=0.1.0\n  Downloading axial_positional_embedding-0.3.12-py3-none-any.whl (6.7 kB)\nCollecting product-key-memory\n  Downloading product_key_memory-0.2.11-py3-none-any.whl (6.5 kB)\nCollecting joblib>=1.1.1\n  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\nCollecting deprecated>=1.2.13\n  Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\nCollecting numba>=0.53\n  Downloading numba-0.58.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting mpmath>=0.19\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from torch==1.7.1->-r requirements.txt (line 12)) (4.12.2)\nCollecting axial-positional-embedding>=0.1.0\n  Downloading axial_positional_embedding-0.3.11-py3-none-any.whl (6.7 kB)\n  Downloading axial_positional_embedding-0.3.10-py3-none-any.whl (6.7 kB)\n  Downloading axial_positional_embedding-0.3.9-py3-none-any.whl (6.7 kB)\n  Downloading axial_positional_embedding-0.3.7-py3-none-any.whl (6.7 kB)\n  Downloading axial_positional_embedding-0.3.6-py3-none-any.whl (6.5 kB)\n  Downloading axial_positional_embedding-0.3.5-py3-none-any.whl (6.3 kB)\n  Downloading axial_positional_embedding-0.3.4-py3-none-any.whl (6.3 kB)\n  Downloading axial_positional_embedding-0.3.3-py3-none-any.whl (6.3 kB)\n  Downloading axial_positional_embedding-0.3.2-py3-none-any.whl (6.3 kB)\n  Downloading axial_positional_embedding-0.3.1-py3-none-any.whl (6.2 kB)\n  Downloading axial_positional_embedding-0.3.0-py3-none-any.whl (4.9 kB)\n  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting wrapt<2,>=1.10\n  Downloading wrapt-1.17.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib==3.7.0->-r requirements.txt (line 3)) (3.20.1)\nCollecting llvmlite<0.42,>=0.41.0dev0\n  Downloading llvmlite-0.41.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: importlib-metadata in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from numba>=0.53->sktime==0.16.1->-r requirements.txt (line 10)) (8.4.0)\nRequirement already satisfied: six>=1.5 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib==3.7.0->-r requirements.txt (line 3)) (1.16.0)\nCollecting colt5-attention>=0.10.14\n  Downloading CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n  Downloading CoLT5_attention-0.11.0-py3-none-any.whl (18 kB)\n  Downloading CoLT5_attention-0.10.20-py3-none-any.whl (18 kB)\n  Downloading CoLT5_attention-0.10.19-py3-none-any.whl (18 kB)\n  Downloading CoLT5_attention-0.10.18-py3-none-any.whl (18 kB)\n  Downloading CoLT5_attention-0.10.17-py3-none-any.whl (18 kB)\n  Downloading CoLT5_attention-0.10.16-py3-none-any.whl (18 kB)\n  Downloading CoLT5_attention-0.10.15-py3-none-any.whl (18 kB)\n  Downloading CoLT5_attention-0.10.14-py3-none-any.whl (18 kB)\nINFO: pip is looking at multiple versions of typing-extensions to determine which version is compatible with other requirements. This could take a while.\nCollecting typing-extensions\n  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hINFO: pip is looking at multiple versions of product-key-memory to determine which version is compatible with other requirements. This could take a while.\nCollecting product-key-memory\n  Downloading product_key_memory-0.2.10-py3-none-any.whl (6.4 kB)\n  Downloading product_key_memory-0.2.9-py3-none-any.whl (6.2 kB)\n  Downloading product_key_memory-0.2.8-py3-none-any.whl (4.5 kB)\n  Downloading product_key_memory-0.2.7-py3-none-any.whl (4.4 kB)\n  Downloading product_key_memory-0.2.6-py3-none-any.whl (4.4 kB)\n  Downloading product_key_memory-0.2.5-py3-none-any.whl (4.4 kB)\n  Downloading product_key_memory-0.2.4-py3-none-any.whl (4.3 kB)\nINFO: pip is looking at multiple versions of product-key-memory to determine which version is compatible with other requirements. This could take a while.\n  Downloading product_key_memory-0.2.3-py3-none-any.whl (4.3 kB)\n  Downloading product_key_memory-0.2.2-py3-none-any.whl (4.1 kB)\nBuilding wheels for collected packages: axial-positional-embedding\n  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2904 sha256=2ec3503872f232fee1f41f0c566793fb736f848b46c4f64150afa8f30fc26408\n  Stored in directory: /root/.cache/pip/wheels/c8/0e/0e/49d404f1196282825c0bda29574947247c462de52f0193b554\nSuccessfully built axial-positional-embedding\nInstalling collected packages: patool, mpmath, wrapt, tqdm, threadpoolctl, sympy, pyparsing, pillow, numpy, llvmlite, kiwisolver, joblib, fonttools, einops, cycler, torch, scipy, PyWavelets, numba, deprecated, contourpy, scikit-learn, product-key-memory, matplotlib, local-attention, axial-positional-embedding, sktime, reformer-pytorch\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.4\n    Not uninstalling numpy at /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages, outside environment /root/venv\n    Can't uninstall 'numpy'. No files were found to uninstall.\nSuccessfully installed PyWavelets-1.4.1 axial-positional-embedding-0.2.1 contourpy-1.1.1 cycler-0.12.1 deprecated-1.2.18 einops-0.8.0 fonttools-4.57.0 joblib-1.4.2 kiwisolver-1.4.7 llvmlite-0.41.1 local-attention-1.9.14 matplotlib-3.7.0 mpmath-1.3.0 numba-0.58.1 numpy-1.23.5 patool-1.12 pillow-10.4.0 product-key-memory-0.2.2 pyparsing-3.1.4 reformer-pytorch-1.4.4 scikit-learn-1.2.2 scipy-1.10.1 sktime-0.16.1 sympy-1.11.1 threadpoolctl-3.5.0 torch-1.7.1 tqdm-4.64.1 wrapt-1.17.2\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/2b2c9676-6469-4933-b125-b3221803b8fc","content_dependencies":{"codeHash":"7958d4a9","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"35ccf442","execution_start":1744874943470,"execution_millis":77123,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"92dcbca374084ec18924982bac1cba57","deepnote_cell_type":"code"},"source":"# ✅ Step 1: 卸载当前 torch\n!pip uninstall torch -y\n\n# ✅ Step 2: 安装适配新架构的 torch（以 CUDA 11.8 为例）\n!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 -f https://download.pytorch.org/whl/torch_stable.html","block_group":"5a3a2f4c0a1a40dd9b9bb090dd9b1f04","execution_count":3,"outputs":[{"name":"stdout","text":"Found existing installation: torch 1.7.1\nUninstalling torch-1.7.1:\n  Successfully uninstalled torch-1.7.1\nLooking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting torch==2.0.1+cu118\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp38-cp38-linux_x86_64.whl (2267.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m463.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torchvision==0.15.2+cu118\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp38-cp38-linux_x86_64.whl (33.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torchaudio==2.0.2\n  Downloading https://download.pytorch.org/whl/rocm5.4.2/torchaudio-2.0.2%2Brocm5.4.2-cp38-cp38-linux_x86_64.whl (4.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from torch==2.0.1+cu118) (3.15.4)\nRequirement already satisfied: sympy in /root/venv/lib/python3.8/site-packages (from torch==2.0.1+cu118) (1.11.1)\nCollecting triton==2.0.0\n  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from torch==2.0.1+cu118) (4.12.2)\nRequirement already satisfied: jinja2 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from torch==2.0.1+cu118) (3.0.3)\nCollecting networkx\n  Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /root/venv/lib/python3.8/site-packages (from torchvision==0.15.2+cu118) (1.23.5)\nRequirement already satisfied: requests in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from torchvision==0.15.2+cu118) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/venv/lib/python3.8/site-packages (from torchvision==0.15.2+cu118) (10.4.0)\nCollecting cmake\n  Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting lit\n  Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from jinja2->torch==2.0.1+cu118) (2.1.5)\nCollecting networkx\n  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from requests->torchvision==0.15.2+cu118) (3.8)\nRequirement already satisfied: charset-normalizer<4,>=2 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from requests->torchvision==0.15.2+cu118) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from requests->torchvision==0.15.2+cu118) (1.26.20)\nRequirement already satisfied: certifi>=2017.4.17 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from requests->torchvision==0.15.2+cu118) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /root/venv/lib/python3.8/site-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\nInstalling collected packages: lit, networkx, cmake, triton, torch, torchvision, torchaudio\nSuccessfully installed cmake-4.0.0 lit-18.1.8 networkx-3.1 torch-2.0.1+cu118 torchaudio-2.0.2+rocm5.4.2 torchvision-0.15.2+cu118 triton-2.0.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/bdc8ec2d-1c76-4f9c-9b5a-fce07a686b0f","content_dependencies":{"codeHash":"35ccf442","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"60f05441","execution_start":1744875020640,"execution_millis":846,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"d4d45c8ccd204f2eb89ba6d6d87b9e3f","deepnote_cell_type":"code"},"source":"!mkdir -p result/PatchTST_ili_24\n!mkdir -p result/PatchTST_ili_36\n!mkdir -p result/PatchTST_ili_48\n!mkdir -p result/PatchTST_ili_60","block_group":"cbda7920e2124a819ed732e6d859be76","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":{"codeHash":"60f05441","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"3ec0f092","execution_start":1744875429610,"execution_millis":35121,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"f279cf5f3461451d90e969b42def1ff1","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_ili_24 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/ILI \\\n--data_path national_illness.csv \\\n--features M \\\n--seq_len 60 \\\n--label_len 48 \\\n--pred_len 24 \\\n--e_layers 4 \\\n--d_layers 1 \\\n--factor 3 \\\n--enc_in 7 \\\n--n_heads 4 \\\n--d_model 128 \\\n--des benchmark_patchtst \\\n--itr 1 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--learning_rate 0.001 \\\n--checkpoints ./result/PatchTST_ili_24 \\\n2>&1 | tee result/PatchTST_ili_24/train_log.txt","block_group":"70db647c296b4f72ae5b146b6c707fb2","execution_count":15,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_ili_24     Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/ILI       \n  Data Path:          national_illness.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/PatchTST_ili_24\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            60                  Label Len:          48                  \n  Pred Len:           24                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             7                   Dec In:             7                   \n  C Out:              7                   d model:            128                 \n  n heads:            4                   e layers:           4                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             3                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_ili_24_PatchTST_custom_ftM_sl60_ll48_pl24_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 593\nval 74\ntest 170\nEpoch: 1 cost time: 1.456653118133545\nEpoch: 1, Steps: 38 | Train Loss: 0.6300219 Vali Loss: 0.2936027 Test Loss: 2.2899284\nValidation loss decreased (inf --> 0.293603).  Saving model ...\nUpdating learning rate to 0.001\nEpoch: 2 cost time: 0.695145845413208\nEpoch: 2, Steps: 38 | Train Loss: 0.4724961 Vali Loss: 0.2995079 Test Loss: 2.4029913\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0005\nEpoch: 3 cost time: 0.6866803169250488\nEpoch: 3, Steps: 38 | Train Loss: 0.3993469 Vali Loss: 0.2367937 Test Loss: 2.0752783\nValidation loss decreased (0.293603 --> 0.236794).  Saving model ...\nUpdating learning rate to 0.00025\nEpoch: 4 cost time: 0.6758480072021484\nEpoch: 4, Steps: 38 | Train Loss: 0.3548938 Vali Loss: 0.1882096 Test Loss: 1.7881603\nValidation loss decreased (0.236794 --> 0.188210).  Saving model ...\nUpdating learning rate to 0.000125\nEpoch: 5 cost time: 0.6676449775695801\nEpoch: 5, Steps: 38 | Train Loss: 0.3352661 Vali Loss: 0.2087940 Test Loss: 1.9898266\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 6.25e-05\nEpoch: 6 cost time: 0.6881911754608154\nEpoch: 6, Steps: 38 | Train Loss: 0.3225823 Vali Loss: 0.1986191 Test Loss: 1.9140061\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 3.125e-05\nEpoch: 7 cost time: 0.6809685230255127\nEpoch: 7, Steps: 38 | Train Loss: 0.3216227 Vali Loss: 0.2098116 Test Loss: 1.8886771\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 1.5625e-05\nEpoch: 8 cost time: 0.6862008571624756\nEpoch: 8, Steps: 38 | Train Loss: 0.3149618 Vali Loss: 0.2134127 Test Loss: 1.8850030\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 7.8125e-06\nEpoch: 9 cost time: 0.6772689819335938\nEpoch: 9, Steps: 38 | Train Loss: 0.3188980 Vali Loss: 0.2108324 Test Loss: 1.8690926\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 3.90625e-06\nEpoch: 10 cost time: 0.6912660598754883\nEpoch: 10, Steps: 38 | Train Loss: 0.3084387 Vali Loss: 0.2047247 Test Loss: 1.8659844\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 1.953125e-06\nEpoch: 11 cost time: 0.6780760288238525\nEpoch: 11, Steps: 38 | Train Loss: 0.3209929 Vali Loss: 0.2062739 Test Loss: 1.8645089\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 9.765625e-07\nEpoch: 12 cost time: 0.6687495708465576\nEpoch: 12, Steps: 38 | Train Loss: 0.3075618 Vali Loss: 0.2034214 Test Loss: 1.8687435\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 4.8828125e-07\nEpoch: 13 cost time: 0.6759517192840576\nEpoch: 13, Steps: 38 | Train Loss: 0.3974422 Vali Loss: 0.1970536 Test Loss: 1.8704247\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 2.44140625e-07\nEpoch: 14 cost time: 0.6922340393066406\nEpoch: 14, Steps: 38 | Train Loss: 0.3103007 Vali Loss: 0.2057366 Test Loss: 1.8684249\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_ili_24_PatchTST_custom_ftM_sl60_ll48_pl24_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 170\ntest shape: (170, 24, 7) (170, 24, 7)\ntest shape: (170, 24, 7) (170, 24, 7)\nmse:1.610073447227478, mae:0.8079586029052734, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/c666c139-dcd1-49bd-8250-b450f4c9f7a6","content_dependencies":{"error":{"type":"SyntaxError","message":"invalid syntax (<unknown>, line 2)"},"codeHash":"3ec0f092","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"35aefa3a","execution_start":1744875501371,"execution_millis":35580,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"97b89a85c30c4b5b84d24feef3fc9335","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_ili_36 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/ILI \\\n--data_path national_illness.csv \\\n--features M \\\n--seq_len 60 \\\n--label_len 48 \\\n--pred_len 36 \\\n--e_layers 4 \\\n--d_layers 1 \\\n--factor 3 \\\n--enc_in 7 \\\n--n_heads 4 \\\n--d_model 128 \\\n--des benchmark_patchtst \\\n--itr 1 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--learning_rate 0.001 \\\n--checkpoints ./result/PatchTST_ili_36 \\\n2>&1 | tee result/PatchTST_ili_36/train_log.txt","block_group":"dd9df068524846d28306adf0b6f3889b","execution_count":18,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_ili_36     Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/ILI       \n  Data Path:          national_illness.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/PatchTST_ili_36\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            60                  Label Len:          48                  \n  Pred Len:           36                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             7                   Dec In:             7                   \n  C Out:              7                   d model:            128                 \n  n heads:            4                   e layers:           4                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             3                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_ili_36_PatchTST_custom_ftM_sl60_ll48_pl36_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 581\nval 62\ntest 158\nEpoch: 1 cost time: 1.3668584823608398\nEpoch: 1, Steps: 37 | Train Loss: 0.6538584 Vali Loss: 0.2464665 Test Loss: 2.5323548\nValidation loss decreased (inf --> 0.246466).  Saving model ...\nUpdating learning rate to 0.001\nEpoch: 2 cost time: 0.6789894104003906\nEpoch: 2, Steps: 37 | Train Loss: 0.5585745 Vali Loss: 0.4279875 Test Loss: 2.7494094\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0005\nEpoch: 3 cost time: 0.6692633628845215\nEpoch: 3, Steps: 37 | Train Loss: 0.4803229 Vali Loss: 0.2606955 Test Loss: 2.2240574\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.00025\nEpoch: 4 cost time: 0.6745588779449463\nEpoch: 4, Steps: 37 | Train Loss: 0.4242030 Vali Loss: 0.2242613 Test Loss: 1.8755451\nValidation loss decreased (0.246466 --> 0.224261).  Saving model ...\nUpdating learning rate to 0.000125\nEpoch: 5 cost time: 0.6756842136383057\nEpoch: 5, Steps: 37 | Train Loss: 0.3968193 Vali Loss: 0.2237912 Test Loss: 1.8713925\nValidation loss decreased (0.224261 --> 0.223791).  Saving model ...\nUpdating learning rate to 6.25e-05\nEpoch: 6 cost time: 0.6594922542572021\nEpoch: 6, Steps: 37 | Train Loss: 0.3808952 Vali Loss: 0.2358952 Test Loss: 1.9236006\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 3.125e-05\nEpoch: 7 cost time: 0.670508623123169\nEpoch: 7, Steps: 37 | Train Loss: 0.3756482 Vali Loss: 0.2368461 Test Loss: 1.9130147\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 1.5625e-05\nEpoch: 8 cost time: 0.6783888339996338\nEpoch: 8, Steps: 37 | Train Loss: 0.3841519 Vali Loss: 0.2354902 Test Loss: 1.8734310\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 7.8125e-06\nEpoch: 9 cost time: 0.6677255630493164\nEpoch: 9, Steps: 37 | Train Loss: 0.3830792 Vali Loss: 0.2334708 Test Loss: 1.9000032\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 3.90625e-06\nEpoch: 10 cost time: 0.67864990234375\nEpoch: 10, Steps: 37 | Train Loss: 0.3918696 Vali Loss: 0.2362233 Test Loss: 1.8986603\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 1.953125e-06\nEpoch: 11 cost time: 0.6900560855865479\nEpoch: 11, Steps: 37 | Train Loss: 0.3666007 Vali Loss: 0.2388120 Test Loss: 1.9137179\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 9.765625e-07\nEpoch: 12 cost time: 0.6666991710662842\nEpoch: 12, Steps: 37 | Train Loss: 0.3733577 Vali Loss: 0.2396469 Test Loss: 1.9142030\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 4.8828125e-07\nEpoch: 13 cost time: 0.6708970069885254\nEpoch: 13, Steps: 37 | Train Loss: 0.3762589 Vali Loss: 0.2362465 Test Loss: 1.9105018\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 2.44140625e-07\nEpoch: 14 cost time: 0.6604876518249512\nEpoch: 14, Steps: 37 | Train Loss: 0.3666307 Vali Loss: 0.2367115 Test Loss: 1.9141449\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 1.220703125e-07\nEpoch: 15 cost time: 0.6589212417602539\nEpoch: 15, Steps: 37 | Train Loss: 0.3708023 Vali Loss: 0.2358399 Test Loss: 1.9148716\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_ili_36_PatchTST_custom_ftM_sl60_ll48_pl36_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 158\ntest shape: (158, 36, 7) (158, 36, 7)\ntest shape: (158, 36, 7) (158, 36, 7)\nmse:1.8403921127319336, mae:0.8819899559020996, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/67ec9c20-e676-4dff-aa34-c479fa7069d7","content_dependencies":{"error":{"type":"SyntaxError","message":"invalid syntax (<unknown>, line 2)"},"codeHash":"35aefa3a","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"91fdb356","execution_start":1744875545948,"execution_millis":36450,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"a07b4efa087246e59b744813189acb0e","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_ili_48 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/ILI \\\n--data_path national_illness.csv \\\n--features M \\\n--seq_len 60 \\\n--label_len 48 \\\n--pred_len 48 \\\n--e_layers 4 \\\n--d_layers 1 \\\n--factor 3 \\\n--enc_in 7 \\\n--n_heads 4 \\\n--d_model 128 \\\n--des benchmark_patchtst \\\n--itr 1 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--learning_rate 0.001 \\\n--checkpoints ./result/PatchTST_ili_48 \\\n2>&1 | tee result/PatchTST_ili_48/train_log.txt","block_group":"2cc4fdb9eca94cf588d89de5eb147555","execution_count":21,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_ili_48     Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/ILI       \n  Data Path:          national_illness.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/PatchTST_ili_48\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            60                  Label Len:          48                  \n  Pred Len:           48                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             7                   Dec In:             7                   \n  C Out:              7                   d model:            128                 \n  n heads:            4                   e layers:           4                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             3                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_ili_48_PatchTST_custom_ftM_sl60_ll48_pl48_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 569\nval 50\ntest 146\nEpoch: 1 cost time: 1.2995259761810303\nEpoch: 1, Steps: 36 | Train Loss: 0.6594253 Vali Loss: 0.2637264 Test Loss: 2.7043445\nValidation loss decreased (inf --> 0.263726).  Saving model ...\nUpdating learning rate to 0.001\nEpoch: 2 cost time: 0.6591758728027344\nEpoch: 2, Steps: 36 | Train Loss: 0.5233320 Vali Loss: 0.2723484 Test Loss: 2.6960232\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0005\nEpoch: 3 cost time: 0.6697862148284912\nEpoch: 3, Steps: 36 | Train Loss: 0.4758737 Vali Loss: 0.2803866 Test Loss: 2.4643104\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.00025\nEpoch: 4 cost time: 0.6539943218231201\nEpoch: 4, Steps: 36 | Train Loss: 0.4408143 Vali Loss: 0.2188658 Test Loss: 2.4802678\nValidation loss decreased (0.263726 --> 0.218866).  Saving model ...\nUpdating learning rate to 0.000125\nEpoch: 5 cost time: 0.6654491424560547\nEpoch: 5, Steps: 36 | Train Loss: 0.4159398 Vali Loss: 0.2506242 Test Loss: 2.3251476\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 6.25e-05\nEpoch: 6 cost time: 0.6637592315673828\nEpoch: 6, Steps: 36 | Train Loss: 0.3986213 Vali Loss: 0.1968810 Test Loss: 2.3429980\nValidation loss decreased (0.218866 --> 0.196881).  Saving model ...\nUpdating learning rate to 3.125e-05\nEpoch: 7 cost time: 0.6957077980041504\nEpoch: 7, Steps: 36 | Train Loss: 0.3989332 Vali Loss: 0.2299835 Test Loss: 2.2648540\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 1.5625e-05\nEpoch: 8 cost time: 0.6566164493560791\nEpoch: 8, Steps: 36 | Train Loss: 0.3909935 Vali Loss: 0.2587754 Test Loss: 2.3459239\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 7.8125e-06\nEpoch: 9 cost time: 0.6846461296081543\nEpoch: 9, Steps: 36 | Train Loss: 0.3891160 Vali Loss: 0.2254216 Test Loss: 2.3227496\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 3.90625e-06\nEpoch: 10 cost time: 0.6685714721679688\nEpoch: 10, Steps: 36 | Train Loss: 0.3864060 Vali Loss: 0.2030312 Test Loss: 2.3174138\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 1.953125e-06\nEpoch: 11 cost time: 0.6714587211608887\nEpoch: 11, Steps: 36 | Train Loss: 0.3881155 Vali Loss: 0.2425529 Test Loss: 2.3112140\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 9.765625e-07\nEpoch: 12 cost time: 0.6845781803131104\nEpoch: 12, Steps: 36 | Train Loss: 0.3868621 Vali Loss: 0.2193026 Test Loss: 2.3130233\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 4.8828125e-07\nEpoch: 13 cost time: 0.6553220748901367\nEpoch: 13, Steps: 36 | Train Loss: 0.3898993 Vali Loss: 0.2252288 Test Loss: 2.3107631\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 2.44140625e-07\nEpoch: 14 cost time: 0.6531264781951904\nEpoch: 14, Steps: 36 | Train Loss: 0.3907865 Vali Loss: 0.2196835 Test Loss: 2.3109009\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 1.220703125e-07\nEpoch: 15 cost time: 0.6536757946014404\nEpoch: 15, Steps: 36 | Train Loss: 0.3919244 Vali Loss: 0.2399023 Test Loss: 2.3079700\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 6.103515625e-08\nEpoch: 16 cost time: 0.6558547019958496\nEpoch: 16, Steps: 36 | Train Loss: 0.3875611 Vali Loss: 0.2224860 Test Loss: 2.3105319\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_ili_48_PatchTST_custom_ftM_sl60_ll48_pl48_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 146\ntest shape: (146, 48, 7) (146, 48, 7)\ntest shape: (146, 48, 7) (146, 48, 7)\nmse:1.9937465190887451, mae:0.9191688299179077, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/e026eb7d-0ca8-4ff7-8a72-aa5450b1d6b1","content_dependencies":{"error":{"type":"SyntaxError","message":"invalid syntax (<unknown>, line 2)"},"codeHash":"91fdb356","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"3d3b0170","execution_start":1744875649610,"execution_millis":41831,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"1fb3392cb3d648049650fbede3539356","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_ili_60 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/ILI \\\n--data_path national_illness.csv \\\n--features M \\\n--seq_len 60 \\\n--label_len 48 \\\n--pred_len 60 \\\n--e_layers 4 \\\n--d_layers 1 \\\n--factor 3 \\\n--enc_in 7 \\\n--n_heads 16 \\\n--d_model 128 \\\n--des benchmark_patchtst \\\n--itr 1 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--learning_rate 0.001 \\\n--checkpoints ./result/PatchTST_ili_60 \\\n2>&1 | tee result/PatchTST_ili_60/train_log.txt","block_group":"cf7268f210b64889b6815b4e6fdb121e","execution_count":24,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_ili_60     Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/ILI       \n  Data Path:          national_illness.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/PatchTST_ili_60\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            60                  Label Len:          48                  \n  Pred Len:           60                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             7                   Dec In:             7                   \n  C Out:              7                   d model:            128                 \n  n heads:            16                  e layers:           4                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             3                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_ili_60_PatchTST_custom_ftM_sl60_ll48_pl60_dm128_nh16_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 557\nval 38\ntest 134\nEpoch: 1 cost time: 1.4179377555847168\nEpoch: 1, Steps: 35 | Train Loss: 0.6718019 Vali Loss: 0.3466450 Test Loss: 2.9976926\nValidation loss decreased (inf --> 0.346645).  Saving model ...\nUpdating learning rate to 0.001\nEpoch: 2 cost time: 0.6780483722686768\nEpoch: 2, Steps: 35 | Train Loss: 0.5521377 Vali Loss: 0.2526321 Test Loss: 2.3841393\nValidation loss decreased (0.346645 --> 0.252632).  Saving model ...\nUpdating learning rate to 0.0005\nEpoch: 3 cost time: 0.6853089332580566\nEpoch: 3, Steps: 35 | Train Loss: 0.4784404 Vali Loss: 0.2888205 Test Loss: 2.3192706\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00025\nEpoch: 4 cost time: 0.7427475452423096\nEpoch: 4, Steps: 35 | Train Loss: 0.4508004 Vali Loss: 0.2158904 Test Loss: 2.2751772\nValidation loss decreased (0.252632 --> 0.215890).  Saving model ...\nUpdating learning rate to 0.000125\nEpoch: 5 cost time: 0.7144346237182617\nEpoch: 5, Steps: 35 | Train Loss: 0.4169624 Vali Loss: 0.2150960 Test Loss: 2.3190098\nValidation loss decreased (0.215890 --> 0.215096).  Saving model ...\nUpdating learning rate to 6.25e-05\nEpoch: 6 cost time: 0.6402304172515869\nEpoch: 6, Steps: 35 | Train Loss: 0.3997744 Vali Loss: 0.2219747 Test Loss: 2.2830026\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 3.125e-05\nEpoch: 7 cost time: 0.6483047008514404\nEpoch: 7, Steps: 35 | Train Loss: 0.3936437 Vali Loss: 0.2193381 Test Loss: 2.1760945\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 1.5625e-05\nEpoch: 8 cost time: 0.6560704708099365\nEpoch: 8, Steps: 35 | Train Loss: 0.3938966 Vali Loss: 0.2078507 Test Loss: 2.2012684\nValidation loss decreased (0.215096 --> 0.207851).  Saving model ...\nUpdating learning rate to 7.8125e-06\nEpoch: 9 cost time: 0.6604466438293457\nEpoch: 9, Steps: 35 | Train Loss: 0.3879312 Vali Loss: 0.2018721 Test Loss: 2.1992335\nValidation loss decreased (0.207851 --> 0.201872).  Saving model ...\nUpdating learning rate to 3.90625e-06\nEpoch: 10 cost time: 0.6610462665557861\nEpoch: 10, Steps: 35 | Train Loss: 0.3889118 Vali Loss: 0.2195849 Test Loss: 2.1853294\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 1.953125e-06\nEpoch: 11 cost time: 0.6569023132324219\nEpoch: 11, Steps: 35 | Train Loss: 0.3901809 Vali Loss: 0.2080658 Test Loss: 2.1946445\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 9.765625e-07\nEpoch: 12 cost time: 0.6731264591217041\nEpoch: 12, Steps: 35 | Train Loss: 0.3874258 Vali Loss: 0.2147958 Test Loss: 2.1965191\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 4.8828125e-07\nEpoch: 13 cost time: 0.669335126876831\nEpoch: 13, Steps: 35 | Train Loss: 0.3874558 Vali Loss: 0.2099472 Test Loss: 2.1996067\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 2.44140625e-07\nEpoch: 14 cost time: 0.6600587368011475\nEpoch: 14, Steps: 35 | Train Loss: 0.3888449 Vali Loss: 0.2128004 Test Loss: 2.2033355\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 1.220703125e-07\nEpoch: 15 cost time: 0.6760661602020264\nEpoch: 15, Steps: 35 | Train Loss: 0.3861230 Vali Loss: 0.2137939 Test Loss: 2.1886394\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 6.103515625e-08\nEpoch: 16 cost time: 0.6867012977600098\nEpoch: 16, Steps: 35 | Train Loss: 0.3860401 Vali Loss: 0.2179797 Test Loss: 2.1915271\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 3.0517578125e-08\nEpoch: 17 cost time: 0.675748348236084\nEpoch: 17, Steps: 35 | Train Loss: 0.3906202 Vali Loss: 0.2274450 Test Loss: 2.2036214\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 1.52587890625e-08\nEpoch: 18 cost time: 0.6651911735534668\nEpoch: 18, Steps: 35 | Train Loss: 0.3873160 Vali Loss: 0.2082825 Test Loss: 2.1999879\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 7.62939453125e-09\nEpoch: 19 cost time: 0.6725051403045654\nEpoch: 19, Steps: 35 | Train Loss: 0.3882296 Vali Loss: 0.2076785 Test Loss: 2.1937034\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_ili_60_PatchTST_custom_ftM_sl60_ll48_pl60_dm128_nh16_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 134\ntest shape: (134, 60, 7) (134, 60, 7)\ntest shape: (134, 60, 7) (134, 60, 7)\nmse:1.9717161655426025, mae:0.9223715662956238, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/2655fb1a-85ca-425a-b77e-c3c403fc8fa6","content_dependencies":{"error":{"type":"SyntaxError","message":"invalid syntax (<unknown>, line 2)"},"codeHash":"3d3b0170","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"c424d4e0","execution_start":1744906074760,"execution_millis":931,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"73c529f3bef444f8a9d2f35b2a74c7d9","deepnote_cell_type":"code"},"source":"!mkdir -p result/Robustness/Hyperparameters/PatchTST_ili_learning_rate005","block_group":"3159920ab85a486c98ef958cf9d772d7","execution_count":27,"outputs":[],"outputs_reference":null,"content_dependencies":{"codeHash":"c424d4e0","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"c742d29d","execution_start":1744906323710,"execution_millis":76368,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"e947a664373c43348724b11c620c9dbc","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_ili_24_lr005 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/ILI \\\n--data_path national_illness.csv \\\n--features M \\\n--seq_len 60 \\\n--label_len 48 \\\n--pred_len 24 \\\n--e_layers 4 \\\n--d_layers 1 \\\n--factor 3 \\\n--enc_in 7 \\\n--n_heads 4 \\\n--d_model 128 \\\n--des benchmark_patchtst \\\n--itr 1 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--learning_rate 0.005 \\\n--checkpoints ./result/Robustness/Hyperparameters/PatchTST_ili_learning_rate005 \\\n2>&1 | tee result/Robustness/Hyperparameters/PatchTST_ili_learning_rate005/train_log.txt","block_group":"c11b74314a164637bc7cae84711451ec","execution_count":30,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_ili_24_lr005Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/ILI       \n  Data Path:          national_illness.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/Robustness/Hyperparameters/PatchTST_ili_learning_rate005\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            60                  Label Len:          48                  \n  Pred Len:           24                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             7                   Dec In:             7                   \n  C Out:              7                   d model:            128                 \n  n heads:            4                   e layers:           4                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             3                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.005               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_ili_24_lr005_PatchTST_custom_ftM_sl60_ll48_pl24_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 593\nval 74\ntest 170\nEpoch: 1 cost time: 3.7365314960479736\nEpoch: 1, Steps: 38 | Train Loss: 1.1402470 Vali Loss: 0.8012296 Test Loss: 4.1368141\nValidation loss decreased (inf --> 0.801230).  Saving model ...\nUpdating learning rate to 0.005\nEpoch: 2 cost time: 2.4897875785827637\nEpoch: 2, Steps: 38 | Train Loss: 0.9937736 Vali Loss: 0.8243659 Test Loss: 4.1389594\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0025\nEpoch: 3 cost time: 2.4982895851135254\nEpoch: 3, Steps: 38 | Train Loss: 0.9533818 Vali Loss: 0.7619025 Test Loss: 4.0832448\nValidation loss decreased (0.801230 --> 0.761902).  Saving model ...\nUpdating learning rate to 0.00125\nEpoch: 4 cost time: 2.474879503250122\nEpoch: 4, Steps: 38 | Train Loss: 0.9134141 Vali Loss: 0.7679933 Test Loss: 4.0792789\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.000625\nEpoch: 5 cost time: 2.4423394203186035\nEpoch: 5, Steps: 38 | Train Loss: 0.8969710 Vali Loss: 0.7795764 Test Loss: 4.0789227\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.0003125\nEpoch: 6 cost time: 2.2783443927764893\nEpoch: 6, Steps: 38 | Train Loss: 0.8946352 Vali Loss: 0.7882626 Test Loss: 4.0798192\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.00015625\nEpoch: 7 cost time: 2.472968578338623\nEpoch: 7, Steps: 38 | Train Loss: 0.8862733 Vali Loss: 0.7706280 Test Loss: 4.0779080\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 7.8125e-05\nEpoch: 8 cost time: 2.455056667327881\nEpoch: 8, Steps: 38 | Train Loss: 0.8956713 Vali Loss: 0.7776826 Test Loss: 4.0782633\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 3.90625e-05\nEpoch: 9 cost time: 2.435178756713867\nEpoch: 9, Steps: 38 | Train Loss: 0.9049572 Vali Loss: 0.7858615 Test Loss: 4.0786753\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 1.953125e-05\nEpoch: 10 cost time: 2.4682557582855225\nEpoch: 10, Steps: 38 | Train Loss: 0.8779297 Vali Loss: 0.7737028 Test Loss: 4.0777626\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 9.765625e-06\nEpoch: 11 cost time: 2.481043815612793\nEpoch: 11, Steps: 38 | Train Loss: 0.8879297 Vali Loss: 0.7628850 Test Loss: 4.0786200\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 4.8828125e-06\nEpoch: 12 cost time: 2.4235754013061523\nEpoch: 12, Steps: 38 | Train Loss: 0.8838533 Vali Loss: 0.7844138 Test Loss: 4.0779247\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 2.44140625e-06\nEpoch: 13 cost time: 2.136781930923462\nEpoch: 13, Steps: 38 | Train Loss: 1.0392530 Vali Loss: 0.7678111 Test Loss: 4.0790806\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_ili_24_lr005_PatchTST_custom_ftM_sl60_ll48_pl24_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 170\ntest shape: (170, 24, 7) (170, 24, 7)\ntest shape: (170, 24, 7) (170, 24, 7)\nmse:4.148509502410889, mae:1.5063292980194092, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/0003ff12-078c-437e-859e-d6582b6c16a1","content_dependencies":{"error":{"type":"SyntaxError","message":"invalid syntax (<unknown>, line 2)"},"codeHash":"c742d29d","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"7b0914ae","execution_start":1744906810791,"execution_millis":70723,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"78d5ad5e6f724ab59edbbd17108115a2","deepnote_cell_type":"code"},"source":"!mkdir -p result/Robustness/Hyperparameters/PatchTST_ili_learning_rate0005\n\n!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_ili_24_lr0005 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/ILI \\\n--data_path national_illness.csv \\\n--features M \\\n--seq_len 60 \\\n--label_len 48 \\\n--pred_len 24 \\\n--e_layers 4 \\\n--d_layers 1 \\\n--factor 3 \\\n--enc_in 7 \\\n--n_heads 4 \\\n--d_model 128 \\\n--des benchmark_patchtst \\\n--itr 1 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--learning_rate 0.0005 \\\n--checkpoints ./result/Robustness/Hyperparameters/PatchTST_ili_learning_rate0005 \\\n2>&1 | tee result/Robustness/Hyperparameters/PatchTST_ili_learning_rate0005/train_log.txt","block_group":"77966a193ff24d0e854431fbc0f8ed9f","execution_count":33,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_ili_24_lr0005Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/ILI       \n  Data Path:          national_illness.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/Robustness/Hyperparameters/PatchTST_ili_learning_rate0005\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            60                  Label Len:          48                  \n  Pred Len:           24                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             7                   Dec In:             7                   \n  C Out:              7                   d model:            128                 \n  n heads:            4                   e layers:           4                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             3                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.0005              \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_ili_24_lr0005_PatchTST_custom_ftM_sl60_ll48_pl24_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 593\nval 74\ntest 170\nEpoch: 1 cost time: 3.6678214073181152\nEpoch: 1, Steps: 38 | Train Loss: 0.6256047 Vali Loss: 0.3165594 Test Loss: 2.2285070\nValidation loss decreased (inf --> 0.316559).  Saving model ...\nUpdating learning rate to 0.0005\nEpoch: 2 cost time: 2.4546897411346436\nEpoch: 2, Steps: 38 | Train Loss: 0.4577322 Vali Loss: 0.2450548 Test Loss: 2.2252667\nValidation loss decreased (0.316559 --> 0.245055).  Saving model ...\nUpdating learning rate to 0.00025\nEpoch: 3 cost time: 2.472121238708496\nEpoch: 3, Steps: 38 | Train Loss: 0.4023877 Vali Loss: 0.2155671 Test Loss: 2.0399511\nValidation loss decreased (0.245055 --> 0.215567).  Saving model ...\nUpdating learning rate to 0.000125\nEpoch: 4 cost time: 2.44209885597229\nEpoch: 4, Steps: 38 | Train Loss: 0.3703010 Vali Loss: 0.1835175 Test Loss: 1.9402261\nValidation loss decreased (0.215567 --> 0.183518).  Saving model ...\nUpdating learning rate to 6.25e-05\nEpoch: 5 cost time: 2.4927446842193604\nEpoch: 5, Steps: 38 | Train Loss: 0.3552600 Vali Loss: 0.2057126 Test Loss: 1.9545054\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 3.125e-05\nEpoch: 6 cost time: 2.4725801944732666\nEpoch: 6, Steps: 38 | Train Loss: 0.3452154 Vali Loss: 0.2031541 Test Loss: 1.9239440\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 1.5625e-05\nEpoch: 7 cost time: 2.4767038822174072\nEpoch: 7, Steps: 38 | Train Loss: 0.3459848 Vali Loss: 0.1981068 Test Loss: 1.9199475\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 7.8125e-06\nEpoch: 8 cost time: 2.441457748413086\nEpoch: 8, Steps: 38 | Train Loss: 0.3422348 Vali Loss: 0.2009811 Test Loss: 1.9256345\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 3.90625e-06\nEpoch: 9 cost time: 2.468078136444092\nEpoch: 9, Steps: 38 | Train Loss: 0.3462333 Vali Loss: 0.2046474 Test Loss: 1.9128238\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 1.953125e-06\nEpoch: 10 cost time: 2.4678757190704346\nEpoch: 10, Steps: 38 | Train Loss: 0.3348823 Vali Loss: 0.1985035 Test Loss: 1.9064946\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 9.765625e-07\nEpoch: 11 cost time: 2.468937873840332\nEpoch: 11, Steps: 38 | Train Loss: 0.3478750 Vali Loss: 0.1982472 Test Loss: 1.8987559\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 4.8828125e-07\nEpoch: 12 cost time: 2.466637372970581\nEpoch: 12, Steps: 38 | Train Loss: 0.3351168 Vali Loss: 0.1994626 Test Loss: 1.8977218\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 2.44140625e-07\nEpoch: 13 cost time: 2.4817326068878174\nEpoch: 13, Steps: 38 | Train Loss: 0.4429251 Vali Loss: 0.1958948 Test Loss: 1.8898692\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 1.220703125e-07\nEpoch: 14 cost time: 2.462836503982544\nEpoch: 14, Steps: 38 | Train Loss: 0.3383233 Vali Loss: 0.1952041 Test Loss: 1.8983182\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_ili_24_lr0005_PatchTST_custom_ftM_sl60_ll48_pl24_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 170\ntest shape: (170, 24, 7) (170, 24, 7)\ntest shape: (170, 24, 7) (170, 24, 7)\nmse:1.7538663148880005, mae:0.8276285529136658, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/db3bc2a0-bd9d-49ef-ac45-b798a700c3cb","content_dependencies":{"error":{"type":"SyntaxError","message":"invalid syntax (<unknown>, line 4)"},"codeHash":"7b0914ae","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"5a7f11fc","execution_start":1744907073380,"execution_millis":109625,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"ceb13bb7ef764efa8cf49e44c9e74aae","deepnote_cell_type":"code"},"source":"!mkdir -p result/Robustness/Hyperparameters/PatchTST_ili_learning_rate0001\n\n!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_ili_24_lr0001 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/ILI \\\n--data_path national_illness.csv \\\n--features M \\\n--seq_len 60 \\\n--label_len 48 \\\n--pred_len 24 \\\n--e_layers 4 \\\n--d_layers 1 \\\n--factor 3 \\\n--enc_in 7 \\\n--n_heads 4 \\\n--d_model 128 \\\n--des benchmark_patchtst \\\n--itr 1 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--learning_rate 0.0001 \\\n--checkpoints ./result/Robustness/Hyperparameters/PatchTST_ili_learning_rate0001 \\\n2>&1 | tee result/Robustness/Hyperparameters/PatchTST_ili_learning_rate0001/train_log.txt","block_group":"22ff77ab492742fa80b9c266b9431500","execution_count":36,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_ili_24_lr0001Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/ILI       \n  Data Path:          national_illness.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/Robustness/Hyperparameters/PatchTST_ili_learning_rate0001\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            60                  Label Len:          48                  \n  Pred Len:           24                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             7                   Dec In:             7                   \n  C Out:              7                   d model:            128                 \n  n heads:            4                   e layers:           4                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             3                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.0001              \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_ili_24_lr0001_PatchTST_custom_ftM_sl60_ll48_pl24_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 593\nval 74\ntest 170\nEpoch: 1 cost time: 3.705181837081909\nEpoch: 1, Steps: 38 | Train Loss: 0.7274435 Vali Loss: 0.3168508 Test Loss: 2.3567393\nValidation loss decreased (inf --> 0.316851).  Saving model ...\nUpdating learning rate to 0.0001\nEpoch: 2 cost time: 2.4632656574249268\nEpoch: 2, Steps: 38 | Train Loss: 0.5479904 Vali Loss: 0.3149780 Test Loss: 2.2260339\nValidation loss decreased (0.316851 --> 0.314978).  Saving model ...\nUpdating learning rate to 5e-05\nEpoch: 3 cost time: 2.4742512702941895\nEpoch: 3, Steps: 38 | Train Loss: 0.4959869 Vali Loss: 0.2791426 Test Loss: 2.1093214\nValidation loss decreased (0.314978 --> 0.279143).  Saving model ...\nUpdating learning rate to 2.5e-05\nEpoch: 4 cost time: 2.4869229793548584\nEpoch: 4, Steps: 38 | Train Loss: 0.4771619 Vali Loss: 0.2597445 Test Loss: 2.1182296\nValidation loss decreased (0.279143 --> 0.259744).  Saving model ...\nUpdating learning rate to 1.25e-05\nEpoch: 5 cost time: 2.4261393547058105\nEpoch: 5, Steps: 38 | Train Loss: 0.4679128 Vali Loss: 0.2742909 Test Loss: 2.0855894\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 6.25e-06\nEpoch: 6 cost time: 2.479161262512207\nEpoch: 6, Steps: 38 | Train Loss: 0.4636085 Vali Loss: 0.2728949 Test Loss: 2.0833871\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 3.125e-06\nEpoch: 7 cost time: 2.4585769176483154\nEpoch: 7, Steps: 38 | Train Loss: 0.4625286 Vali Loss: 0.2634538 Test Loss: 2.0857766\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 1.5625e-06\nEpoch: 8 cost time: 2.4873440265655518\nEpoch: 8, Steps: 38 | Train Loss: 0.4717890 Vali Loss: 0.2648367 Test Loss: 2.0823181\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 7.8125e-07\nEpoch: 9 cost time: 2.491516590118408\nEpoch: 9, Steps: 38 | Train Loss: 0.4669832 Vali Loss: 0.2700276 Test Loss: 2.0818145\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 3.90625e-07\nEpoch: 10 cost time: 2.4674158096313477\nEpoch: 10, Steps: 38 | Train Loss: 0.4547107 Vali Loss: 0.2637171 Test Loss: 2.0790207\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 1.953125e-07\nEpoch: 11 cost time: 2.449045419692993\nEpoch: 11, Steps: 38 | Train Loss: 0.4699191 Vali Loss: 0.2606220 Test Loss: 2.0791800\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 9.765625e-08\nEpoch: 12 cost time: 2.4563112258911133\nEpoch: 12, Steps: 38 | Train Loss: 0.4569446 Vali Loss: 0.2715020 Test Loss: 2.0761361\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 4.8828125e-08\nEpoch: 13 cost time: 2.4511399269104004\nEpoch: 13, Steps: 38 | Train Loss: 0.6094680 Vali Loss: 0.2643374 Test Loss: 2.0674157\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 2.44140625e-08\nEpoch: 14 cost time: 2.490837812423706\nEpoch: 14, Steps: 38 | Train Loss: 0.4592484 Vali Loss: 0.2590491 Test Loss: 2.0730443\nValidation loss decreased (0.259744 --> 0.259049).  Saving model ...\nUpdating learning rate to 1.220703125e-08\nEpoch: 15 cost time: 2.475911855697632\nEpoch: 15, Steps: 38 | Train Loss: 0.4570751 Vali Loss: 0.2561495 Test Loss: 2.0860553\nValidation loss decreased (0.259049 --> 0.256150).  Saving model ...\nUpdating learning rate to 6.103515625e-09\nEpoch: 16 cost time: 2.4838802814483643\nEpoch: 16, Steps: 38 | Train Loss: 0.4637844 Vali Loss: 0.2693052 Test Loss: 2.0761600\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 3.0517578125e-09\nEpoch: 17 cost time: 2.459141969680786\nEpoch: 17, Steps: 38 | Train Loss: 0.4659475 Vali Loss: 0.2773032 Test Loss: 2.0804675\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 1.52587890625e-09\nEpoch: 18 cost time: 2.4726295471191406\nEpoch: 18, Steps: 38 | Train Loss: 0.4674936 Vali Loss: 0.2702116 Test Loss: 2.0800333\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 7.62939453125e-10\nEpoch: 19 cost time: 2.4713306427001953\nEpoch: 19, Steps: 38 | Train Loss: 0.4570855 Vali Loss: 0.2606191 Test Loss: 2.0828371\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 3.814697265625e-10\nEpoch: 20 cost time: 2.455925464630127\nEpoch: 20, Steps: 38 | Train Loss: 0.4587695 Vali Loss: 0.2636327 Test Loss: 2.0797327\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 1.9073486328125e-10\nEpoch: 21 cost time: 2.450263500213623\nEpoch: 21, Steps: 38 | Train Loss: 0.4568360 Vali Loss: 0.2620812 Test Loss: 2.0737374\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 9.5367431640625e-11\nEpoch: 22 cost time: 2.462203025817871\nEpoch: 22, Steps: 38 | Train Loss: 0.4573950 Vali Loss: 0.2704357 Test Loss: 2.0801346\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 4.76837158203125e-11\nEpoch: 23 cost time: 2.474078416824341\nEpoch: 23, Steps: 38 | Train Loss: 0.4699255 Vali Loss: 0.2649710 Test Loss: 2.0813811\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 2.384185791015625e-11\nEpoch: 24 cost time: 2.4689159393310547\nEpoch: 24, Steps: 38 | Train Loss: 0.4678052 Vali Loss: 0.2678024 Test Loss: 2.0809348\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 1.1920928955078126e-11\nEpoch: 25 cost time: 2.4568397998809814\nEpoch: 25, Steps: 38 | Train Loss: 0.4837645 Vali Loss: 0.2597019 Test Loss: 2.0763319\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_ili_24_lr0001_PatchTST_custom_ftM_sl60_ll48_pl24_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 170\ntest shape: (170, 24, 7) (170, 24, 7)\ntest shape: (170, 24, 7) (170, 24, 7)\nmse:1.972815752029419, mae:0.8847302198410034, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/08a5a14b-4c8e-4f60-b753-b69f78fa5a3b","content_dependencies":{"error":{"type":"SyntaxError","message":"invalid syntax (<unknown>, line 4)"},"codeHash":"5a7f11fc","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"code","metadata":{"source_hash":"ea131809","execution_start":1744907314180,"execution_millis":70190,"execution_context_id":"3599fef3-ff9d-463c-b3b9-d570c969d784","cell_id":"39089f07fc1f4bd9acc9cff4a22b6f39","deepnote_cell_type":"code"},"source":"!mkdir -p result/Robustness/Hyperparameters/PatchTST_ili_batchsize32\n\n!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_ili_24_bs32 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/ILI \\\n--data_path national_illness.csv \\\n--features M \\\n--seq_len 60 \\\n--label_len 48 \\\n--pred_len 24 \\\n--e_layers 4 \\\n--d_layers 1 \\\n--factor 3 \\\n--enc_in 7 \\\n--n_heads 4 \\\n--d_model 128 \\\n--des benchmark_patchtst \\\n--itr 1 \\\n--train_epochs 50 \\\n--batch_size 32 \\\n--patience 10 \\\n--learning_rate 0.001 \\\n--checkpoints ./result/Robustness/Hyperparameters/PatchTST_ili_batchsize32 \\\n2>&1 | tee result/Robustness/Hyperparameters/PatchTST_ili_batchsize32/train_log.txt","block_group":"4ddf976092b248bfa2af106b00c80fad","execution_count":39,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_ili_24_bs32Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/ILI       \n  Data Path:          national_illness.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/Robustness/Hyperparameters/PatchTST_ili_batchsize32\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            60                  Label Len:          48                  \n  Pred Len:           24                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             7                   Dec In:             7                   \n  C Out:              7                   d model:            128                 \n  n heads:            4                   e layers:           4                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             3                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         32                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_ili_24_bs32_PatchTST_custom_ftM_sl60_ll48_pl24_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 593\nval 74\ntest 170\nEpoch: 1 cost time: 3.212824583053589\nEpoch: 1, Steps: 19 | Train Loss: 0.6776935 Vali Loss: 0.2916422 Test Loss: 2.4274108\nValidation loss decreased (inf --> 0.291642).  Saving model ...\nUpdating learning rate to 0.001\nEpoch: 2 cost time: 2.0090675354003906\nEpoch: 2, Steps: 19 | Train Loss: 0.4944349 Vali Loss: 0.2571128 Test Loss: 2.6032035\nValidation loss decreased (0.291642 --> 0.257113).  Saving model ...\nUpdating learning rate to 0.0005\nEpoch: 3 cost time: 2.0243637561798096\nEpoch: 3, Steps: 19 | Train Loss: 0.4241110 Vali Loss: 0.2385609 Test Loss: 2.2441046\nValidation loss decreased (0.257113 --> 0.238561).  Saving model ...\nUpdating learning rate to 0.00025\nEpoch: 4 cost time: 2.026449203491211\nEpoch: 4, Steps: 19 | Train Loss: 0.3951978 Vali Loss: 0.1964661 Test Loss: 2.4418848\nValidation loss decreased (0.238561 --> 0.196466).  Saving model ...\nUpdating learning rate to 0.000125\nEpoch: 5 cost time: 2.000783681869507\nEpoch: 5, Steps: 19 | Train Loss: 0.3597798 Vali Loss: 0.2129709 Test Loss: 2.1343937\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 6.25e-05\nEpoch: 6 cost time: 2.036700487136841\nEpoch: 6, Steps: 19 | Train Loss: 0.3583985 Vali Loss: 0.2157882 Test Loss: 2.2066023\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 3.125e-05\nEpoch: 7 cost time: 2.008812427520752\nEpoch: 7, Steps: 19 | Train Loss: 0.3545141 Vali Loss: 0.1965518 Test Loss: 2.2973654\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 1.5625e-05\nEpoch: 8 cost time: 2.012340545654297\nEpoch: 8, Steps: 19 | Train Loss: 0.3512885 Vali Loss: 0.2074528 Test Loss: 2.3431313\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 7.8125e-06\nEpoch: 9 cost time: 2.0399346351623535\nEpoch: 9, Steps: 19 | Train Loss: 0.3433732 Vali Loss: 0.2237290 Test Loss: 2.3310950\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 3.90625e-06\nEpoch: 10 cost time: 2.051974058151245\nEpoch: 10, Steps: 19 | Train Loss: 0.3435233 Vali Loss: 0.2045135 Test Loss: 2.3311675\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 1.953125e-06\nEpoch: 11 cost time: 2.0132222175598145\nEpoch: 11, Steps: 19 | Train Loss: 0.3488477 Vali Loss: 0.2090837 Test Loss: 2.3111115\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 9.765625e-07\nEpoch: 12 cost time: 2.162442445755005\nEpoch: 12, Steps: 19 | Train Loss: 0.3479351 Vali Loss: 0.2091525 Test Loss: 2.2990587\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 4.8828125e-07\nEpoch: 13 cost time: 2.0883829593658447\nEpoch: 13, Steps: 19 | Train Loss: 0.3464021 Vali Loss: 0.2109269 Test Loss: 2.3067372\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 2.44140625e-07\nEpoch: 14 cost time: 2.117462158203125\nEpoch: 14, Steps: 19 | Train Loss: 0.3510802 Vali Loss: 0.1972484 Test Loss: 2.3274033\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_ili_24_bs32_PatchTST_custom_ftM_sl60_ll48_pl24_dm128_nh4_el4_dl1_df2048_expand2_dc4_fc3_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 170\ntest shape: (170, 24, 7) (170, 24, 7)\ntest shape: (170, 24, 7) (170, 24, 7)\nmse:1.9081417322158813, mae:0.8676585555076599, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/f10ca28b-81da-49d3-91c3-62fe8262abc3","content_dependencies":{"error":{"type":"SyntaxError","message":"invalid syntax (<unknown>, line 4)"},"codeHash":"ea131809","usedVariables":[],"importedModules":[],"definedVariables":[]}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=25aca1a4-5304-4528-85ec-154f53dfeb1c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"793451dadad4416b8d5ad7e2252dd9cd"}}