{"cells":[{"cell_type":"code","metadata":{"source_hash":"1acff9c4","execution_start":1744871933860,"execution_millis":73,"execution_context_id":"71545753-af2c-4f73-a72a-1e25f94f8ba5","cell_id":"9130d872abfc4c06983ee5c9ef86bba8","deepnote_cell_type":"code"},"source":"%cd Time-Series-Library\n!pwd","block_group":"9130d872abfc4c06983ee5c9ef86bba8","execution_count":1,"outputs":[{"name":"stdout","text":"/datasets/_deepnote_work/Time-Series-Library\n/datasets/_deepnote_work/Time-Series-Library\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/31bf8b18-bbc4-421a-8a4e-9b78052eb99b","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"7958d4a9","execution_start":1744871933990,"execution_millis":15622,"execution_context_id":"71545753-af2c-4f73-a72a-1e25f94f8ba5","cell_id":"1840781c3e8442efa8f60fb1c0feec91","deepnote_cell_type":"code"},"source":"!pip install -r requirements.txt","block_group":"4d1156cf099c44b3b8340f055018b3bd","execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: einops==0.8.0 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.8.0)\nRequirement already satisfied: local-attention==1.9.14 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.9.14)\nRequirement already satisfied: matplotlib==3.7.0 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.7.0)\nRequirement already satisfied: numpy==1.23.5 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.23.5)\nRequirement already satisfied: pandas==1.5.3 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.5.3)\nRequirement already satisfied: patool==1.12 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.12)\nRequirement already satisfied: reformer-pytorch==1.4.4 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.4.4)\nRequirement already satisfied: scikit-learn==1.2.2 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (1.2.2)\nRequirement already satisfied: scipy==1.10.1 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (1.10.1)\nRequirement already satisfied: sktime==0.16.1 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (0.16.1)\nRequirement already satisfied: sympy==1.11.1 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.11.1)\nCollecting torch==1.7.1\n  Using cached torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\nRequirement already satisfied: tqdm==4.64.1 in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (4.64.1)\nRequirement already satisfied: PyWavelets in /root/venv/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (1.4.1)\nRequirement already satisfied: cycler>=0.10 in /root/venv/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (0.12.1)\nRequirement already satisfied: packaging>=20.0 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (24.1)\nRequirement already satisfied: pillow>=6.2.0 in /root/venv/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (10.4.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /root/venv/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (3.1.4)\nRequirement already satisfied: contourpy>=1.0.1 in /root/venv/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (1.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (2.9.0.post0)\nRequirement already satisfied: fonttools>=4.22.0 in /root/venv/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /root/venv/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (1.4.7)\nRequirement already satisfied: importlib-resources>=3.2.0 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from matplotlib==3.7.0->-r requirements.txt (line 3)) (6.4.4)\nRequirement already satisfied: pytz>=2020.1 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2024.1)\nRequirement already satisfied: product-key-memory in /root/venv/lib/python3.8/site-packages (from reformer-pytorch==1.4.4->-r requirements.txt (line 7)) (0.2.2)\nRequirement already satisfied: axial-positional-embedding>=0.1.0 in /root/venv/lib/python3.8/site-packages (from reformer-pytorch==1.4.4->-r requirements.txt (line 7)) (0.2.1)\nRequirement already satisfied: joblib>=1.1.1 in /root/venv/lib/python3.8/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 8)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /root/venv/lib/python3.8/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 8)) (3.5.0)\nRequirement already satisfied: numba>=0.53 in /root/venv/lib/python3.8/site-packages (from sktime==0.16.1->-r requirements.txt (line 10)) (0.58.1)\nRequirement already satisfied: deprecated>=1.2.13 in /root/venv/lib/python3.8/site-packages (from sktime==0.16.1->-r requirements.txt (line 10)) (1.2.18)\nRequirement already satisfied: mpmath>=0.19 in /root/venv/lib/python3.8/site-packages (from sympy==1.11.1->-r requirements.txt (line 11)) (1.3.0)\nRequirement already satisfied: typing-extensions in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from torch==1.7.1->-r requirements.txt (line 12)) (4.12.2)\nRequirement already satisfied: wrapt<2,>=1.10 in /root/venv/lib/python3.8/site-packages (from deprecated>=1.2.13->sktime==0.16.1->-r requirements.txt (line 10)) (1.17.2)\nRequirement already satisfied: zipp>=3.1.0 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib==3.7.0->-r requirements.txt (line 3)) (3.20.1)\nRequirement already satisfied: importlib-metadata in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from numba>=0.53->sktime==0.16.1->-r requirements.txt (line 10)) (8.4.0)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /root/venv/lib/python3.8/site-packages (from numba>=0.53->sktime==0.16.1->-r requirements.txt (line 10)) (0.41.1)\nRequirement already satisfied: six>=1.5 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib==3.7.0->-r requirements.txt (line 3)) (1.16.0)\n\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: torch\n\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.7.1 which is incompatible.\ntorchaudio 2.0.2+rocm5.4.2 requires torch==2.0.1, but you have torch 1.7.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.7.1\n\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/03b8bf88-217c-48eb-b3f8-fb242356eda4","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"35ccf442","execution_start":1744871949660,"execution_millis":75505,"execution_context_id":"71545753-af2c-4f73-a72a-1e25f94f8ba5","cell_id":"4b72c378543c433f88ef5d9a620cd3f3","deepnote_cell_type":"code"},"source":"# ✅ Step 1: 卸载当前 torch\n!pip uninstall torch -y\n\n# ✅ Step 2: 安装适配新架构的 torch（以 CUDA 11.8 为例）\n!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 -f https://download.pytorch.org/whl/torch_stable.html","block_group":"747f80d484674337ad2b7541b12a9344","execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: torch 1.7.1\nUninstalling torch-1.7.1:\n  Successfully uninstalled torch-1.7.1\n\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0mLooking in links: https://download.pytorch.org/whl/torch_stable.html\nCollecting torch==2.0.1+cu118\n  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp38-cp38-linux_x86_64.whl (2267.3 MB)\nRequirement already satisfied: torchvision==0.15.2+cu118 in /root/venv/lib/python3.8/site-packages (0.15.2+cu118)\nRequirement already satisfied: torchaudio==2.0.2 in /root/venv/lib/python3.8/site-packages (2.0.2+rocm5.4.2)\nRequirement already satisfied: triton==2.0.0 in /root/venv/lib/python3.8/site-packages (from torch==2.0.1+cu118) (2.0.0)\nRequirement already satisfied: typing-extensions in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from torch==2.0.1+cu118) (4.12.2)\nRequirement already satisfied: filelock in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from torch==2.0.1+cu118) (3.15.4)\nRequirement already satisfied: networkx in /root/venv/lib/python3.8/site-packages (from torch==2.0.1+cu118) (3.1)\nRequirement already satisfied: sympy in /root/venv/lib/python3.8/site-packages (from torch==2.0.1+cu118) (1.11.1)\nRequirement already satisfied: jinja2 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from torch==2.0.1+cu118) (3.0.3)\nRequirement already satisfied: requests in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from torchvision==0.15.2+cu118) (2.32.3)\nRequirement already satisfied: numpy in /root/venv/lib/python3.8/site-packages (from torchvision==0.15.2+cu118) (1.23.5)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/venv/lib/python3.8/site-packages (from torchvision==0.15.2+cu118) (10.4.0)\nRequirement already satisfied: cmake in /root/venv/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.1+cu118) (4.0.0)\nRequirement already satisfied: lit in /root/venv/lib/python3.8/site-packages (from triton==2.0.0->torch==2.0.1+cu118) (18.1.8)\nRequirement already satisfied: MarkupSafe>=2.0 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from jinja2->torch==2.0.1+cu118) (2.1.5)\nRequirement already satisfied: certifi>=2017.4.17 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from requests->torchvision==0.15.2+cu118) (2024.8.30)\nRequirement already satisfied: charset-normalizer<4,>=2 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from requests->torchvision==0.15.2+cu118) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from requests->torchvision==0.15.2+cu118) (3.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /toolkit-cache/0.2.14/python3.8/kernel-libs/lib/python3.8/site-packages (from requests->torchvision==0.15.2+cu118) (1.26.20)\nRequirement already satisfied: mpmath>=0.19 in /root/venv/lib/python3.8/site-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: torch\n\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0mSuccessfully installed torch-2.0.1+cu118\n\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/root/venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/e8a76053-7e3a-47b9-af27-464268268f1b","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"711c6658","execution_start":1744872025210,"execution_millis":475,"execution_context_id":"71545753-af2c-4f73-a72a-1e25f94f8ba5","cell_id":"04e8ed36742347c79b6ba9230ed7bf0b","deepnote_cell_type":"code"},"source":"!mkdir -p result/PatchTST_exchange_96\n!mkdir -p result/PatchTST_exchange_192\n!mkdir -p result/PatchTST_exchange_336\n!mkdir -p result/PatchTST_exchange_720","block_group":"f5886ef1d45442328c9b7e5cba6d30b2","execution_count":4,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"e27730eb","execution_start":1744872025750,"execution_millis":162739,"execution_context_id":"71545753-af2c-4f73-a72a-1e25f94f8ba5","cell_id":"f693affa68684ed0bb973a5d62c1ed1d","deepnote_cell_type":"code"},"source":"!mkdir -p result/PatchTST_exchange_96\n!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_exchange_96 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/Exchange \\\n--data_path exchange_rate.csv \\\n--features M \\\n--seq_len 336 \\\n--pred_len 96 \\\n--e_layers 2 \\\n--d_layers 1 \\\n--enc_in 8 \\\n--d_model 256 \\\n--learning_rate 0.001 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--checkpoints ./result/PatchTST_exchange_96 \\\n--des benchmark_patchtst \\\n2>&1 | tee result/PatchTST_exchange_96/train_log.txt","block_group":"87c4f33ceec0446293a1150eea45d2da","execution_count":5,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_exchange_96Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/Exchange  \n  Data Path:          exchange_rate.csv   Features:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/PatchTST_exchange_96\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            336                 Label Len:          48                  \n  Pred Len:           96                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             8                   Dec In:             7                   \n  C Out:              7                   d model:            256                 \n  n heads:            8                   e layers:           2                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             1                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_exchange_96_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4880\nval 665\ntest 1422\n\titers: 100, epoch: 1 | loss: 0.2797022\n\tspeed: 0.0337s/iter; left time: 510.8529s\n\titers: 200, epoch: 1 | loss: 0.3117331\n\tspeed: 0.0264s/iter; left time: 397.4728s\n\titers: 300, epoch: 1 | loss: 0.2289656\n\tspeed: 0.0264s/iter; left time: 394.5718s\nEpoch: 1 cost time: 8.805248022079468\nEpoch: 1, Steps: 305 | Train Loss: 0.2316765 Vali Loss: 0.1905573 Test Loss: 0.1152264\nValidation loss decreased (inf --> 0.190557).  Saving model ...\nUpdating learning rate to 0.001\n\titers: 100, epoch: 2 | loss: 0.3305613\n\tspeed: 0.0470s/iter; left time: 697.8923s\n\titers: 200, epoch: 2 | loss: 0.1624169\n\tspeed: 0.0265s/iter; left time: 390.5923s\n\titers: 300, epoch: 2 | loss: 0.2374402\n\tspeed: 0.0265s/iter; left time: 387.7519s\nEpoch: 2 cost time: 8.266701936721802\nEpoch: 2, Steps: 305 | Train Loss: 0.2050750 Vali Loss: 0.2586085 Test Loss: 0.2005388\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0005\n\titers: 100, epoch: 3 | loss: 0.1958217\n\tspeed: 0.0417s/iter; left time: 606.5833s\n\titers: 200, epoch: 3 | loss: 0.2158316\n\tspeed: 0.0265s/iter; left time: 382.5834s\n\titers: 300, epoch: 3 | loss: 0.1725123\n\tspeed: 0.0266s/iter; left time: 380.8410s\nEpoch: 3 cost time: 8.252257823944092\nEpoch: 3, Steps: 305 | Train Loss: 0.1765299 Vali Loss: 0.1705840 Test Loss: 0.1120700\nValidation loss decreased (0.190557 --> 0.170584).  Saving model ...\nUpdating learning rate to 0.00025\n\titers: 100, epoch: 4 | loss: 0.1136662\n\tspeed: 0.0493s/iter; left time: 701.4951s\n\titers: 200, epoch: 4 | loss: 0.2078013\n\tspeed: 0.0266s/iter; left time: 376.0776s\n\titers: 300, epoch: 4 | loss: 0.1100320\n\tspeed: 0.0266s/iter; left time: 373.2704s\nEpoch: 4 cost time: 8.288252353668213\nEpoch: 4, Steps: 305 | Train Loss: 0.1440408 Vali Loss: 0.1570344 Test Loss: 0.1000959\nValidation loss decreased (0.170584 --> 0.157034).  Saving model ...\nUpdating learning rate to 0.000125\n\titers: 100, epoch: 5 | loss: 0.1543108\n\tspeed: 0.0485s/iter; left time: 675.2597s\n\titers: 200, epoch: 5 | loss: 0.1532877\n\tspeed: 0.0267s/iter; left time: 369.2297s\n\titers: 300, epoch: 5 | loss: 0.1117727\n\tspeed: 0.0267s/iter; left time: 366.1790s\nEpoch: 5 cost time: 8.32396912574768\nEpoch: 5, Steps: 305 | Train Loss: 0.1316793 Vali Loss: 0.1662845 Test Loss: 0.1056175\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 6.25e-05\n\titers: 100, epoch: 6 | loss: 0.1647909\n\tspeed: 0.0421s/iter; left time: 573.4201s\n\titers: 200, epoch: 6 | loss: 0.1028010\n\tspeed: 0.0267s/iter; left time: 361.7504s\n\titers: 300, epoch: 6 | loss: 0.0933078\n\tspeed: 0.0267s/iter; left time: 358.2554s\nEpoch: 6 cost time: 8.305123090744019\nEpoch: 6, Steps: 305 | Train Loss: 0.1251788 Vali Loss: 0.1707984 Test Loss: 0.1064762\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 3.125e-05\n\titers: 100, epoch: 7 | loss: 0.1459105\n\tspeed: 0.0422s/iter; left time: 562.2914s\n\titers: 200, epoch: 7 | loss: 0.1357875\n\tspeed: 0.0268s/iter; left time: 353.7133s\n\titers: 300, epoch: 7 | loss: 0.1215752\n\tspeed: 0.0267s/iter; left time: 349.9224s\nEpoch: 7 cost time: 8.322712659835815\nEpoch: 7, Steps: 305 | Train Loss: 0.1214725 Vali Loss: 0.1681954 Test Loss: 0.1039875\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 1.5625e-05\n\titers: 100, epoch: 8 | loss: 0.0987416\n\tspeed: 0.0419s/iter; left time: 545.8613s\n\titers: 200, epoch: 8 | loss: 0.1018020\n\tspeed: 0.0268s/iter; left time: 346.1770s\n\titers: 300, epoch: 8 | loss: 0.1538139\n\tspeed: 0.0268s/iter; left time: 343.8501s\nEpoch: 8 cost time: 8.346492767333984\nEpoch: 8, Steps: 305 | Train Loss: 0.1201969 Vali Loss: 0.1702272 Test Loss: 0.1014135\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 7.8125e-06\n\titers: 100, epoch: 9 | loss: 0.1382400\n\tspeed: 0.0425s/iter; left time: 539.9334s\n\titers: 200, epoch: 9 | loss: 0.1311439\n\tspeed: 0.0268s/iter; left time: 338.2844s\n\titers: 300, epoch: 9 | loss: 0.0981884\n\tspeed: 0.0268s/iter; left time: 335.2558s\nEpoch: 9 cost time: 8.375356435775757\nEpoch: 9, Steps: 305 | Train Loss: 0.1187083 Vali Loss: 0.1662668 Test Loss: 0.1015388\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 3.90625e-06\n\titers: 100, epoch: 10 | loss: 0.0913368\n\tspeed: 0.0426s/iter; left time: 528.2637s\n\titers: 200, epoch: 10 | loss: 0.1204346\n\tspeed: 0.0269s/iter; left time: 331.6443s\n\titers: 300, epoch: 10 | loss: 0.1106851\n\tspeed: 0.0269s/iter; left time: 328.5523s\nEpoch: 10 cost time: 8.404583930969238\nEpoch: 10, Steps: 305 | Train Loss: 0.1183892 Vali Loss: 0.1715428 Test Loss: 0.1018486\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 1.953125e-06\n\titers: 100, epoch: 11 | loss: 0.1223027\n\tspeed: 0.0426s/iter; left time: 515.4403s\n\titers: 200, epoch: 11 | loss: 0.1165184\n\tspeed: 0.0270s/iter; left time: 324.3731s\n\titers: 300, epoch: 11 | loss: 0.0936194\n\tspeed: 0.0271s/iter; left time: 322.0759s\nEpoch: 11 cost time: 8.418799877166748\nEpoch: 11, Steps: 305 | Train Loss: 0.1182759 Vali Loss: 0.1720134 Test Loss: 0.1010484\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 9.765625e-07\n\titers: 100, epoch: 12 | loss: 0.1339705\n\tspeed: 0.0425s/iter; left time: 501.5377s\n\titers: 200, epoch: 12 | loss: 0.0994975\n\tspeed: 0.0271s/iter; left time: 316.9634s\n\titers: 300, epoch: 12 | loss: 0.0867700\n\tspeed: 0.0271s/iter; left time: 314.1371s\nEpoch: 12 cost time: 8.437158107757568\nEpoch: 12, Steps: 305 | Train Loss: 0.1182016 Vali Loss: 0.1702670 Test Loss: 0.1009420\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 4.8828125e-07\n\titers: 100, epoch: 13 | loss: 0.1068375\n\tspeed: 0.0429s/iter; left time: 492.6525s\n\titers: 200, epoch: 13 | loss: 0.0875161\n\tspeed: 0.0271s/iter; left time: 309.1377s\n\titers: 300, epoch: 13 | loss: 0.1282700\n\tspeed: 0.0271s/iter; left time: 305.9329s\nEpoch: 13 cost time: 8.45158314704895\nEpoch: 13, Steps: 305 | Train Loss: 0.1185476 Vali Loss: 0.1693683 Test Loss: 0.1013374\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 2.44140625e-07\n\titers: 100, epoch: 14 | loss: 0.0809036\n\tspeed: 0.0429s/iter; left time: 480.0285s\n\titers: 200, epoch: 14 | loss: 0.1241956\n\tspeed: 0.0272s/iter; left time: 301.4558s\n\titers: 300, epoch: 14 | loss: 0.1385666\n\tspeed: 0.0272s/iter; left time: 298.5977s\nEpoch: 14 cost time: 8.455315828323364\nEpoch: 14, Steps: 305 | Train Loss: 0.1180537 Vali Loss: 0.1725188 Test Loss: 0.1008053\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_exchange_96_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1422\ntest shape: (1422, 96, 8) (1422, 96, 8)\ntest shape: (1422, 96, 8) (1422, 96, 8)\nmse:0.1000538244843483, mae:0.2247844785451889, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/95a48c8a-7995-49f6-bf3a-2b17f237e13a","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"2d11f961","execution_start":1744872188540,"execution_millis":166290,"execution_context_id":"71545753-af2c-4f73-a72a-1e25f94f8ba5","cell_id":"5cb8abce23e4428ea3256a948b4ecb92","deepnote_cell_type":"code"},"source":"!mkdir -p result/PatchTST_exchange_192\n!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_exchange_192 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/Exchange \\\n--data_path exchange_rate.csv \\\n--features M \\\n--seq_len 336 \\\n--pred_len 192 \\\n--e_layers 2 \\\n--d_layers 1 \\\n--enc_in 8 \\\n--d_model 256 \\\n--learning_rate 0.001 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--checkpoints ./result/PatchTST_exchange_192 \\\n--des benchmark_patchtst \\\n2>&1 | tee result/PatchTST_exchange_192/train_log.txt","block_group":"5e6e25454c374a89b4b4bde3593faad9","execution_count":6,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_exchange_192Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/Exchange  \n  Data Path:          exchange_rate.csv   Features:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/PatchTST_exchange_192\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            336                 Label Len:          48                  \n  Pred Len:           192                 Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             8                   Dec In:             7                   \n  C Out:              7                   d model:            256                 \n  n heads:            8                   e layers:           2                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             1                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_exchange_192_PatchTST_custom_ftM_sl336_ll48_pl192_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4784\nval 569\ntest 1326\n\titers: 100, epoch: 1 | loss: 0.3590777\n\tspeed: 0.0349s/iter; left time: 517.8585s\n\titers: 200, epoch: 1 | loss: 0.2578550\n\tspeed: 0.0274s/iter; left time: 404.4047s\nEpoch: 1 cost time: 8.963098287582397\nEpoch: 1, Steps: 299 | Train Loss: 0.3614689 Vali Loss: 0.2745826 Test Loss: 0.2082196\nValidation loss decreased (inf --> 0.274583).  Saving model ...\nUpdating learning rate to 0.001\n\titers: 100, epoch: 2 | loss: 0.2701963\n\tspeed: 0.0724s/iter; left time: 1054.1924s\n\titers: 200, epoch: 2 | loss: 0.2638596\n\tspeed: 0.0275s/iter; left time: 397.5828s\nEpoch: 2 cost time: 8.3928382396698\nEpoch: 2, Steps: 299 | Train Loss: 0.3409566 Vali Loss: 0.2657813 Test Loss: 0.4016919\nValidation loss decreased (0.274583 --> 0.265781).  Saving model ...\nUpdating learning rate to 0.0005\n\titers: 100, epoch: 3 | loss: 0.2628231\n\tspeed: 0.0761s/iter; left time: 1085.2493s\n\titers: 200, epoch: 3 | loss: 0.3437592\n\tspeed: 0.0276s/iter; left time: 390.9162s\nEpoch: 3 cost time: 8.425771951675415\nEpoch: 3, Steps: 299 | Train Loss: 0.3076644 Vali Loss: 0.2671453 Test Loss: 0.2129745\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00025\n\titers: 100, epoch: 4 | loss: 0.2077392\n\tspeed: 0.0685s/iter; left time: 956.1031s\n\titers: 200, epoch: 4 | loss: 0.3659060\n\tspeed: 0.0276s/iter; left time: 382.5896s\nEpoch: 4 cost time: 8.436196327209473\nEpoch: 4, Steps: 299 | Train Loss: 0.2656137 Vali Loss: 0.2562061 Test Loss: 0.2078959\nValidation loss decreased (0.265781 --> 0.256206).  Saving model ...\nUpdating learning rate to 0.000125\n\titers: 100, epoch: 5 | loss: 0.2026043\n\tspeed: 0.0744s/iter; left time: 1015.7669s\n\titers: 200, epoch: 5 | loss: 0.2411011\n\tspeed: 0.0277s/iter; left time: 375.8996s\nEpoch: 5 cost time: 8.460285663604736\nEpoch: 5, Steps: 299 | Train Loss: 0.2460363 Vali Loss: 0.2528994 Test Loss: 0.2232281\nValidation loss decreased (0.256206 --> 0.252899).  Saving model ...\nUpdating learning rate to 6.25e-05\n\titers: 100, epoch: 6 | loss: 0.2374913\n\tspeed: 0.0767s/iter; left time: 1024.1848s\n\titers: 200, epoch: 6 | loss: 0.2338338\n\tspeed: 0.0277s/iter; left time: 366.8606s\nEpoch: 6 cost time: 8.433458805084229\nEpoch: 6, Steps: 299 | Train Loss: 0.2356313 Vali Loss: 0.2624376 Test Loss: 0.2089363\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 3.125e-05\n\titers: 100, epoch: 7 | loss: 0.2574903\n\tspeed: 0.0687s/iter; left time: 897.3230s\n\titers: 200, epoch: 7 | loss: 0.1943583\n\tspeed: 0.0277s/iter; left time: 358.5417s\nEpoch: 7 cost time: 8.437512636184692\nEpoch: 7, Steps: 299 | Train Loss: 0.2285519 Vali Loss: 0.2900512 Test Loss: 0.2082941\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 1.5625e-05\n\titers: 100, epoch: 8 | loss: 0.1989164\n\tspeed: 0.0693s/iter; left time: 883.8414s\n\titers: 200, epoch: 8 | loss: 0.1741431\n\tspeed: 0.0277s/iter; left time: 351.1581s\nEpoch: 8 cost time: 8.466695547103882\nEpoch: 8, Steps: 299 | Train Loss: 0.2250367 Vali Loss: 0.2744613 Test Loss: 0.2113065\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 7.8125e-06\n\titers: 100, epoch: 9 | loss: 0.2699560\n\tspeed: 0.0691s/iter; left time: 860.4618s\n\titers: 200, epoch: 9 | loss: 0.1946347\n\tspeed: 0.0278s/iter; left time: 343.4297s\nEpoch: 9 cost time: 8.474563598632812\nEpoch: 9, Steps: 299 | Train Loss: 0.2227428 Vali Loss: 0.2697910 Test Loss: 0.2125532\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 3.90625e-06\n\titers: 100, epoch: 10 | loss: 0.2007697\n\tspeed: 0.0692s/iter; left time: 841.6036s\n\titers: 200, epoch: 10 | loss: 0.2151553\n\tspeed: 0.0279s/iter; left time: 336.4632s\nEpoch: 10 cost time: 8.515071868896484\nEpoch: 10, Steps: 299 | Train Loss: 0.2232631 Vali Loss: 0.2740836 Test Loss: 0.2102517\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 1.953125e-06\n\titers: 100, epoch: 11 | loss: 0.2324507\n\tspeed: 0.0694s/iter; left time: 823.6684s\n\titers: 200, epoch: 11 | loss: 0.3130983\n\tspeed: 0.0279s/iter; left time: 328.5038s\nEpoch: 11 cost time: 8.533654928207397\nEpoch: 11, Steps: 299 | Train Loss: 0.2215259 Vali Loss: 0.2730243 Test Loss: 0.2105459\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 9.765625e-07\n\titers: 100, epoch: 12 | loss: 0.2327833\n\tspeed: 0.0695s/iter; left time: 803.0478s\n\titers: 200, epoch: 12 | loss: 0.2502987\n\tspeed: 0.0279s/iter; left time: 320.1227s\nEpoch: 12 cost time: 8.530344009399414\nEpoch: 12, Steps: 299 | Train Loss: 0.2207591 Vali Loss: 0.2729407 Test Loss: 0.2113924\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 4.8828125e-07\n\titers: 100, epoch: 13 | loss: 0.1752165\n\tspeed: 0.0692s/iter; left time: 779.5465s\n\titers: 200, epoch: 13 | loss: 0.1689015\n\tspeed: 0.0280s/iter; left time: 312.4568s\nEpoch: 13 cost time: 8.536474466323853\nEpoch: 13, Steps: 299 | Train Loss: 0.2222505 Vali Loss: 0.2704779 Test Loss: 0.2119617\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 2.44140625e-07\n\titers: 100, epoch: 14 | loss: 0.2499291\n\tspeed: 0.0698s/iter; left time: 765.6737s\n\titers: 200, epoch: 14 | loss: 0.1949223\n\tspeed: 0.0280s/iter; left time: 304.3220s\nEpoch: 14 cost time: 8.564401626586914\nEpoch: 14, Steps: 299 | Train Loss: 0.2210179 Vali Loss: 0.2758771 Test Loss: 0.2113638\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 1.220703125e-07\n\titers: 100, epoch: 15 | loss: 0.2117212\n\tspeed: 0.0696s/iter; left time: 741.9202s\n\titers: 200, epoch: 15 | loss: 0.2398069\n\tspeed: 0.0279s/iter; left time: 295.0308s\nEpoch: 15 cost time: 8.541175127029419\nEpoch: 15, Steps: 299 | Train Loss: 0.2215918 Vali Loss: 0.2760833 Test Loss: 0.2118263\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_exchange_192_PatchTST_custom_ftM_sl336_ll48_pl192_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1326\ntest shape: (1326, 192, 8) (1326, 192, 8)\ntest shape: (1326, 192, 8) (1326, 192, 8)\nmse:0.22320562601089478, mae:0.3411005437374115, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/1d019b8f-30d6-4313-8d58-5dde231dd207","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"743f33b5","execution_start":1744872354890,"execution_millis":155332,"execution_context_id":"71545753-af2c-4f73-a72a-1e25f94f8ba5","cell_id":"f06c8467b055463c8d13ed1eb70ac0a1","deepnote_cell_type":"code"},"source":"!mkdir -p result/PatchTST_exchange_336\n!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_exchange_336 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/Exchange \\\n--data_path exchange_rate.csv \\\n--features M \\\n--seq_len 336 \\\n--pred_len 336 \\\n--e_layers 2 \\\n--d_layers 1 \\\n--enc_in 8 \\\n--d_model 256 \\\n--learning_rate 0.001 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--checkpoints ./result/PatchTST_exchange_336 \\\n--des benchmark_patchtst \\\n2>&1 | tee result/PatchTST_exchange_336/train_log.txt","block_group":"79a7a1c62df84fdb8f28bba1b9293c80","execution_count":7,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_exchange_336Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/Exchange  \n  Data Path:          exchange_rate.csv   Features:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/PatchTST_exchange_336\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            336                 Label Len:          48                  \n  Pred Len:           336                 Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             8                   Dec In:             7                   \n  C Out:              7                   d model:            256                 \n  n heads:            8                   e layers:           2                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             1                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_exchange_336_PatchTST_custom_ftM_sl336_ll48_pl336_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4640\nval 425\ntest 1182\n\titers: 100, epoch: 1 | loss: 0.5529241\n\tspeed: 0.0354s/iter; left time: 510.1796s\n\titers: 200, epoch: 1 | loss: 0.6053340\n\tspeed: 0.0280s/iter; left time: 400.7971s\nEpoch: 1 cost time: 8.889715194702148\nEpoch: 1, Steps: 290 | Train Loss: 0.5552479 Vali Loss: 0.4558870 Test Loss: 0.3628620\nValidation loss decreased (inf --> 0.455887).  Saving model ...\nUpdating learning rate to 0.001\n\titers: 100, epoch: 2 | loss: 0.5086333\n\tspeed: 0.0723s/iter; left time: 1019.8249s\n\titers: 200, epoch: 2 | loss: 0.6641141\n\tspeed: 0.0281s/iter; left time: 393.9973s\nEpoch: 2 cost time: 8.329268455505371\nEpoch: 2, Steps: 290 | Train Loss: 0.5778401 Vali Loss: 0.4672264 Test Loss: 0.3494382\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0005\n\titers: 100, epoch: 3 | loss: 0.3625170\n\tspeed: 0.0669s/iter; left time: 924.1977s\n\titers: 200, epoch: 3 | loss: 0.4821285\n\tspeed: 0.0282s/iter; left time: 386.7080s\nEpoch: 3 cost time: 8.37111520767212\nEpoch: 3, Steps: 290 | Train Loss: 0.4872156 Vali Loss: 0.5259052 Test Loss: 0.3420490\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.00025\n\titers: 100, epoch: 4 | loss: 0.3653511\n\tspeed: 0.0673s/iter; left time: 910.3257s\n\titers: 200, epoch: 4 | loss: 0.2851827\n\tspeed: 0.0283s/iter; left time: 380.5081s\nEpoch: 4 cost time: 8.405449628829956\nEpoch: 4, Steps: 290 | Train Loss: 0.4286772 Vali Loss: 0.4473315 Test Loss: 0.3856877\nValidation loss decreased (0.455887 --> 0.447331).  Saving model ...\nUpdating learning rate to 0.000125\n\titers: 100, epoch: 5 | loss: 0.3721532\n\tspeed: 0.0749s/iter; left time: 991.2695s\n\titers: 200, epoch: 5 | loss: 0.4811644\n\tspeed: 0.0284s/iter; left time: 372.9090s\nEpoch: 5 cost time: 8.413973093032837\nEpoch: 5, Steps: 290 | Train Loss: 0.3918523 Vali Loss: 0.5096269 Test Loss: 0.4169544\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 6.25e-05\n\titers: 100, epoch: 6 | loss: 0.3210061\n\tspeed: 0.0676s/iter; left time: 875.0817s\n\titers: 200, epoch: 6 | loss: 0.2949557\n\tspeed: 0.0285s/iter; left time: 366.0929s\nEpoch: 6 cost time: 8.43654203414917\nEpoch: 6, Steps: 290 | Train Loss: 0.3726733 Vali Loss: 0.5336710 Test Loss: 0.4266953\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 3.125e-05\n\titers: 100, epoch: 7 | loss: 0.3715146\n\tspeed: 0.0673s/iter; left time: 852.7021s\n\titers: 200, epoch: 7 | loss: 0.3918716\n\tspeed: 0.0286s/iter; left time: 359.1217s\nEpoch: 7 cost time: 8.454626083374023\nEpoch: 7, Steps: 290 | Train Loss: 0.3618559 Vali Loss: 0.5971920 Test Loss: 0.4215596\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 1.5625e-05\n\titers: 100, epoch: 8 | loss: 0.4145428\n\tspeed: 0.0676s/iter; left time: 836.3850s\n\titers: 200, epoch: 8 | loss: 0.3760821\n\tspeed: 0.0286s/iter; left time: 351.3758s\nEpoch: 8 cost time: 8.483476877212524\nEpoch: 8, Steps: 290 | Train Loss: 0.3574827 Vali Loss: 0.5569004 Test Loss: 0.4331428\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 7.8125e-06\n\titers: 100, epoch: 9 | loss: 0.3963010\n\tspeed: 0.0677s/iter; left time: 817.6068s\n\titers: 200, epoch: 9 | loss: 0.4964882\n\tspeed: 0.0287s/iter; left time: 343.4115s\nEpoch: 9 cost time: 8.484370708465576\nEpoch: 9, Steps: 290 | Train Loss: 0.3554824 Vali Loss: 0.5512703 Test Loss: 0.4357669\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 3.90625e-06\n\titers: 100, epoch: 10 | loss: 0.3655123\n\tspeed: 0.0679s/iter; left time: 800.7540s\n\titers: 200, epoch: 10 | loss: 0.3400949\n\tspeed: 0.0287s/iter; left time: 335.6019s\nEpoch: 10 cost time: 8.506754159927368\nEpoch: 10, Steps: 290 | Train Loss: 0.3526596 Vali Loss: 0.5458760 Test Loss: 0.4413370\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 1.953125e-06\n\titers: 100, epoch: 11 | loss: 0.4324376\n\tspeed: 0.0679s/iter; left time: 780.7814s\n\titers: 200, epoch: 11 | loss: 0.3776244\n\tspeed: 0.0284s/iter; left time: 323.9844s\nEpoch: 11 cost time: 8.4275803565979\nEpoch: 11, Steps: 290 | Train Loss: 0.3528922 Vali Loss: 0.5598938 Test Loss: 0.4450409\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 9.765625e-07\n\titers: 100, epoch: 12 | loss: 0.3494293\n\tspeed: 0.0668s/iter; left time: 748.6350s\n\titers: 200, epoch: 12 | loss: 0.2950520\n\tspeed: 0.0283s/iter; left time: 314.1806s\nEpoch: 12 cost time: 8.357056856155396\nEpoch: 12, Steps: 290 | Train Loss: 0.3521877 Vali Loss: 0.5551403 Test Loss: 0.4314126\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 4.8828125e-07\n\titers: 100, epoch: 13 | loss: 0.3803320\n\tspeed: 0.0669s/iter; left time: 730.9597s\n\titers: 200, epoch: 13 | loss: 0.3047845\n\tspeed: 0.0282s/iter; left time: 305.1683s\nEpoch: 13 cost time: 8.353440284729004\nEpoch: 13, Steps: 290 | Train Loss: 0.3511874 Vali Loss: 0.5592348 Test Loss: 0.4368216\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 2.44140625e-07\n\titers: 100, epoch: 14 | loss: 0.3790367\n\tspeed: 0.0667s/iter; left time: 708.5745s\n\titers: 200, epoch: 14 | loss: 0.3515731\n\tspeed: 0.0281s/iter; left time: 296.3178s\nEpoch: 14 cost time: 8.330148935317993\nEpoch: 14, Steps: 290 | Train Loss: 0.3523936 Vali Loss: 0.5620645 Test Loss: 0.4392062\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_exchange_336_PatchTST_custom_ftM_sl336_ll48_pl336_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1182\ntest shape: (1182, 336, 8) (1182, 336, 8)\ntest shape: (1182, 336, 8) (1182, 336, 8)\nmse:0.38564929366111755, mae:0.45475688576698303, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/b8ddf48c-d1da-410f-ad78-356ae6658b89","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"1ecb501e","execution_start":1744872510280,"execution_millis":130627,"execution_context_id":"71545753-af2c-4f73-a72a-1e25f94f8ba5","cell_id":"9598c6941cd4410080bc7378e69719d9","deepnote_cell_type":"code"},"source":"!mkdir -p result/PatchTST_exchange_720\n!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_exchange_720 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/Exchange \\\n--data_path exchange_rate.csv \\\n--features M \\\n--seq_len 336 \\\n--pred_len 720 \\\n--e_layers 2 \\\n--d_layers 1 \\\n--enc_in 8 \\\n--d_model 256 \\\n--learning_rate 0.001 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--checkpoints ./result/PatchTST_exchange_720 \\\n--des benchmark_patchtst \\\n2>&1 | tee result/PatchTST_exchange_720/train_log.txt","block_group":"1ed74199f0fb4f0cb70523bb67ec47f4","execution_count":8,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_exchange_720Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/Exchange  \n  Data Path:          exchange_rate.csv   Features:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/PatchTST_exchange_720\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            336                 Label Len:          48                  \n  Pred Len:           720                 Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             8                   Dec In:             7                   \n  C Out:              7                   d model:            256                 \n  n heads:            8                   e layers:           2                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             1                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_exchange_720_PatchTST_custom_ftM_sl336_ll48_pl720_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4256\nval 41\ntest 798\n\titers: 100, epoch: 1 | loss: 1.0504659\n\tspeed: 0.0387s/iter; left time: 510.8901s\n\titers: 200, epoch: 1 | loss: 0.8723514\n\tspeed: 0.0300s/iter; left time: 393.4353s\nEpoch: 1 cost time: 8.871572732925415\nEpoch: 1, Steps: 266 | Train Loss: 0.9216215 Vali Loss: 1.9183122 Test Loss: 0.7114298\nValidation loss decreased (inf --> 1.918312).  Saving model ...\nUpdating learning rate to 0.001\n\titers: 100, epoch: 2 | loss: 0.9738464\n\tspeed: 0.0667s/iter; left time: 863.2086s\n\titers: 200, epoch: 2 | loss: 1.0620298\n\tspeed: 0.0302s/iter; left time: 387.4492s\nEpoch: 2 cost time: 8.229167699813843\nEpoch: 2, Steps: 266 | Train Loss: 0.9108918 Vali Loss: 1.3828764 Test Loss: 0.8803100\nValidation loss decreased (1.918312 --> 1.382876).  Saving model ...\nUpdating learning rate to 0.0005\n\titers: 100, epoch: 3 | loss: 0.8966203\n\tspeed: 0.0694s/iter; left time: 879.5699s\n\titers: 200, epoch: 3 | loss: 0.7542177\n\tspeed: 0.0302s/iter; left time: 379.8508s\nEpoch: 3 cost time: 8.224925518035889\nEpoch: 3, Steps: 266 | Train Loss: 0.8406276 Vali Loss: 2.1289921 Test Loss: 0.7608016\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00025\n\titers: 100, epoch: 4 | loss: 0.7265636\n\tspeed: 0.0606s/iter; left time: 751.3061s\n\titers: 200, epoch: 4 | loss: 0.5799122\n\tspeed: 0.0303s/iter; left time: 373.0857s\nEpoch: 4 cost time: 8.238056659698486\nEpoch: 4, Steps: 266 | Train Loss: 0.7455129 Vali Loss: 1.8596061 Test Loss: 0.7444141\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.000125\n\titers: 100, epoch: 5 | loss: 0.4955007\n\tspeed: 0.0605s/iter; left time: 734.6166s\n\titers: 200, epoch: 5 | loss: 0.7869644\n\tspeed: 0.0305s/iter; left time: 367.1620s\nEpoch: 5 cost time: 8.264065504074097\nEpoch: 5, Steps: 266 | Train Loss: 0.6824382 Vali Loss: 2.0713081 Test Loss: 0.9154034\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 6.25e-05\n\titers: 100, epoch: 6 | loss: 0.6081433\n\tspeed: 0.0613s/iter; left time: 727.3418s\n\titers: 200, epoch: 6 | loss: 0.7493198\n\tspeed: 0.0306s/iter; left time: 360.0294s\nEpoch: 6 cost time: 8.315102815628052\nEpoch: 6, Steps: 266 | Train Loss: 0.6453988 Vali Loss: 1.6500124 Test Loss: 0.9818861\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 3.125e-05\n\titers: 100, epoch: 7 | loss: 0.5686231\n\tspeed: 0.0614s/iter; left time: 712.3809s\n\titers: 200, epoch: 7 | loss: 0.7159244\n\tspeed: 0.0307s/iter; left time: 352.9671s\nEpoch: 7 cost time: 8.344075679779053\nEpoch: 7, Steps: 266 | Train Loss: 0.6287877 Vali Loss: 1.5894704 Test Loss: 1.1085281\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 1.5625e-05\n\titers: 100, epoch: 8 | loss: 0.6474094\n\tspeed: 0.0615s/iter; left time: 697.3030s\n\titers: 200, epoch: 8 | loss: 0.6669675\n\tspeed: 0.0307s/iter; left time: 345.3596s\nEpoch: 8 cost time: 8.369926929473877\nEpoch: 8, Steps: 266 | Train Loss: 0.6177258 Vali Loss: 1.6579742 Test Loss: 1.0980641\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 7.8125e-06\n\titers: 100, epoch: 9 | loss: 0.6915506\n\tspeed: 0.0618s/iter; left time: 683.7789s\n\titers: 200, epoch: 9 | loss: 0.4829060\n\tspeed: 0.0309s/iter; left time: 338.6126s\nEpoch: 9 cost time: 8.384981393814087\nEpoch: 9, Steps: 266 | Train Loss: 0.6114923 Vali Loss: 1.6765447 Test Loss: 1.1009632\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 3.90625e-06\n\titers: 100, epoch: 10 | loss: 0.6019232\n\tspeed: 0.0621s/iter; left time: 671.3752s\n\titers: 200, epoch: 10 | loss: 0.6466063\n\tspeed: 0.0308s/iter; left time: 329.6130s\nEpoch: 10 cost time: 8.370330810546875\nEpoch: 10, Steps: 266 | Train Loss: 0.6083607 Vali Loss: 1.6615511 Test Loss: 1.1123421\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 1.953125e-06\n\titers: 100, epoch: 11 | loss: 0.5531770\n\tspeed: 0.0618s/iter; left time: 651.8074s\n\titers: 200, epoch: 11 | loss: 0.5916321\n\tspeed: 0.0307s/iter; left time: 320.0692s\nEpoch: 11 cost time: 8.35361623764038\nEpoch: 11, Steps: 266 | Train Loss: 0.6057583 Vali Loss: 1.6988539 Test Loss: 1.0817188\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 9.765625e-07\n\titers: 100, epoch: 12 | loss: 0.5957004\n\tspeed: 0.0614s/iter; left time: 631.0102s\n\titers: 200, epoch: 12 | loss: 0.6027269\n\tspeed: 0.0306s/iter; left time: 311.5802s\nEpoch: 12 cost time: 8.328907489776611\nEpoch: 12, Steps: 266 | Train Loss: 0.6081690 Vali Loss: 1.6799688 Test Loss: 1.1069704\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_exchange_720_PatchTST_custom_ftM_sl336_ll48_pl720_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 798\ntest shape: (798, 720, 8) (798, 720, 8)\ntest shape: (798, 720, 8) (798, 720, 8)\nmse:0.8816307187080383, mae:0.7016471028327942, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/373f3ba2-ba1b-41db-8580-d496a3d819e9","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=25aca1a4-5304-4528-85ec-154f53dfeb1c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"fbce8eb164ba491aa13b5edc51a88011"}}