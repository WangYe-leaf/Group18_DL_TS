{"cells":[{"cell_type":"code","metadata":{"source_hash":"1acff9c4","execution_start":1744903379950,"execution_millis":87,"execution_context_id":"a2dbb6c0-0756-4681-bff1-884731f15d53","cell_id":"89e4a75ca2a84c26b942d9c5884d0bae","deepnote_cell_type":"code"},"source":"%cd Time-Series-Library\n!pwd","block_group":"89e4a75ca2a84c26b942d9c5884d0bae","execution_count":1,"outputs":[{"name":"stdout","text":"/datasets/_deepnote_work/Time-Series-Library\n/datasets/_deepnote_work/Time-Series-Library\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/29046049-6f5e-48fc-92e9-4749e9c9f814","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"60b56d39","execution_start":1744903387193,"execution_millis":1158,"execution_context_id":"a2dbb6c0-0756-4681-bff1-884731f15d53","cell_id":"daa7f94aa5da4b918e9f907de58c43fe","deepnote_cell_type":"code"},"source":"import pandas as pd\nimport numpy as np\n\ndef add_gaussian_noise(file_path, save_path, noise_level=0.05):\n    df = pd.read_csv(file_path)\n    numeric_cols = df.columns[1:]  # 保留时间戳列不动\n\n    for col in numeric_cols:\n        std = df[col].std()\n        noise = np.random.normal(loc=0.0, scale=std * noise_level, size=len(df))\n        df[col] += noise\n\n    df.to_csv(save_path, index=False)\n    print(f\"✅ Noise level {noise_level*100:.0f}% added to {save_path}\")\n\n# 原始路径\nfile_path = './dataset/Exchange/exchange_rate.csv'\n\n# 生成三个不同噪声比例的数据集\nadd_gaussian_noise(file_path, './dataset/Exchange/exchange_rate_noise5.csv', noise_level=0.05)\nadd_gaussian_noise(file_path, './dataset/Exchange/exchange_rate_noise10.csv', noise_level=0.10)\nadd_gaussian_noise(file_path, './dataset/Exchange/exchange_rate_noise20.csv', noise_level=0.20)","block_group":"908c36609f8f4c3b8c7804c5139b83c2","execution_count":4,"outputs":[{"name":"stdout","text":"✅ Noise level 5% added to ./dataset/Exchange/exchange_rate_noise5.csv\n✅ Noise level 10% added to ./dataset/Exchange/exchange_rate_noise10.csv\n✅ Noise level 20% added to ./dataset/Exchange/exchange_rate_noise20.csv\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/1cdc4840-64c7-4608-8e38-ba6e65d3f392","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"a894431b","execution_start":1744903417090,"execution_millis":1101,"execution_context_id":"a2dbb6c0-0756-4681-bff1-884731f15d53","cell_id":"6ef90f7f99104373ac1ac8f011a5a0a7","deepnote_cell_type":"code"},"source":"!mkdir -p result/Robustness/InputNoise/PatchTST_exchange_noise5\n!mkdir -p result/Robustness/InputNoise/PatchTST_exchange_noise10\n!mkdir -p result/Robustness/InputNoise/PatchTST_exchange_noise20","block_group":"7583ce563a754af7b79496564a8e7ccb","execution_count":7,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"e5bb854","execution_start":1744904368650,"execution_millis":479021,"execution_context_id":"a2dbb6c0-0756-4681-bff1-884731f15d53","cell_id":"0c44eaa7f35f4507a6ab1bbb07e32484","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_exchange_noise5 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/Exchange \\\n--data_path exchange_rate_noise5.csv \\\n--features M \\\n--seq_len 336 \\\n--pred_len 96 \\\n--e_layers 2 \\\n--d_layers 1 \\\n--enc_in 8 \\\n--d_model 256 \\\n--learning_rate 0.001 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--checkpoints ./result/Robustness/InputNoise/PatchTST_exchange_noise5 \\\n--des benchmark_patchtst \\\n2>&1 | tee result/Robustness/InputNoise/PatchTST_exchange_noise5/train_log.txt","block_group":"b1f5d4fd633a45fd9450abccc5a8d582","execution_count":10,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_exchange_noise5Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/Exchange  \n  Data Path:          exchange_rate_noise5.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/Robustness/InputNoise/PatchTST_exchange_noise5\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            336                 Label Len:          48                  \n  Pred Len:           96                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             8                   Dec In:             7                   \n  C Out:              7                   d model:            256                 \n  n heads:            8                   e layers:           2                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             1                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_exchange_noise5_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4880\nval 665\ntest 1422\n\titers: 100, epoch: 1 | loss: 0.3316176\n\tspeed: 0.1156s/iter; left time: 1751.5097s\n\titers: 200, epoch: 1 | loss: 0.3359469\n\tspeed: 0.1022s/iter; left time: 1538.4284s\n\titers: 300, epoch: 1 | loss: 0.1917431\n\tspeed: 0.1022s/iter; left time: 1527.4177s\nEpoch: 1 cost time: 32.52637553215027\nEpoch: 1, Steps: 305 | Train Loss: 0.2399106 Vali Loss: 0.1766608 Test Loss: 0.1796074\nValidation loss decreased (inf --> 0.176661).  Saving model ...\nUpdating learning rate to 0.001\n\titers: 100, epoch: 2 | loss: 0.2912932\n\tspeed: 0.1523s/iter; left time: 2260.5741s\n\titers: 200, epoch: 2 | loss: 0.1367706\n\tspeed: 0.1021s/iter; left time: 1505.2832s\n\titers: 300, epoch: 2 | loss: 0.1632237\n\tspeed: 0.1023s/iter; left time: 1497.9652s\nEpoch: 2 cost time: 31.379705905914307\nEpoch: 2, Steps: 305 | Train Loss: 0.1836226 Vali Loss: 0.1895025 Test Loss: 0.1407448\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0005\n\titers: 100, epoch: 3 | loss: 0.1531163\n\tspeed: 0.1385s/iter; left time: 2013.8154s\n\titers: 200, epoch: 3 | loss: 0.2149629\n\tspeed: 0.0972s/iter; left time: 1403.9049s\n\titers: 300, epoch: 3 | loss: 0.1578160\n\tspeed: 0.0969s/iter; left time: 1389.6831s\nEpoch: 3 cost time: 29.837584018707275\nEpoch: 3, Steps: 305 | Train Loss: 0.1515710 Vali Loss: 0.1579628 Test Loss: 0.1186968\nValidation loss decreased (0.176661 --> 0.157963).  Saving model ...\nUpdating learning rate to 0.00025\n\titers: 100, epoch: 4 | loss: 0.1073898\n\tspeed: 0.1442s/iter; left time: 2052.7035s\n\titers: 200, epoch: 4 | loss: 0.2008976\n\tspeed: 0.0970s/iter; left time: 1370.6968s\n\titers: 300, epoch: 4 | loss: 0.1077824\n\tspeed: 0.0975s/iter; left time: 1368.5682s\nEpoch: 4 cost time: 29.531807899475098\nEpoch: 4, Steps: 305 | Train Loss: 0.1414353 Vali Loss: 0.1642457 Test Loss: 0.1042678\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.000125\n\titers: 100, epoch: 5 | loss: 0.1480920\n\tspeed: 0.1408s/iter; left time: 1961.9449s\n\titers: 200, epoch: 5 | loss: 0.1640947\n\tspeed: 0.0966s/iter; left time: 1336.2627s\n\titers: 300, epoch: 5 | loss: 0.1065589\n\tspeed: 0.0970s/iter; left time: 1332.4802s\nEpoch: 5 cost time: 29.754578113555908\nEpoch: 5, Steps: 305 | Train Loss: 0.1351121 Vali Loss: 0.1687990 Test Loss: 0.1129081\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 6.25e-05\n\titers: 100, epoch: 6 | loss: 0.1810227\n\tspeed: 0.1416s/iter; left time: 1928.9388s\n\titers: 200, epoch: 6 | loss: 0.1103135\n\tspeed: 0.0902s/iter; left time: 1220.4718s\n\titers: 300, epoch: 6 | loss: 0.1003729\n\tspeed: 0.1022s/iter; left time: 1372.6722s\nEpoch: 6 cost time: 29.702840089797974\nEpoch: 6, Steps: 305 | Train Loss: 0.1299767 Vali Loss: 0.1774300 Test Loss: 0.1087335\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 3.125e-05\n\titers: 100, epoch: 7 | loss: 0.1526929\n\tspeed: 0.1463s/iter; left time: 1949.2548s\n\titers: 200, epoch: 7 | loss: 0.1239165\n\tspeed: 0.1022s/iter; left time: 1351.3449s\n\titers: 300, epoch: 7 | loss: 0.1231648\n\tspeed: 0.1025s/iter; left time: 1344.3935s\nEpoch: 7 cost time: 31.385202884674072\nEpoch: 7, Steps: 305 | Train Loss: 0.1276765 Vali Loss: 0.1753053 Test Loss: 0.1079362\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 1.5625e-05\n\titers: 100, epoch: 8 | loss: 0.1027011\n\tspeed: 0.1465s/iter; left time: 1906.4710s\n\titers: 200, epoch: 8 | loss: 0.1098714\n\tspeed: 0.0996s/iter; left time: 1286.0519s\n\titers: 300, epoch: 8 | loss: 0.1594885\n\tspeed: 0.1017s/iter; left time: 1303.6922s\nEpoch: 8 cost time: 31.021045446395874\nEpoch: 8, Steps: 305 | Train Loss: 0.1254043 Vali Loss: 0.1888967 Test Loss: 0.1068183\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 7.8125e-06\n\titers: 100, epoch: 9 | loss: 0.1529136\n\tspeed: 0.1413s/iter; left time: 1796.3306s\n\titers: 200, epoch: 9 | loss: 0.1537348\n\tspeed: 0.1024s/iter; left time: 1291.0357s\n\titers: 300, epoch: 9 | loss: 0.0985086\n\tspeed: 0.1019s/iter; left time: 1274.6491s\nEpoch: 9 cost time: 30.825441360473633\nEpoch: 9, Steps: 305 | Train Loss: 0.1246677 Vali Loss: 0.1816355 Test Loss: 0.1072131\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 3.90625e-06\n\titers: 100, epoch: 10 | loss: 0.1025671\n\tspeed: 0.1459s/iter; left time: 1810.3780s\n\titers: 200, epoch: 10 | loss: 0.1300448\n\tspeed: 0.1023s/iter; left time: 1259.3188s\n\titers: 300, epoch: 10 | loss: 0.1085689\n\tspeed: 0.1019s/iter; left time: 1243.5062s\nEpoch: 10 cost time: 31.32051992416382\nEpoch: 10, Steps: 305 | Train Loss: 0.1240180 Vali Loss: 0.1772451 Test Loss: 0.1068420\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 1.953125e-06\n\titers: 100, epoch: 11 | loss: 0.1272754\n\tspeed: 0.1461s/iter; left time: 1768.0644s\n\titers: 200, epoch: 11 | loss: 0.1211854\n\tspeed: 0.1023s/iter; left time: 1227.2268s\n\titers: 300, epoch: 11 | loss: 0.0945119\n\tspeed: 0.1024s/iter; left time: 1218.0866s\nEpoch: 11 cost time: 31.382067680358887\nEpoch: 11, Steps: 305 | Train Loss: 0.1238398 Vali Loss: 0.1837267 Test Loss: 0.1073150\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 9.765625e-07\n\titers: 100, epoch: 12 | loss: 0.1528240\n\tspeed: 0.1461s/iter; left time: 1723.8338s\n\titers: 200, epoch: 12 | loss: 0.1152747\n\tspeed: 0.1023s/iter; left time: 1196.4853s\n\titers: 300, epoch: 12 | loss: 0.0976109\n\tspeed: 0.1021s/iter; left time: 1183.6987s\nEpoch: 12 cost time: 31.348623275756836\nEpoch: 12, Steps: 305 | Train Loss: 0.1236943 Vali Loss: 0.1809296 Test Loss: 0.1071682\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 4.8828125e-07\n\titers: 100, epoch: 13 | loss: 0.1145782\n\tspeed: 0.1459s/iter; left time: 1676.2907s\n\titers: 200, epoch: 13 | loss: 0.0985968\n\tspeed: 0.1023s/iter; left time: 1165.1952s\n\titers: 300, epoch: 13 | loss: 0.1342968\n\tspeed: 0.1021s/iter; left time: 1153.3266s\nEpoch: 13 cost time: 31.37147808074951\nEpoch: 13, Steps: 305 | Train Loss: 0.1243970 Vali Loss: 0.1804078 Test Loss: 0.1073098\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_exchange_noise5_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1422\ntest shape: (1422, 96, 8) (1422, 96, 8)\ntest shape: (1422, 96, 8) (1422, 96, 8)\nmse:0.11861252784729004, mae:0.24808244407176971, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/9ee9b1cf-3b84-44a8-884b-1ab87c68cedd","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"87b3f4c1","execution_start":1744904847730,"execution_millis":410628,"execution_context_id":"a2dbb6c0-0756-4681-bff1-884731f15d53","cell_id":"67458a93b2f84ea28eaf5ae6fa85b781","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_exchange_noise10 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/Exchange \\\n--data_path exchange_rate_noise10.csv \\\n--features M \\\n--seq_len 336 \\\n--pred_len 96 \\\n--e_layers 2 \\\n--d_layers 1 \\\n--enc_in 8 \\\n--d_model 256 \\\n--learning_rate 0.001 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--checkpoints ./result/Robustness/InputNoise/PatchTST_exchange_noise10 \\\n--des benchmark_patchtst \\\n2>&1 | tee result/Robustness/InputNoise/PatchTST_exchange_noise10/train_log.txt","block_group":"671ea41b6ba74243a3d6a257c763c4f1","execution_count":11,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_exchange_noise10Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/Exchange  \n  Data Path:          exchange_rate_noise10.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/Robustness/InputNoise/PatchTST_exchange_noise10\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            336                 Label Len:          48                  \n  Pred Len:           96                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             8                   Dec In:             7                   \n  C Out:              7                   d model:            256                 \n  n heads:            8                   e layers:           2                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             1                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_exchange_noise10_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4880\nval 665\ntest 1422\n\titers: 100, epoch: 1 | loss: 0.3315849\n\tspeed: 0.1160s/iter; left time: 1757.2725s\n\titers: 200, epoch: 1 | loss: 0.3258429\n\tspeed: 0.1021s/iter; left time: 1536.0567s\n\titers: 300, epoch: 1 | loss: 0.2087657\n\tspeed: 0.1021s/iter; left time: 1526.2687s\nEpoch: 1 cost time: 32.54846167564392\nEpoch: 1, Steps: 305 | Train Loss: 0.2580083 Vali Loss: 0.1707896 Test Loss: 0.1192791\nValidation loss decreased (inf --> 0.170790).  Saving model ...\nUpdating learning rate to 0.001\n\titers: 100, epoch: 2 | loss: 0.3605064\n\tspeed: 0.1515s/iter; left time: 2249.6354s\n\titers: 200, epoch: 2 | loss: 0.1775426\n\tspeed: 0.1023s/iter; left time: 1509.2297s\n\titers: 300, epoch: 2 | loss: 0.2599064\n\tspeed: 0.1023s/iter; left time: 1498.0280s\nEpoch: 2 cost time: 31.39033317565918\nEpoch: 2, Steps: 305 | Train Loss: 0.2343252 Vali Loss: 0.2157690 Test Loss: 0.1436785\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0005\n\titers: 100, epoch: 3 | loss: 0.2374360\n\tspeed: 0.1463s/iter; left time: 2127.5798s\n\titers: 200, epoch: 3 | loss: 0.2713431\n\tspeed: 0.1023s/iter; left time: 1477.4869s\n\titers: 300, epoch: 3 | loss: 0.2105753\n\tspeed: 0.1023s/iter; left time: 1466.9626s\nEpoch: 3 cost time: 31.388697147369385\nEpoch: 3, Steps: 305 | Train Loss: 0.2220431 Vali Loss: 0.2088414 Test Loss: 0.1085426\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.00025\n\titers: 100, epoch: 4 | loss: 0.1660266\n\tspeed: 0.1459s/iter; left time: 2077.6944s\n\titers: 200, epoch: 4 | loss: 0.2686437\n\tspeed: 0.1023s/iter; left time: 1446.2994s\n\titers: 300, epoch: 4 | loss: 0.1433017\n\tspeed: 0.1025s/iter; left time: 1439.0877s\nEpoch: 4 cost time: 31.34574007987976\nEpoch: 4, Steps: 305 | Train Loss: 0.1881647 Vali Loss: 0.1843390 Test Loss: 0.1150490\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 0.000125\n\titers: 100, epoch: 5 | loss: 0.2142189\n\tspeed: 0.1466s/iter; left time: 2041.8385s\n\titers: 200, epoch: 5 | loss: 0.1903963\n\tspeed: 0.1023s/iter; left time: 1414.4766s\n\titers: 300, epoch: 5 | loss: 0.1260714\n\tspeed: 0.1024s/iter; left time: 1405.6151s\nEpoch: 5 cost time: 31.388108491897583\nEpoch: 5, Steps: 305 | Train Loss: 0.1695490 Vali Loss: 0.1959697 Test Loss: 0.1168478\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 6.25e-05\n\titers: 100, epoch: 6 | loss: 0.2367544\n\tspeed: 0.1460s/iter; left time: 1989.6772s\n\titers: 200, epoch: 6 | loss: 0.1189673\n\tspeed: 0.1023s/iter; left time: 1383.0665s\n\titers: 300, epoch: 6 | loss: 0.1224403\n\tspeed: 0.1022s/iter; left time: 1372.1025s\nEpoch: 6 cost time: 31.339622259140015\nEpoch: 6, Steps: 305 | Train Loss: 0.1605388 Vali Loss: 0.1764733 Test Loss: 0.1125335\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 3.125e-05\n\titers: 100, epoch: 7 | loss: 0.1934278\n\tspeed: 0.1464s/iter; left time: 1950.1924s\n\titers: 200, epoch: 7 | loss: 0.1876329\n\tspeed: 0.1022s/iter; left time: 1350.6809s\n\titers: 300, epoch: 7 | loss: 0.1544263\n\tspeed: 0.1024s/iter; left time: 1343.5723s\nEpoch: 7 cost time: 31.36261820793152\nEpoch: 7, Steps: 305 | Train Loss: 0.1569332 Vali Loss: 0.1715911 Test Loss: 0.1124314\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 1.5625e-05\n\titers: 100, epoch: 8 | loss: 0.1389361\n\tspeed: 0.1464s/iter; left time: 1905.2958s\n\titers: 200, epoch: 8 | loss: 0.1242706\n\tspeed: 0.1024s/iter; left time: 1322.8302s\n\titers: 300, epoch: 8 | loss: 0.1704597\n\tspeed: 0.1001s/iter; left time: 1282.4441s\nEpoch: 8 cost time: 31.15972399711609\nEpoch: 8, Steps: 305 | Train Loss: 0.1544585 Vali Loss: 0.1808180 Test Loss: 0.1098803\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 7.8125e-06\n\titers: 100, epoch: 9 | loss: 0.1698105\n\tspeed: 0.1472s/iter; left time: 1871.6125s\n\titers: 200, epoch: 9 | loss: 0.1754206\n\tspeed: 0.1015s/iter; left time: 1279.6702s\n\titers: 300, epoch: 9 | loss: 0.1347385\n\tspeed: 0.0954s/iter; left time: 1193.6059s\nEpoch: 9 cost time: 30.60975217819214\nEpoch: 9, Steps: 305 | Train Loss: 0.1534361 Vali Loss: 0.1754373 Test Loss: 0.1126899\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 3.90625e-06\n\titers: 100, epoch: 10 | loss: 0.1207981\n\tspeed: 0.1465s/iter; left time: 1817.2775s\n\titers: 200, epoch: 10 | loss: 0.1666416\n\tspeed: 0.1022s/iter; left time: 1257.5316s\n\titers: 300, epoch: 10 | loss: 0.1348253\n\tspeed: 0.1022s/iter; left time: 1247.6571s\nEpoch: 10 cost time: 31.349555730819702\nEpoch: 10, Steps: 305 | Train Loss: 0.1534423 Vali Loss: 0.1736833 Test Loss: 0.1099938\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 1.953125e-06\n\titers: 100, epoch: 11 | loss: 0.1593793\n\tspeed: 0.1465s/iter; left time: 1772.2466s\n\titers: 200, epoch: 11 | loss: 0.1468267\n\tspeed: 0.1023s/iter; left time: 1228.2027s\n\titers: 300, epoch: 11 | loss: 0.1185429\n\tspeed: 0.1022s/iter; left time: 1216.6883s\nEpoch: 11 cost time: 31.375306129455566\nEpoch: 11, Steps: 305 | Train Loss: 0.1527711 Vali Loss: 0.1755985 Test Loss: 0.1105936\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_exchange_noise10_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1422\ntest shape: (1422, 96, 8) (1422, 96, 8)\ntest shape: (1422, 96, 8) (1422, 96, 8)\nmse:0.11924323439598083, mae:0.25675588846206665, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/d0aa6991-425f-4085-b2ea-dd046c190283","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"ca452972","execution_start":1744905258410,"execution_millis":436713,"execution_context_id":"a2dbb6c0-0756-4681-bff1-884731f15d53","cell_id":"5c9f5033394f45e78a014fcd6146a2fe","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_exchange_noise20 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/Exchange \\\n--data_path exchange_rate_noise20.csv \\\n--features M \\\n--seq_len 336 \\\n--pred_len 96 \\\n--e_layers 2 \\\n--d_layers 1 \\\n--enc_in 8 \\\n--d_model 256 \\\n--learning_rate 0.001 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--checkpoints ./result/Robustness/InputNoise/PatchTST_exchange_noise20 \\\n--des benchmark_patchtst \\\n2>&1 | tee result/Robustness/InputNoise/PatchTST_exchange_noise20/train_log.txt","block_group":"5e7fad9dbc20437096799f921e86b393","execution_count":12,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_exchange_noise20Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/Exchange  \n  Data Path:          exchange_rate_noise20.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/Robustness/InputNoise/PatchTST_exchange_noise20\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            336                 Label Len:          48                  \n  Pred Len:           96                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             8                   Dec In:             7                   \n  C Out:              7                   d model:            256                 \n  n heads:            8                   e layers:           2                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             1                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_exchange_noise20_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4880\nval 665\ntest 1422\n\titers: 100, epoch: 1 | loss: 0.3951210\n\tspeed: 0.1151s/iter; left time: 1743.1946s\n\titers: 200, epoch: 1 | loss: 0.3499379\n\tspeed: 0.1021s/iter; left time: 1536.8880s\n\titers: 300, epoch: 1 | loss: 0.2814336\n\tspeed: 0.1022s/iter; left time: 1527.4976s\nEpoch: 1 cost time: 32.46708559989929\nEpoch: 1, Steps: 305 | Train Loss: 0.3110412 Vali Loss: 0.2060101 Test Loss: 0.2168961\nValidation loss decreased (inf --> 0.206010).  Saving model ...\nUpdating learning rate to 0.001\n\titers: 100, epoch: 2 | loss: 0.4827261\n\tspeed: 0.1525s/iter; left time: 2264.1285s\n\titers: 200, epoch: 2 | loss: 0.2707565\n\tspeed: 0.1021s/iter; left time: 1506.0181s\n\titers: 300, epoch: 2 | loss: 0.2478113\n\tspeed: 0.1021s/iter; left time: 1495.0157s\nEpoch: 2 cost time: 31.352502822875977\nEpoch: 2, Steps: 305 | Train Loss: 0.2810914 Vali Loss: 0.2000219 Test Loss: 0.2280369\nValidation loss decreased (0.206010 --> 0.200022).  Saving model ...\nUpdating learning rate to 0.0005\n\titers: 100, epoch: 3 | loss: 0.2316290\n\tspeed: 0.1521s/iter; left time: 2211.5205s\n\titers: 200, epoch: 3 | loss: 0.2428179\n\tspeed: 0.1022s/iter; left time: 1475.2769s\n\titers: 300, epoch: 3 | loss: 0.2479027\n\tspeed: 0.1019s/iter; left time: 1461.9816s\nEpoch: 3 cost time: 31.33735227584839\nEpoch: 3, Steps: 305 | Train Loss: 0.2213997 Vali Loss: 0.2053563 Test Loss: 0.1627287\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00025\n\titers: 100, epoch: 4 | loss: 0.1671891\n\tspeed: 0.1465s/iter; left time: 2085.5666s\n\titers: 200, epoch: 4 | loss: 0.2609894\n\tspeed: 0.1020s/iter; left time: 1442.2274s\n\titers: 300, epoch: 4 | loss: 0.1609454\n\tspeed: 0.1025s/iter; left time: 1438.3821s\nEpoch: 4 cost time: 31.354880332946777\nEpoch: 4, Steps: 305 | Train Loss: 0.1983088 Vali Loss: 0.2120202 Test Loss: 0.1619981\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.000125\n\titers: 100, epoch: 5 | loss: 0.2221055\n\tspeed: 0.1462s/iter; left time: 2037.2392s\n\titers: 200, epoch: 5 | loss: 0.1950438\n\tspeed: 0.1021s/iter; left time: 1412.7048s\n\titers: 300, epoch: 5 | loss: 0.1739958\n\tspeed: 0.1022s/iter; left time: 1402.7962s\nEpoch: 5 cost time: 31.337782382965088\nEpoch: 5, Steps: 305 | Train Loss: 0.1882465 Vali Loss: 0.2189074 Test Loss: 0.1652024\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 6.25e-05\n\titers: 100, epoch: 6 | loss: 0.2335185\n\tspeed: 0.1465s/iter; left time: 1996.5426s\n\titers: 200, epoch: 6 | loss: 0.1579344\n\tspeed: 0.1022s/iter; left time: 1381.7088s\n\titers: 300, epoch: 6 | loss: 0.1505534\n\tspeed: 0.1024s/iter; left time: 1375.3057s\nEpoch: 6 cost time: 31.39163875579834\nEpoch: 6, Steps: 305 | Train Loss: 0.1816566 Vali Loss: 0.2099177 Test Loss: 0.1667280\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 3.125e-05\n\titers: 100, epoch: 7 | loss: 0.2127615\n\tspeed: 0.1467s/iter; left time: 1954.4144s\n\titers: 200, epoch: 7 | loss: 0.2041835\n\tspeed: 0.1023s/iter; left time: 1352.2205s\n\titers: 300, epoch: 7 | loss: 0.1751346\n\tspeed: 0.1020s/iter; left time: 1338.2813s\nEpoch: 7 cost time: 31.337505340576172\nEpoch: 7, Steps: 305 | Train Loss: 0.1781973 Vali Loss: 0.2160093 Test Loss: 0.1692395\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 1.5625e-05\n\titers: 100, epoch: 8 | loss: 0.1622889\n\tspeed: 0.1461s/iter; left time: 1901.9997s\n\titers: 200, epoch: 8 | loss: 0.1592479\n\tspeed: 0.1024s/iter; left time: 1322.3467s\n\titers: 300, epoch: 8 | loss: 0.1921352\n\tspeed: 0.1025s/iter; left time: 1313.2969s\nEpoch: 8 cost time: 31.412315130233765\nEpoch: 8, Steps: 305 | Train Loss: 0.1759567 Vali Loss: 0.2180583 Test Loss: 0.1626241\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 7.8125e-06\n\titers: 100, epoch: 9 | loss: 0.1927279\n\tspeed: 0.1389s/iter; left time: 1765.2822s\n\titers: 200, epoch: 9 | loss: 0.1842425\n\tspeed: 0.0970s/iter; left time: 1223.2757s\n\titers: 300, epoch: 9 | loss: 0.1576418\n\tspeed: 0.0972s/iter; left time: 1215.9912s\nEpoch: 9 cost time: 29.740094900131226\nEpoch: 9, Steps: 305 | Train Loss: 0.1748503 Vali Loss: 0.2176751 Test Loss: 0.1640438\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 3.90625e-06\n\titers: 100, epoch: 10 | loss: 0.1554309\n\tspeed: 0.1354s/iter; left time: 1680.0013s\n\titers: 200, epoch: 10 | loss: 0.1788582\n\tspeed: 0.0967s/iter; left time: 1190.3529s\n\titers: 300, epoch: 10 | loss: 0.1715273\n\tspeed: 0.0958s/iter; left time: 1169.5081s\nEpoch: 10 cost time: 29.02422523498535\nEpoch: 10, Steps: 305 | Train Loss: 0.1743661 Vali Loss: 0.2152923 Test Loss: 0.1638200\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 1.953125e-06\n\titers: 100, epoch: 11 | loss: 0.1733044\n\tspeed: 0.1344s/iter; left time: 1626.3654s\n\titers: 200, epoch: 11 | loss: 0.1813363\n\tspeed: 0.0931s/iter; left time: 1117.0590s\n\titers: 300, epoch: 11 | loss: 0.1372661\n\tspeed: 0.0935s/iter; left time: 1113.3324s\nEpoch: 11 cost time: 28.305574655532837\nEpoch: 11, Steps: 305 | Train Loss: 0.1736492 Vali Loss: 0.2201691 Test Loss: 0.1625625\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 9.765625e-07\n\titers: 100, epoch: 12 | loss: 0.1804820\n\tspeed: 0.1396s/iter; left time: 1646.3059s\n\titers: 200, epoch: 12 | loss: 0.1614305\n\tspeed: 0.0964s/iter; left time: 1126.9309s\n\titers: 300, epoch: 12 | loss: 0.1494740\n\tspeed: 0.0915s/iter; left time: 1061.5654s\nEpoch: 12 cost time: 28.896744966506958\nEpoch: 12, Steps: 305 | Train Loss: 0.1738630 Vali Loss: 0.2198755 Test Loss: 0.1636376\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_exchange_noise20_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1422\ntest shape: (1422, 96, 8) (1422, 96, 8)\ntest shape: (1422, 96, 8) (1422, 96, 8)\nmse:0.22793754935264587, mae:0.3659455478191376, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/833214b0-a23f-436d-984a-2905d122e011","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=25aca1a4-5304-4528-85ec-154f53dfeb1c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2025-04-17T17:54:43.834Z"},"deepnote_notebook_id":"792919e59fe0424faf3a025898ec19bf"}}