{"cells":[{"cell_type":"code","metadata":{"source_hash":"1acff9c4","execution_start":1744901110160,"execution_millis":87,"execution_context_id":"f4031142-3b73-49d3-bce5-2d0dfa2fcd5f","cell_id":"beb6c3dde5814b489f6a8a181f03713f","deepnote_cell_type":"code"},"source":"%cd Time-Series-Library\n!pwd","block_group":"beb6c3dde5814b489f6a8a181f03713f","execution_count":1,"outputs":[{"name":"stdout","text":"/datasets/_deepnote_work/Time-Series-Library\n/datasets/_deepnote_work/Time-Series-Library\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/d52fcd5c-03ca-4cf6-ae60-a020be081532","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"a9d189bf","execution_start":1744902620730,"execution_millis":934,"execution_context_id":"f4031142-3b73-49d3-bce5-2d0dfa2fcd5f","cell_id":"1e17c714bf7a46af963a1e8bb44f9c9c","deepnote_cell_type":"code"},"source":"import pandas as pd\nimport numpy as np\n\ndef add_missing_values(file_path, save_path, missing_ratio=0.1):\n    df = pd.read_csv(file_path)\n\n    # 保留时间列不动（假设第一列是时间戳）\n    numeric_cols = df.columns[1:]\n\n    # 遍历所有数值列，随机置 NaN\n    for col in numeric_cols:\n        mask = np.random.rand(len(df)) < missing_ratio\n        df.loc[mask, col] = np.nan\n\n    # 填补缺失值（先用前一个值填充，再用后一个补充头部缺失）\n    df.fillna(method='ffill', inplace=True)\n    df.fillna(method='bfill', inplace=True)\n\n    # 保存\n    df.to_csv(save_path, index=False)\n    print(f\"✅ Missing values added & filled at {missing_ratio*100:.0f}% ratio. Saved to {save_path}\")\n\n# 设置路径和比例\nfile_path = './dataset/Exchange/exchange_rate.csv'\n\n# 示例：加三个版本并填补\nadd_missing_values(file_path, './dataset/Exchange/exchange_rate_missing5_filled.csv', missing_ratio=0.05)\nadd_missing_values(file_path, './dataset/Exchange/exchange_rate_missing10_filled.csv', missing_ratio=0.10)\nadd_missing_values(file_path, './dataset/Exchange/exchange_rate_missing20_filled.csv', missing_ratio=0.20)","block_group":"eff66f3ad3424078963f5fcd663f799c","execution_count":16,"outputs":[{"name":"stdout","text":"✅ Missing values added & filled at 5% ratio. Saved to ./dataset/Exchange/exchange_rate_missing5_filled.csv\n✅ Missing values added & filled at 10% ratio. Saved to ./dataset/Exchange/exchange_rate_missing10_filled.csv\n✅ Missing values added & filled at 20% ratio. Saved to ./dataset/Exchange/exchange_rate_missing20_filled.csv\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/6744ab44-cbdc-4002-86b0-87236fd858ae","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"b94afce3","execution_start":1744901347760,"execution_millis":1074,"execution_context_id":"f4031142-3b73-49d3-bce5-2d0dfa2fcd5f","cell_id":"92ad476b586045988233997a61fa1fce","deepnote_cell_type":"code"},"source":"!mkdir -p result/Robustness/MissingValues/PatchTST_exchange_missing5\n!mkdir -p result/Robustness/MissingValues/PatchTST_exchange_missing10\n!mkdir -p result/Robustness/MissingValues/PatchTST_exchange_missing20","block_group":"60676f59e96c487fb3549f047774f33f","execution_count":7,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"d7576e3","execution_start":1744902840760,"execution_millis":314742,"execution_context_id":"f4031142-3b73-49d3-bce5-2d0dfa2fcd5f","cell_id":"d535c810870847bd9ebffb08d70c075d","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_exchange_missing5 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/Exchange \\\n--data_path exchange_rate_missing5_filled.csv \\\n--features M \\\n--seq_len 336 \\\n--pred_len 96 \\\n--e_layers 2 \\\n--d_layers 1 \\\n--enc_in 8 \\\n--d_model 256 \\\n--learning_rate 0.001 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--checkpoints ./result/Robustness/MissingValues/PatchTST_exchange_missing5 \\\n--des benchmark_patchtst \\\n2>&1 | tee result/Robustness/MissingValues/PatchTST_exchange_missing5/train_log.txt","block_group":"893c60ee1fb342d8b672d20730e0fdbb","execution_count":25,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_exchange_missing5Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/Exchange  \n  Data Path:          exchange_rate_missing5_filled.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/Robustness/MissingValues/PatchTST_exchange_missing5\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            336                 Label Len:          48                  \n  Pred Len:           96                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             8                   Dec In:             7                   \n  C Out:              7                   d model:            256                 \n  n heads:            8                   e layers:           2                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             1                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_exchange_missing5_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4880\nval 665\ntest 1422\n\titers: 100, epoch: 1 | loss: 0.2864032\n\tspeed: 0.0756s/iter; left time: 1145.9139s\n\titers: 200, epoch: 1 | loss: 0.3554923\n\tspeed: 0.0639s/iter; left time: 962.3893s\n\titers: 300, epoch: 1 | loss: 0.2231696\n\tspeed: 0.0636s/iter; left time: 951.1665s\nEpoch: 1 cost time: 20.653640270233154\nEpoch: 1, Steps: 305 | Train Loss: 0.2317203 Vali Loss: 0.1847133 Test Loss: 0.0945183\nValidation loss decreased (inf --> 0.184713).  Saving model ...\nUpdating learning rate to 0.001\n\titers: 100, epoch: 2 | loss: 0.3395948\n\tspeed: 0.1010s/iter; left time: 1499.9954s\n\titers: 200, epoch: 2 | loss: 0.1682051\n\tspeed: 0.0635s/iter; left time: 936.0936s\n\titers: 300, epoch: 2 | loss: 0.2004906\n\tspeed: 0.0633s/iter; left time: 926.5974s\nEpoch: 2 cost time: 19.537626028060913\nEpoch: 2, Steps: 305 | Train Loss: 0.2040323 Vali Loss: 0.2207912 Test Loss: 0.1295783\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.0005\n\titers: 100, epoch: 3 | loss: 0.2079260\n\tspeed: 0.0933s/iter; left time: 1356.0692s\n\titers: 200, epoch: 3 | loss: 0.2134653\n\tspeed: 0.0634s/iter; left time: 916.1032s\n\titers: 300, epoch: 3 | loss: 0.1766809\n\tspeed: 0.0634s/iter; left time: 908.5822s\nEpoch: 3 cost time: 19.543815851211548\nEpoch: 3, Steps: 305 | Train Loss: 0.1749648 Vali Loss: 0.1656602 Test Loss: 0.2079728\nValidation loss decreased (0.184713 --> 0.165660).  Saving model ...\nUpdating learning rate to 0.00025\n\titers: 100, epoch: 4 | loss: 0.1225776\n\tspeed: 0.1010s/iter; left time: 1437.5515s\n\titers: 200, epoch: 4 | loss: 0.2076461\n\tspeed: 0.0637s/iter; left time: 900.3082s\n\titers: 300, epoch: 4 | loss: 0.1163128\n\tspeed: 0.0640s/iter; left time: 898.9663s\nEpoch: 4 cost time: 19.68403148651123\nEpoch: 4, Steps: 305 | Train Loss: 0.1455963 Vali Loss: 0.1836420 Test Loss: 0.1009873\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.000125\n\titers: 100, epoch: 5 | loss: 0.1390156\n\tspeed: 0.0944s/iter; left time: 1314.9888s\n\titers: 200, epoch: 5 | loss: 0.1463491\n\tspeed: 0.0644s/iter; left time: 890.6019s\n\titers: 300, epoch: 5 | loss: 0.1149727\n\tspeed: 0.0646s/iter; left time: 887.4795s\nEpoch: 5 cost time: 19.844276666641235\nEpoch: 5, Steps: 305 | Train Loss: 0.1319397 Vali Loss: 0.2005999 Test Loss: 0.1025211\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 6.25e-05\n\titers: 100, epoch: 6 | loss: 0.1867657\n\tspeed: 0.0953s/iter; left time: 1299.1040s\n\titers: 200, epoch: 6 | loss: 0.1053467\n\tspeed: 0.0650s/iter; left time: 879.8521s\n\titers: 300, epoch: 6 | loss: 0.0907443\n\tspeed: 0.0651s/iter; left time: 874.6391s\nEpoch: 6 cost time: 20.055174827575684\nEpoch: 6, Steps: 305 | Train Loss: 0.1260811 Vali Loss: 0.2012150 Test Loss: 0.1104472\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 3.125e-05\n\titers: 100, epoch: 7 | loss: 0.1503814\n\tspeed: 0.0951s/iter; left time: 1266.2425s\n\titers: 200, epoch: 7 | loss: 0.1416146\n\tspeed: 0.0646s/iter; left time: 853.7610s\n\titers: 300, epoch: 7 | loss: 0.1291836\n\tspeed: 0.0642s/iter; left time: 842.2505s\nEpoch: 7 cost time: 19.88201379776001\nEpoch: 7, Steps: 305 | Train Loss: 0.1223973 Vali Loss: 0.1907734 Test Loss: 0.1037378\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 1.5625e-05\n\titers: 100, epoch: 8 | loss: 0.1030017\n\tspeed: 0.0940s/iter; left time: 1223.0787s\n\titers: 200, epoch: 8 | loss: 0.1045017\n\tspeed: 0.0640s/iter; left time: 826.7526s\n\titers: 300, epoch: 8 | loss: 0.1551590\n\tspeed: 0.0642s/iter; left time: 822.3152s\nEpoch: 8 cost time: 19.74040985107422\nEpoch: 8, Steps: 305 | Train Loss: 0.1202262 Vali Loss: 0.1945728 Test Loss: 0.0984169\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 7.8125e-06\n\titers: 100, epoch: 9 | loss: 0.1344198\n\tspeed: 0.0940s/iter; left time: 1195.0573s\n\titers: 200, epoch: 9 | loss: 0.1350454\n\tspeed: 0.0644s/iter; left time: 812.1465s\n\titers: 300, epoch: 9 | loss: 0.1042600\n\tspeed: 0.0647s/iter; left time: 809.7667s\nEpoch: 9 cost time: 19.836681127548218\nEpoch: 9, Steps: 305 | Train Loss: 0.1188773 Vali Loss: 0.1905139 Test Loss: 0.1014758\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 3.90625e-06\n\titers: 100, epoch: 10 | loss: 0.0911636\n\tspeed: 0.0952s/iter; left time: 1180.7693s\n\titers: 200, epoch: 10 | loss: 0.1153528\n\tspeed: 0.0651s/iter; left time: 801.4925s\n\titers: 300, epoch: 10 | loss: 0.1089469\n\tspeed: 0.0652s/iter; left time: 795.8309s\nEpoch: 10 cost time: 20.047849893569946\nEpoch: 10, Steps: 305 | Train Loss: 0.1185893 Vali Loss: 0.1919170 Test Loss: 0.1014967\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 1.953125e-06\n\titers: 100, epoch: 11 | loss: 0.1228791\n\tspeed: 0.0951s/iter; left time: 1150.4621s\n\titers: 200, epoch: 11 | loss: 0.1155247\n\tspeed: 0.0645s/iter; left time: 774.1488s\n\titers: 300, epoch: 11 | loss: 0.0818339\n\tspeed: 0.0643s/iter; left time: 765.2348s\nEpoch: 11 cost time: 19.85787343978882\nEpoch: 11, Steps: 305 | Train Loss: 0.1184261 Vali Loss: 0.1946559 Test Loss: 0.1001457\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 9.765625e-07\n\titers: 100, epoch: 12 | loss: 0.1350780\n\tspeed: 0.0945s/iter; left time: 1115.2601s\n\titers: 200, epoch: 12 | loss: 0.1074329\n\tspeed: 0.0643s/iter; left time: 752.1631s\n\titers: 300, epoch: 12 | loss: 0.0885337\n\tspeed: 0.0645s/iter; left time: 747.6367s\nEpoch: 12 cost time: 19.865015745162964\nEpoch: 12, Steps: 305 | Train Loss: 0.1179479 Vali Loss: 0.1945024 Test Loss: 0.0994034\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 4.8828125e-07\n\titers: 100, epoch: 13 | loss: 0.1004499\n\tspeed: 0.0947s/iter; left time: 1087.7844s\n\titers: 200, epoch: 13 | loss: 0.0909729\n\tspeed: 0.0650s/iter; left time: 740.1776s\n\titers: 300, epoch: 13 | loss: 0.1193725\n\tspeed: 0.0652s/iter; left time: 735.8083s\nEpoch: 13 cost time: 20.007166624069214\nEpoch: 13, Steps: 305 | Train Loss: 0.1179787 Vali Loss: 0.1939902 Test Loss: 0.1001619\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_exchange_missing5_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1422\ntest shape: (1422, 96, 8) (1422, 96, 8)\ntest shape: (1422, 96, 8) (1422, 96, 8)\nmse:0.2080349624156952, mae:0.3052237331867218, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/26c2c91f-d772-46e2-9797-f8334012dbe6","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"925b0cc5","execution_start":1744903239240,"execution_millis":612329,"execution_context_id":"f4031142-3b73-49d3-bce5-2d0dfa2fcd5f","cell_id":"cefb243883ca406b94fc5161ef19fb2e","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_exchange_missing10 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/Exchange \\\n--data_path exchange_rate_missing10_filled.csv \\\n--features M \\\n--seq_len 336 \\\n--pred_len 96 \\\n--e_layers 2 \\\n--d_layers 1 \\\n--enc_in 8 \\\n--d_model 256 \\\n--learning_rate 0.001 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--checkpoints ./result/Robustness/MissingValues/PatchTST_exchange_missing10 \\\n--des benchmark_patchtst \\\n2>&1 | tee result/Robustness/MissingValues/PatchTST_exchange_missing10/train_log.txt","block_group":"daddf9893e834fe1817e2aff2866cf4e","execution_count":28,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_exchange_missing10Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/Exchange  \n  Data Path:          exchange_rate_missing10_filled.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/Robustness/MissingValues/PatchTST_exchange_missing10\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            336                 Label Len:          48                  \n  Pred Len:           96                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             8                   Dec In:             7                   \n  C Out:              7                   d model:            256                 \n  n heads:            8                   e layers:           2                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             1                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_exchange_missing10_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4880\nval 665\ntest 1422\n\titers: 100, epoch: 1 | loss: 0.2798336\n\tspeed: 0.1144s/iter; left time: 1733.1677s\n\titers: 200, epoch: 1 | loss: 0.3038395\n\tspeed: 0.1021s/iter; left time: 1536.3371s\n\titers: 300, epoch: 1 | loss: 0.2166248\n\tspeed: 0.1020s/iter; left time: 1525.7432s\nEpoch: 1 cost time: 32.39211678504944\nEpoch: 1, Steps: 305 | Train Loss: 0.2334770 Vali Loss: 0.2547842 Test Loss: 0.1139369\nValidation loss decreased (inf --> 0.254784).  Saving model ...\nUpdating learning rate to 0.001\n\titers: 100, epoch: 2 | loss: 0.3330037\n\tspeed: 0.1507s/iter; left time: 2237.3924s\n\titers: 200, epoch: 2 | loss: 0.1608226\n\tspeed: 0.1021s/iter; left time: 1506.0546s\n\titers: 300, epoch: 2 | loss: 0.2136494\n\tspeed: 0.1022s/iter; left time: 1496.5568s\nEpoch: 2 cost time: 31.28861403465271\nEpoch: 2, Steps: 305 | Train Loss: 0.2106407 Vali Loss: 0.2302556 Test Loss: 0.1780516\nValidation loss decreased (0.254784 --> 0.230256).  Saving model ...\nUpdating learning rate to 0.0005\n\titers: 100, epoch: 3 | loss: 0.1820107\n\tspeed: 0.1515s/iter; left time: 2202.9223s\n\titers: 200, epoch: 3 | loss: 0.2377238\n\tspeed: 0.1025s/iter; left time: 1479.8414s\n\titers: 300, epoch: 3 | loss: 0.1738780\n\tspeed: 0.1024s/iter; left time: 1468.5708s\nEpoch: 3 cost time: 31.36378049850464\nEpoch: 3, Steps: 305 | Train Loss: 0.1779091 Vali Loss: 0.1742267 Test Loss: 0.1316547\nValidation loss decreased (0.230256 --> 0.174227).  Saving model ...\nUpdating learning rate to 0.00025\n\titers: 100, epoch: 4 | loss: 0.1059942\n\tspeed: 0.1523s/iter; left time: 2167.5798s\n\titers: 200, epoch: 4 | loss: 0.2199765\n\tspeed: 0.1024s/iter; left time: 1447.2858s\n\titers: 300, epoch: 4 | loss: 0.1087888\n\tspeed: 0.1024s/iter; left time: 1437.2036s\nEpoch: 4 cost time: 31.363086938858032\nEpoch: 4, Steps: 305 | Train Loss: 0.1503371 Vali Loss: 0.1701452 Test Loss: 0.1059953\nValidation loss decreased (0.174227 --> 0.170145).  Saving model ...\nUpdating learning rate to 0.000125\n\titers: 100, epoch: 5 | loss: 0.1726017\n\tspeed: 0.1460s/iter; left time: 2034.4635s\n\titers: 200, epoch: 5 | loss: 0.1583874\n\tspeed: 0.0970s/iter; left time: 1341.3064s\n\titers: 300, epoch: 5 | loss: 0.1062076\n\tspeed: 0.0970s/iter; left time: 1331.3525s\nEpoch: 5 cost time: 29.810152769088745\nEpoch: 5, Steps: 305 | Train Loss: 0.1405984 Vali Loss: 0.1778875 Test Loss: 0.0967496\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 6.25e-05\n\titers: 100, epoch: 6 | loss: 0.2204884\n\tspeed: 0.1357s/iter; left time: 1848.9036s\n\titers: 200, epoch: 6 | loss: 0.1071739\n\tspeed: 0.0972s/iter; left time: 1314.0613s\n\titers: 300, epoch: 6 | loss: 0.1008397\n\tspeed: 0.0971s/iter; left time: 1303.6108s\nEpoch: 6 cost time: 29.586280584335327\nEpoch: 6, Steps: 305 | Train Loss: 0.1366144 Vali Loss: 0.1651049 Test Loss: 0.1025281\nValidation loss decreased (0.170145 --> 0.165105).  Saving model ...\nUpdating learning rate to 3.125e-05\n\titers: 100, epoch: 7 | loss: 0.1768095\n\tspeed: 0.1469s/iter; left time: 1956.2872s\n\titers: 200, epoch: 7 | loss: 0.1637060\n\tspeed: 0.0972s/iter; left time: 1285.4604s\n\titers: 300, epoch: 7 | loss: 0.1355084\n\tspeed: 0.0967s/iter; left time: 1268.3952s\nEpoch: 7 cost time: 29.796591758728027\nEpoch: 7, Steps: 305 | Train Loss: 0.1344537 Vali Loss: 0.1633663 Test Loss: 0.0982562\nValidation loss decreased (0.165105 --> 0.163366).  Saving model ...\nUpdating learning rate to 1.5625e-05\n\titers: 100, epoch: 8 | loss: 0.1135328\n\tspeed: 0.1468s/iter; left time: 1910.3501s\n\titers: 200, epoch: 8 | loss: 0.1082456\n\tspeed: 0.0890s/iter; left time: 1149.1744s\n\titers: 300, epoch: 8 | loss: 0.1487231\n\tspeed: 0.1022s/iter; left time: 1309.2226s\nEpoch: 8 cost time: 29.434921979904175\nEpoch: 8, Steps: 305 | Train Loss: 0.1323801 Vali Loss: 0.1641522 Test Loss: 0.0951226\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 7.8125e-06\n\titers: 100, epoch: 9 | loss: 0.1456416\n\tspeed: 0.1460s/iter; left time: 1856.2444s\n\titers: 200, epoch: 9 | loss: 0.1465838\n\tspeed: 0.1024s/iter; left time: 1291.3086s\n\titers: 300, epoch: 9 | loss: 0.1005457\n\tspeed: 0.1023s/iter; left time: 1279.3058s\nEpoch: 9 cost time: 31.38395047187805\nEpoch: 9, Steps: 305 | Train Loss: 0.1317631 Vali Loss: 0.1635094 Test Loss: 0.0957770\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 3.90625e-06\n\titers: 100, epoch: 10 | loss: 0.1037081\n\tspeed: 0.1458s/iter; left time: 1809.0594s\n\titers: 200, epoch: 10 | loss: 0.1340911\n\tspeed: 0.1022s/iter; left time: 1258.2264s\n\titers: 300, epoch: 10 | loss: 0.1202627\n\tspeed: 0.1023s/iter; left time: 1248.2753s\nEpoch: 10 cost time: 31.344769716262817\nEpoch: 10, Steps: 305 | Train Loss: 0.1319975 Vali Loss: 0.1640314 Test Loss: 0.0966774\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 1.953125e-06\n\titers: 100, epoch: 11 | loss: 0.1390638\n\tspeed: 0.1459s/iter; left time: 1765.6153s\n\titers: 200, epoch: 11 | loss: 0.1182160\n\tspeed: 0.1023s/iter; left time: 1227.7848s\n\titers: 300, epoch: 11 | loss: 0.0906417\n\tspeed: 0.1022s/iter; left time: 1216.8443s\nEpoch: 11 cost time: 31.362337827682495\nEpoch: 11, Steps: 305 | Train Loss: 0.1315740 Vali Loss: 0.1643712 Test Loss: 0.0961929\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 9.765625e-07\n\titers: 100, epoch: 12 | loss: 0.1660722\n\tspeed: 0.1461s/iter; left time: 1723.6092s\n\titers: 200, epoch: 12 | loss: 0.1176063\n\tspeed: 0.1025s/iter; left time: 1199.0197s\n\titers: 300, epoch: 12 | loss: 0.0915043\n\tspeed: 0.1022s/iter; left time: 1184.7415s\nEpoch: 12 cost time: 31.379050493240356\nEpoch: 12, Steps: 305 | Train Loss: 0.1319265 Vali Loss: 0.1641706 Test Loss: 0.0955876\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 4.8828125e-07\n\titers: 100, epoch: 13 | loss: 0.1120260\n\tspeed: 0.1461s/iter; left time: 1678.5542s\n\titers: 200, epoch: 13 | loss: 0.1013224\n\tspeed: 0.1023s/iter; left time: 1165.6347s\n\titers: 300, epoch: 13 | loss: 0.1421818\n\tspeed: 0.1005s/iter; left time: 1134.7831s\nEpoch: 13 cost time: 31.172375440597534\nEpoch: 13, Steps: 305 | Train Loss: 0.1317406 Vali Loss: 0.1641085 Test Loss: 0.0962396\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 2.44140625e-07\n\titers: 100, epoch: 14 | loss: 0.0887926\n\tspeed: 0.1467s/iter; left time: 1640.6962s\n\titers: 200, epoch: 14 | loss: 0.1456253\n\tspeed: 0.0970s/iter; left time: 1075.1855s\n\titers: 300, epoch: 14 | loss: 0.1420636\n\tspeed: 0.1022s/iter; left time: 1123.0951s\nEpoch: 14 cost time: 30.82513952255249\nEpoch: 14, Steps: 305 | Train Loss: 0.1315633 Vali Loss: 0.1652504 Test Loss: 0.0966025\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 1.220703125e-07\n\titers: 100, epoch: 15 | loss: 0.1882190\n\tspeed: 0.1463s/iter; left time: 1591.5129s\n\titers: 200, epoch: 15 | loss: 0.1554407\n\tspeed: 0.1023s/iter; left time: 1102.5500s\n\titers: 300, epoch: 15 | loss: 0.0979280\n\tspeed: 0.1025s/iter; left time: 1094.5068s\nEpoch: 15 cost time: 31.381292581558228\nEpoch: 15, Steps: 305 | Train Loss: 0.1316810 Vali Loss: 0.1637089 Test Loss: 0.0969237\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 6.103515625e-08\n\titers: 100, epoch: 16 | loss: 0.1409648\n\tspeed: 0.1454s/iter; left time: 1537.2299s\n\titers: 200, epoch: 16 | loss: 0.1292162\n\tspeed: 0.1024s/iter; left time: 1072.2776s\n\titers: 300, epoch: 16 | loss: 0.1288165\n\tspeed: 0.1024s/iter; left time: 1062.8175s\nEpoch: 16 cost time: 31.363508939743042\nEpoch: 16, Steps: 305 | Train Loss: 0.1314001 Vali Loss: 0.1653619 Test Loss: 0.0964312\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 3.0517578125e-08\n\titers: 100, epoch: 17 | loss: 0.1563038\n\tspeed: 0.1460s/iter; left time: 1499.1620s\n\titers: 200, epoch: 17 | loss: 0.0930764\n\tspeed: 0.1024s/iter; left time: 1041.3788s\n\titers: 300, epoch: 17 | loss: 0.1241918\n\tspeed: 0.1023s/iter; left time: 1030.5232s\nEpoch: 17 cost time: 31.404934406280518\nEpoch: 17, Steps: 305 | Train Loss: 0.1310012 Vali Loss: 0.1642518 Test Loss: 0.0960945\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_exchange_missing10_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1422\ntest shape: (1422, 96, 8) (1422, 96, 8)\ntest shape: (1422, 96, 8) (1422, 96, 8)\nmse:0.09821172803640366, mae:0.2232934832572937, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/5558eae3-cad6-4e92-95df-592f7d5db80b","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"d08328fa","execution_start":1744903851620,"execution_millis":444629,"execution_context_id":"f4031142-3b73-49d3-bce5-2d0dfa2fcd5f","cell_id":"9e805812c32b4905a91d26dc93e11270","deepnote_cell_type":"code"},"source":"!python run.py \\\n--is_training 1 \\\n--task_name long_term_forecast \\\n--model_id PatchTST_exchange_missing20 \\\n--model PatchTST \\\n--data custom \\\n--root_path ./dataset/Exchange \\\n--data_path exchange_rate_missing20_filled.csv \\\n--features M \\\n--seq_len 336 \\\n--pred_len 96 \\\n--e_layers 2 \\\n--d_layers 1 \\\n--enc_in 8 \\\n--d_model 256 \\\n--learning_rate 0.001 \\\n--train_epochs 50 \\\n--batch_size 16 \\\n--patience 10 \\\n--checkpoints ./result/Robustness/MissingValues/PatchTST_exchange_missing20 \\\n--des benchmark_patchtst \\\n2>&1 | tee result/Robustness/MissingValues/PatchTST_exchange_missing20/train_log.txt","block_group":"7e158659d7d64000bbc9c884ca1c8368","execution_count":29,"outputs":[{"name":"stdout","text":"Using GPU\nArgs in experiment:\n\u001b[1mBasic Config\u001b[0m\n  Task Name:          long_term_forecast  Is Training:        1                   \n  Model ID:           PatchTST_exchange_missing20Model:              PatchTST            \n\n\u001b[1mData Loader\u001b[0m\n  Data:               custom              Root Path:          ./dataset/Exchange  \n  Data Path:          exchange_rate_missing20_filled.csvFeatures:           M                   \n  Target:             OT                  Freq:               h                   \n  Checkpoints:        ./result/Robustness/MissingValues/PatchTST_exchange_missing20\n\n\u001b[1mForecasting Task\u001b[0m\n  Seq Len:            336                 Label Len:          48                  \n  Pred Len:           96                  Seasonal Patterns:  Monthly             \n  Inverse:            0                   \n\n\u001b[1mModel Parameters\u001b[0m\n  Top k:              5                   Num Kernels:        6                   \n  Enc In:             8                   Dec In:             7                   \n  C Out:              7                   d model:            256                 \n  n heads:            8                   e layers:           2                   \n  d layers:           1                   d FF:               2048                \n  Moving Avg:         25                  Factor:             1                   \n  Distil:             1                   Dropout:            0.1                 \n  Embed:              timeF               Activation:         gelu                \n\n\u001b[1mRun Parameters\u001b[0m\n  Num Workers:        10                  Itr:                1                   \n  Train Epochs:       50                  Batch Size:         16                  \n  Patience:           10                  Learning Rate:      0.001               \n  Des:                benchmark_patchtst  Loss:               MSE                 \n  Lradj:              type1               Use Amp:            0                   \n\n\u001b[1mGPU\u001b[0m\n  Use GPU:            1                   GPU:                0                   \n  Use Multi GPU:      0                   Devices:            0,1,2,3             \n\n\u001b[1mDe-stationary Projector Params\u001b[0m\n  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n\nUse GPU: cuda:0\n>>>>>>>start training : long_term_forecast_PatchTST_exchange_missing20_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0>>>>>>>>>>>>>>>>>>>>>>>>>>\ntrain 4880\nval 665\ntest 1422\n\titers: 100, epoch: 1 | loss: 0.2932058\n\tspeed: 0.1153s/iter; left time: 1747.4302s\n\titers: 200, epoch: 1 | loss: 0.3815643\n\tspeed: 0.1022s/iter; left time: 1538.4306s\n\titers: 300, epoch: 1 | loss: 0.2259871\n\tspeed: 0.1023s/iter; left time: 1530.1565s\nEpoch: 1 cost time: 32.52452039718628\nEpoch: 1, Steps: 305 | Train Loss: 0.2342760 Vali Loss: 0.2127062 Test Loss: 0.1039621\nValidation loss decreased (inf --> 0.212706).  Saving model ...\nUpdating learning rate to 0.001\n\titers: 100, epoch: 2 | loss: 0.3485215\n\tspeed: 0.1505s/iter; left time: 2234.7521s\n\titers: 200, epoch: 2 | loss: 0.1555965\n\tspeed: 0.1022s/iter; left time: 1507.1780s\n\titers: 300, epoch: 2 | loss: 0.2217422\n\tspeed: 0.1022s/iter; left time: 1496.5739s\nEpoch: 2 cost time: 31.34671711921692\nEpoch: 2, Steps: 305 | Train Loss: 0.2058111 Vali Loss: 0.1572732 Test Loss: 0.2220125\nValidation loss decreased (0.212706 --> 0.157273).  Saving model ...\nUpdating learning rate to 0.0005\n\titers: 100, epoch: 3 | loss: 0.1797395\n\tspeed: 0.1535s/iter; left time: 2231.3557s\n\titers: 200, epoch: 3 | loss: 0.2237356\n\tspeed: 0.1021s/iter; left time: 1474.7866s\n\titers: 300, epoch: 3 | loss: 0.1746213\n\tspeed: 0.1021s/iter; left time: 1464.4135s\nEpoch: 3 cost time: 31.343223571777344\nEpoch: 3, Steps: 305 | Train Loss: 0.1796108 Vali Loss: 0.1781336 Test Loss: 0.1190745\nEarlyStopping counter: 1 out of 10\nUpdating learning rate to 0.00025\n\titers: 100, epoch: 4 | loss: 0.1089374\n\tspeed: 0.1460s/iter; left time: 2077.9660s\n\titers: 200, epoch: 4 | loss: 0.2057901\n\tspeed: 0.1022s/iter; left time: 1445.1533s\n\titers: 300, epoch: 4 | loss: 0.1104273\n\tspeed: 0.1024s/iter; left time: 1437.0206s\nEpoch: 4 cost time: 31.37413001060486\nEpoch: 4, Steps: 305 | Train Loss: 0.1464011 Vali Loss: 0.1750341 Test Loss: 0.1033406\nEarlyStopping counter: 2 out of 10\nUpdating learning rate to 0.000125\n\titers: 100, epoch: 5 | loss: 0.1676959\n\tspeed: 0.1466s/iter; left time: 2042.4796s\n\titers: 200, epoch: 5 | loss: 0.1615370\n\tspeed: 0.1023s/iter; left time: 1414.5027s\n\titers: 300, epoch: 5 | loss: 0.0995869\n\tspeed: 0.1026s/iter; left time: 1409.4631s\nEpoch: 5 cost time: 31.40325951576233\nEpoch: 5, Steps: 305 | Train Loss: 0.1317119 Vali Loss: 0.1866182 Test Loss: 0.1164134\nEarlyStopping counter: 3 out of 10\nUpdating learning rate to 6.25e-05\n\titers: 100, epoch: 6 | loss: 0.1580602\n\tspeed: 0.1462s/iter; left time: 1991.6981s\n\titers: 200, epoch: 6 | loss: 0.0954013\n\tspeed: 0.1022s/iter; left time: 1382.4075s\n\titers: 300, epoch: 6 | loss: 0.0907915\n\tspeed: 0.1022s/iter; left time: 1372.0549s\nEpoch: 6 cost time: 31.35520911216736\nEpoch: 6, Steps: 305 | Train Loss: 0.1247140 Vali Loss: 0.2057883 Test Loss: 0.1141076\nEarlyStopping counter: 4 out of 10\nUpdating learning rate to 3.125e-05\n\titers: 100, epoch: 7 | loss: 0.1478002\n\tspeed: 0.1456s/iter; left time: 1940.2017s\n\titers: 200, epoch: 7 | loss: 0.1332620\n\tspeed: 0.1021s/iter; left time: 1350.0978s\n\titers: 300, epoch: 7 | loss: 0.1160969\n\tspeed: 0.1022s/iter; left time: 1341.2341s\nEpoch: 7 cost time: 31.33430528640747\nEpoch: 7, Steps: 305 | Train Loss: 0.1208518 Vali Loss: 0.1900480 Test Loss: 0.1132013\nEarlyStopping counter: 5 out of 10\nUpdating learning rate to 1.5625e-05\n\titers: 100, epoch: 8 | loss: 0.0999253\n\tspeed: 0.1465s/iter; left time: 1906.9720s\n\titers: 200, epoch: 8 | loss: 0.1038809\n\tspeed: 0.1022s/iter; left time: 1319.7572s\n\titers: 300, epoch: 8 | loss: 0.1525833\n\tspeed: 0.1023s/iter; left time: 1310.4606s\nEpoch: 8 cost time: 31.38159418106079\nEpoch: 8, Steps: 305 | Train Loss: 0.1182177 Vali Loss: 0.2006396 Test Loss: 0.1113724\nEarlyStopping counter: 6 out of 10\nUpdating learning rate to 7.8125e-06\n\titers: 100, epoch: 9 | loss: 0.1334947\n\tspeed: 0.1458s/iter; left time: 1853.0064s\n\titers: 200, epoch: 9 | loss: 0.1276723\n\tspeed: 0.1024s/iter; left time: 1291.2193s\n\titers: 300, epoch: 9 | loss: 0.0970061\n\tspeed: 0.1019s/iter; left time: 1275.4165s\nEpoch: 9 cost time: 31.31832790374756\nEpoch: 9, Steps: 305 | Train Loss: 0.1172918 Vali Loss: 0.1923774 Test Loss: 0.1128639\nEarlyStopping counter: 7 out of 10\nUpdating learning rate to 3.90625e-06\n\titers: 100, epoch: 10 | loss: 0.0912104\n\tspeed: 0.1455s/iter; left time: 1805.6241s\n\titers: 200, epoch: 10 | loss: 0.1205095\n\tspeed: 0.1014s/iter; left time: 1247.4870s\n\titers: 300, epoch: 10 | loss: 0.1108617\n\tspeed: 0.1019s/iter; left time: 1243.5197s\nEpoch: 10 cost time: 31.058139324188232\nEpoch: 10, Steps: 305 | Train Loss: 0.1168773 Vali Loss: 0.1923724 Test Loss: 0.1130773\nEarlyStopping counter: 8 out of 10\nUpdating learning rate to 1.953125e-06\n\titers: 100, epoch: 11 | loss: 0.1290296\n\tspeed: 0.1400s/iter; left time: 1694.0859s\n\titers: 200, epoch: 11 | loss: 0.1168674\n\tspeed: 0.1021s/iter; left time: 1225.8017s\n\titers: 300, epoch: 11 | loss: 0.0797230\n\tspeed: 0.1025s/iter; left time: 1220.3079s\nEpoch: 11 cost time: 31.37116050720215\nEpoch: 11, Steps: 305 | Train Loss: 0.1165306 Vali Loss: 0.1968819 Test Loss: 0.1129420\nEarlyStopping counter: 9 out of 10\nUpdating learning rate to 9.765625e-07\n\titers: 100, epoch: 12 | loss: 0.1322406\n\tspeed: 0.1460s/iter; left time: 1722.6898s\n\titers: 200, epoch: 12 | loss: 0.1007078\n\tspeed: 0.1025s/iter; left time: 1198.5738s\n\titers: 300, epoch: 12 | loss: 0.0855566\n\tspeed: 0.1023s/iter; left time: 1186.5133s\nEpoch: 12 cost time: 31.400158405303955\nEpoch: 12, Steps: 305 | Train Loss: 0.1163847 Vali Loss: 0.1967954 Test Loss: 0.1118777\nEarlyStopping counter: 10 out of 10\nEarly stopping\n>>>>>>>testing : long_term_forecast_PatchTST_exchange_missing20_PatchTST_custom_ftM_sl336_ll48_pl96_dm256_nh8_el2_dl1_df2048_expand2_dc4_fc1_ebtimeF_dtTrue_benchmark_patchtst_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\ntest 1422\ntest shape: (1422, 96, 8) (1422, 96, 8)\ntest shape: (1422, 96, 8) (1422, 96, 8)\nmse:0.22202326357364655, mae:0.34485262632369995, dtw:Not calculated\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/2cd15651-d6bc-4a53-a5f4-bcccec3899cd","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=25aca1a4-5304-4528-85ec-154f53dfeb1c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2025-04-17T17:54:43.844Z"},"deepnote_notebook_id":"5f4fd72cf4fc48dfa6ade98bdc252c3e"}}