{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b300b017",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322c0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = [(1,1,0), (2,1,0), (1,1,1), (2,1,1), (3,1,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3608a9",
   "metadata": {},
   "source": [
    "## datasetï¼šelectricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d4d3356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…ARIMA output summaryï¼š\n",
      "  ARIMA (p,d,q)  MSE (mean)  MAE (mean)  RMSE (mean)  MAPE (mean)  \\\n",
      "0     (1, 1, 0)    1.520146    0.931935     1.122130   626.804221   \n",
      "1     (2, 1, 0)    1.459520    0.912953     1.102187   556.008825   \n",
      "2     (1, 1, 1)    1.478330    0.924679     1.108324   615.709813   \n",
      "3     (2, 1, 1)    1.220565    0.823749     1.003301   402.740179   \n",
      "4     (3, 1, 2)    0.896729    0.723728     0.890235   352.798529   \n",
      "\n",
      "    MSPE (mean)  Train Time (mean s)  Predict Time (mean s)  \n",
      "0  2.899220e+06             0.013027               0.002518  \n",
      "1  1.746539e+06             0.022007               0.002270  \n",
      "2  2.620643e+06             0.038710               0.002397  \n",
      "3  4.969518e+05             0.081400               0.002560  \n",
      "4  5.034175e+05             0.207830               0.002644  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "\n",
    "# start time:train\n",
    "train_start = time.time()\n",
    "model = ARIMA(train_series[-seq_len:], order=(1, 1, 0))\n",
    "model_fit = model.fit()\n",
    "train_time = time.time() - train_start  #trainning time (seconds)\n",
    "\n",
    "# start time:predict\n",
    "predict_start = time.time()\n",
    "forecast = model_fit.forecast(steps=pred_len)\n",
    "predict_time = time.time() - predict_start  #predicting time (seconds)\n",
    "\n",
    "\n",
    "# data processing\n",
    "df = pd.read_csv('../DSS5104_TeamWork/data/electricity.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Z-score \n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# parameter settings\n",
    "param_list = [(1,1,0), (2,1,0), (1,1,1), (2,1,1), (3,1,2)]\n",
    "seq_len = 96\n",
    "pred_len = 96\n",
    "train_ratio = 0.77\n",
    "\n",
    "# model training and evaluation\n",
    "summary_results = []\n",
    "\n",
    "for param in param_list:\n",
    "    mse_list, mae_list, rmse_list, mape_list, mspe_list = [], [], [], [], []\n",
    "    train_times, predict_times = [], []\n",
    "\n",
    "    for col in df_scaled.columns:\n",
    "        series = df_scaled[col].values\n",
    "        train_size = int(len(series) * train_ratio)\n",
    "        train_series = series[:train_size]\n",
    "        test_series = series[train_size:train_size + pred_len]\n",
    "\n",
    "        try:\n",
    "            train_start = time.time()\n",
    "            model = ARIMA(train_series[-seq_len:], order=param)\n",
    "            model_fit = model.fit()\n",
    "            train_time = time.time() - train_start\n",
    "\n",
    "            predict_start = time.time()\n",
    "            forecast = model_fit.forecast(steps=pred_len)\n",
    "            predict_time = time.time() - predict_start\n",
    "\n",
    "            mse = mean_squared_error(test_series, forecast)\n",
    "            mae = mean_absolute_error(test_series, forecast)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = np.mean(np.abs((test_series - forecast) / test_series)) * 100\n",
    "            mspe = np.mean(np.square((test_series - forecast) / test_series)) * 100\n",
    "\n",
    "            mse_list.append(mse)\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            mape_list.append(mape)\n",
    "            mspe_list.append(mspe)\n",
    "            train_times.append(train_time)\n",
    "            predict_times.append(predict_time)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    summary_results.append({\n",
    "        'ARIMA (p,d,q)': param,\n",
    "        'MSE (mean)': np.mean(mse_list),\n",
    "        'MAE (mean)': np.mean(mae_list),\n",
    "        'RMSE (mean)': np.mean(rmse_list),\n",
    "        'MAPE (mean)': np.mean(mape_list),\n",
    "        'MSPE (mean)': np.mean(mspe_list),\n",
    "        'Train Time (mean s)': np.mean(train_times),\n",
    "        'Predict Time (mean s)': np.mean(predict_times)\n",
    "    })\n",
    "\n",
    "# output summary\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "print(\"âœ…ARIMA output summaryï¼š\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5fa245",
   "metadata": {},
   "source": [
    "## datasetï¼šILI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c630f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…ARIMA output summaryï¼š\n",
      "  ARIMA (p,d,q)  MSE (mean)  MAE (mean)  RMSE (mean)  MAPE (mean)  \\\n",
      "0     (1, 1, 0)    0.639663    0.597068     0.735715   259.343676   \n",
      "1     (2, 1, 0)    0.702437    0.621719     0.766125   284.788794   \n",
      "2     (1, 1, 1)    0.778307    0.649057     0.800735   300.761157   \n",
      "3     (2, 1, 1)    0.778146    0.647824     0.800441   299.863246   \n",
      "4     (3, 1, 2)    0.708276    0.621266     0.765755   277.591877   \n",
      "\n",
      "   MSPE (mean)  Train Time (mean s)  Predict Time (mean s)  \n",
      "0  4635.269474             0.016109               0.000964  \n",
      "1  5313.260529             0.028181               0.000949  \n",
      "2  5723.907070             0.049288               0.001166  \n",
      "3  5681.742732             0.070122               0.001523  \n",
      "4  5147.999971             0.207768               0.001829  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "\n",
    "# start time:train\n",
    "train_start = time.time()\n",
    "model = ARIMA(train_series[-seq_len:], order=(1, 1, 0))\n",
    "model_fit = model.fit()\n",
    "train_time = time.time() - train_start  #trainning time (seconds)\n",
    "\n",
    "# start time:predict\n",
    "predict_start = time.time()\n",
    "forecast = model_fit.forecast(steps=pred_len)\n",
    "predict_time = time.time() - predict_start  #predicting time (seconds)\n",
    "\n",
    "# data processing\n",
    "df = pd.read_csv('../DSS5104_TeamWork/data/national_illness.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Z-score \n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# parameter settings\n",
    "param_list = [(1,1,0), (2,1,0), (1,1,1), (2,1,1), (3,1,2)]\n",
    "seq_len = 60\n",
    "pred_len = 24\n",
    "train_ratio = 0.77\n",
    "\n",
    "# model training and evaluation\n",
    "summary_results = []\n",
    "\n",
    "for param in param_list:\n",
    "    mse_list, mae_list, rmse_list, mape_list, mspe_list = [], [], [], [], []\n",
    "    train_times, predict_times = [], []\n",
    "\n",
    "    for col in df_scaled.columns:\n",
    "        series = df_scaled[col].values\n",
    "        train_size = int(len(series) * train_ratio)\n",
    "        train_series = series[:train_size]\n",
    "        test_series = series[train_size:train_size + pred_len]\n",
    "\n",
    "        try:\n",
    "            train_start = time.time()\n",
    "            model = ARIMA(train_series[-seq_len:], order=param)\n",
    "            model_fit = model.fit()\n",
    "            train_time = time.time() - train_start\n",
    "\n",
    "            predict_start = time.time()\n",
    "            forecast = model_fit.forecast(steps=pred_len)\n",
    "            predict_time = time.time() - predict_start\n",
    "\n",
    "            mse = mean_squared_error(test_series, forecast)\n",
    "            mae = mean_absolute_error(test_series, forecast)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = np.mean(np.abs((test_series - forecast) / test_series)) * 100\n",
    "            mspe = np.mean(np.square((test_series - forecast) / test_series)) * 100\n",
    "\n",
    "            mse_list.append(mse)\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            mape_list.append(mape)\n",
    "            mspe_list.append(mspe)\n",
    "            train_times.append(train_time)\n",
    "            predict_times.append(predict_time)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    summary_results.append({\n",
    "        'ARIMA (p,d,q)': param,\n",
    "        'MSE (mean)': np.mean(mse_list),\n",
    "        'MAE (mean)': np.mean(mae_list),\n",
    "        'RMSE (mean)': np.mean(rmse_list),\n",
    "        'MAPE (mean)': np.mean(mape_list),\n",
    "        'MSPE (mean)': np.mean(mspe_list),\n",
    "        'Train Time (mean s)': np.mean(train_times),\n",
    "        'Predict Time (mean s)': np.mean(predict_times)\n",
    "    })\n",
    "\n",
    "# output summary\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "print(\"âœ…ARIMA output summaryï¼š\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf5a19",
   "metadata": {},
   "source": [
    "## datasetï¼šTraffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36a4774a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…ARIMA output summaryï¼š\n",
      "  ARIMA (p,d,q)  MSE (mean)  MAE (mean)  RMSE (mean)  MAPE (mean)  \\\n",
      "0     (1, 1, 0)    0.574231    0.546732     0.676584   326.036543   \n",
      "1     (2, 1, 0)    0.578212    0.549931     0.675058   319.399620   \n",
      "2     (1, 1, 1)    0.568561    0.552527     0.676693   316.486161   \n",
      "3     (2, 1, 1)    0.598102    0.572389     0.698529   338.077015   \n",
      "4     (3, 1, 2)    0.610030    0.573860     0.705521   346.655723   \n",
      "\n",
      "     MSPE (mean)  Train Time (mean s)  Predict Time (mean s)  \n",
      "0  253343.387227             0.012464               0.001319  \n",
      "1  305162.964326             0.022583               0.001275  \n",
      "2  218506.911170             0.036864               0.001256  \n",
      "3  255117.590053             0.065671               0.001263  \n",
      "4  215842.050457             0.156434               0.001313  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "\n",
    "# start time:train\n",
    "train_start = time.time()\n",
    "model = ARIMA(train_series[-seq_len:], order=(1, 1, 0))\n",
    "model_fit = model.fit()\n",
    "train_time = time.time() - train_start  #trainning time (seconds)\n",
    "\n",
    "# start time:predict\n",
    "predict_start = time.time()\n",
    "forecast = model_fit.forecast(steps=pred_len)\n",
    "predict_time = time.time() - predict_start  #predicting time (seconds)\n",
    "\n",
    "# data processing\n",
    "df = pd.read_csv('../DSS5104_TeamWork/data/traffic.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Z-score \n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# parameter settings\n",
    "param_list = [(1,1,0), (2,1,0), (1,1,1), (2,1,1), (3,1,2)]\n",
    "seq_len = 60\n",
    "pred_len = 24\n",
    "train_ratio = 0.77\n",
    "\n",
    "# model training and evaluation\n",
    "summary_results = []\n",
    "\n",
    "for param in param_list:\n",
    "    mse_list, mae_list, rmse_list, mape_list, mspe_list = [], [], [], [], []\n",
    "    train_times, predict_times = [], []\n",
    "\n",
    "    for col in df_scaled.columns:\n",
    "        series = df_scaled[col].values\n",
    "        train_size = int(len(series) * train_ratio)\n",
    "        train_series = series[:train_size]\n",
    "        test_series = series[train_size:train_size + pred_len]\n",
    "\n",
    "        try:\n",
    "            train_start = time.time()\n",
    "            model = ARIMA(train_series[-seq_len:], order=param)\n",
    "            model_fit = model.fit()\n",
    "            train_time = time.time() - train_start\n",
    "\n",
    "            predict_start = time.time()\n",
    "            forecast = model_fit.forecast(steps=pred_len)\n",
    "            predict_time = time.time() - predict_start\n",
    "\n",
    "            mse = mean_squared_error(test_series, forecast)\n",
    "            mae = mean_absolute_error(test_series, forecast)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = np.mean(np.abs((test_series - forecast) / test_series)) * 100\n",
    "            mspe = np.mean(np.square((test_series - forecast) / test_series)) * 100\n",
    "\n",
    "            mse_list.append(mse)\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            mape_list.append(mape)\n",
    "            mspe_list.append(mspe)\n",
    "            train_times.append(train_time)\n",
    "            predict_times.append(predict_time)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    summary_results.append({\n",
    "        'ARIMA (p,d,q)': param,\n",
    "        'MSE (mean)': np.mean(mse_list),\n",
    "        'MAE (mean)': np.mean(mae_list),\n",
    "        'RMSE (mean)': np.mean(rmse_list),\n",
    "        'MAPE (mean)': np.mean(mape_list),\n",
    "        'MSPE (mean)': np.mean(mspe_list),\n",
    "        'Train Time (mean s)': np.mean(train_times),\n",
    "        'Predict Time (mean s)': np.mean(predict_times)\n",
    "    })\n",
    "\n",
    "# output summary\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "print(\"âœ…ARIMA output summaryï¼š\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ccd74",
   "metadata": {},
   "source": [
    "## datasetï¼šWeather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae15a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…ARIMA output summaryï¼š\n",
      "  ARIMA (p,d,q)  MSE (mean)  MAE (mean)  RMSE (mean)  MAPE (mean)  \\\n",
      "0     (1, 1, 0)    0.034974    0.060279     0.069774    12.919488   \n",
      "1     (2, 1, 0)    0.032207    0.056784     0.066093    12.129932   \n",
      "2     (1, 1, 1)    0.030943    0.051624     0.061287    11.308488   \n",
      "3     (2, 1, 1)    0.031284    0.052588     0.062153    11.251691   \n",
      "4     (3, 1, 2)    0.026288    0.054022     0.063327    11.852916   \n",
      "\n",
      "   MSPE (mean)  Train Time (mean s)  Predict Time (mean s)  \n",
      "0     5.407654             0.012470               0.001313  \n",
      "1     5.151483             0.030963               0.001056  \n",
      "2     7.520504             0.063833               0.000849  \n",
      "3     4.318399             0.081198               0.001041  \n",
      "4     4.696286             0.150500               0.000861  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "\n",
    "# start time:train\n",
    "train_start = time.time()\n",
    "model = ARIMA(train_series[-seq_len:], order=(1, 1, 0))\n",
    "model_fit = model.fit()\n",
    "train_time = time.time() - train_start  #trainning time (seconds)\n",
    "\n",
    "# start time:predict\n",
    "predict_start = time.time()\n",
    "forecast = model_fit.forecast(steps=pred_len)\n",
    "predict_time = time.time() - predict_start  #predicting time (seconds)\n",
    "\n",
    "# data processing\n",
    "df = pd.read_csv('../DSS5104_TeamWork/data/weather.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Z-score \n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# parameter settings\n",
    "param_list = [(1,1,0), (2,1,0), (1,1,1), (2,1,1), (3,1,2)]\n",
    "seq_len = 60\n",
    "pred_len = 24\n",
    "train_ratio = 0.77\n",
    "\n",
    "# model training and evaluation\n",
    "summary_results = []\n",
    "\n",
    "for param in param_list:\n",
    "    mse_list, mae_list, rmse_list, mape_list, mspe_list = [], [], [], [], []\n",
    "    train_times, predict_times = [], []\n",
    "\n",
    "    for col in df_scaled.columns:\n",
    "        series = df_scaled[col].values\n",
    "        train_size = int(len(series) * train_ratio)\n",
    "        train_series = series[:train_size]\n",
    "        test_series = series[train_size:train_size + pred_len]\n",
    "\n",
    "        try:\n",
    "            train_start = time.time()\n",
    "            model = ARIMA(train_series[-seq_len:], order=param)\n",
    "            model_fit = model.fit()\n",
    "            train_time = time.time() - train_start\n",
    "\n",
    "            predict_start = time.time()\n",
    "            forecast = model_fit.forecast(steps=pred_len)\n",
    "            predict_time = time.time() - predict_start\n",
    "\n",
    "            mse = mean_squared_error(test_series, forecast)\n",
    "            mae = mean_absolute_error(test_series, forecast)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = np.mean(np.abs((test_series - forecast) / test_series)) * 100\n",
    "            mspe = np.mean(np.square((test_series - forecast) / test_series)) * 100\n",
    "\n",
    "            mse_list.append(mse)\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            mape_list.append(mape)\n",
    "            mspe_list.append(mspe)\n",
    "            train_times.append(train_time)\n",
    "            predict_times.append(predict_time)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    summary_results.append({\n",
    "        'ARIMA (p,d,q)': param,\n",
    "        'MSE (mean)': np.mean(mse_list),\n",
    "        'MAE (mean)': np.mean(mae_list),\n",
    "        'RMSE (mean)': np.mean(rmse_list),\n",
    "        'MAPE (mean)': np.mean(mape_list),\n",
    "        'MSPE (mean)': np.mean(mspe_list),\n",
    "        'Train Time (mean s)': np.mean(train_times),\n",
    "        'Predict Time (mean s)': np.mean(predict_times)\n",
    "    })\n",
    "\n",
    "# output summary\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "print(\"âœ…ARIMA output summaryï¼š\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bbe3f0",
   "metadata": {},
   "source": [
    "## datasetï¼šExchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db3c1d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…ARIMA output summaryï¼š\n",
      "  ARIMA (p,d,q)  MSE (mean)  MAE (mean)  RMSE (mean)  MAPE (mean)  \\\n",
      "0     (1, 1, 0)    0.042029    0.166289     0.184061    16.358474   \n",
      "1     (2, 1, 0)    0.041135    0.164464     0.182103    15.919832   \n",
      "2     (1, 1, 1)    0.041965    0.166754     0.184517    15.920315   \n",
      "3     (2, 1, 1)    0.040957    0.164124     0.181749    15.863293   \n",
      "4     (3, 1, 2)    0.042987    0.168868     0.186271    16.122773   \n",
      "\n",
      "   MSPE (mean)  Train Time (mean s)  Predict Time (mean s)  \n",
      "0     7.551899             0.016375               0.002191  \n",
      "1     6.922385             0.032552               0.000910  \n",
      "2     6.709979             0.053335               0.001464  \n",
      "3     6.852107             0.067804               0.000714  \n",
      "4     6.812041             0.189027               0.000973  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "\n",
    "# start time:train\n",
    "train_start = time.time()\n",
    "model = ARIMA(train_series[-seq_len:], order=(1, 1, 0))\n",
    "model_fit = model.fit()\n",
    "train_time = time.time() - train_start  #trainning time (seconds)\n",
    "\n",
    "# start time:predict\n",
    "predict_start = time.time()\n",
    "forecast = model_fit.forecast(steps=pred_len)\n",
    "predict_time = time.time() - predict_start  #predicting time (seconds)\n",
    "\n",
    "# data processing\n",
    "df = pd.read_csv('../DSS5104_TeamWork/data/exchange_rate.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Z-score \n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# parameter settings\n",
    "param_list = [(1,1,0), (2,1,0), (1,1,1), (2,1,1), (3,1,2)]\n",
    "seq_len = 60\n",
    "pred_len = 24\n",
    "train_ratio = 0.77\n",
    "\n",
    "# model training and evaluation\n",
    "summary_results = []\n",
    "\n",
    "for param in param_list:\n",
    "    mse_list, mae_list, rmse_list, mape_list, mspe_list = [], [], [], [], []\n",
    "    train_times, predict_times = [], []\n",
    "\n",
    "    for col in df_scaled.columns:\n",
    "        series = df_scaled[col].values\n",
    "        train_size = int(len(series) * train_ratio)\n",
    "        train_series = series[:train_size]\n",
    "        test_series = series[train_size:train_size + pred_len]\n",
    "\n",
    "        try:\n",
    "            train_start = time.time()\n",
    "            model = ARIMA(train_series[-seq_len:], order=param)\n",
    "            model_fit = model.fit()\n",
    "            train_time = time.time() - train_start\n",
    "\n",
    "            predict_start = time.time()\n",
    "            forecast = model_fit.forecast(steps=pred_len)\n",
    "            predict_time = time.time() - predict_start\n",
    "\n",
    "            mse = mean_squared_error(test_series, forecast)\n",
    "            mae = mean_absolute_error(test_series, forecast)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mape = np.mean(np.abs((test_series - forecast) / test_series)) * 100\n",
    "            mspe = np.mean(np.square((test_series - forecast) / test_series)) * 100\n",
    "\n",
    "            mse_list.append(mse)\n",
    "            mae_list.append(mae)\n",
    "            rmse_list.append(rmse)\n",
    "            mape_list.append(mape)\n",
    "            mspe_list.append(mspe)\n",
    "            train_times.append(train_time)\n",
    "            predict_times.append(predict_time)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    summary_results.append({\n",
    "        'ARIMA (p,d,q)': param,\n",
    "        'MSE (mean)': np.mean(mse_list),\n",
    "        'MAE (mean)': np.mean(mae_list),\n",
    "        'RMSE (mean)': np.mean(rmse_list),\n",
    "        'MAPE (mean)': np.mean(mape_list),\n",
    "        'MSPE (mean)': np.mean(mspe_list),\n",
    "        'Train Time (mean s)': np.mean(train_times),\n",
    "        'Predict Time (mean s)': np.mean(predict_times)\n",
    "    })\n",
    "\n",
    "# output summary\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "print(\"âœ…ARIMA output summaryï¼š\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702b673",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b09b2c",
   "metadata": {},
   "source": [
    "## dataset:electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b6ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/321] Running: 0\n",
      "âœ… Done: 0 in 77.25 sec\n",
      "[2/321] Running: 1\n",
      "âœ… Done: 1 in 94.34 sec\n",
      "[3/321] Running: 2\n",
      "âœ… Done: 2 in 44.55 sec\n",
      "[4/321] Running: 3\n",
      "âœ… Done: 3 in 125.35 sec\n",
      "[5/321] Running: 4\n",
      "âœ… Done: 4 in 116.04 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 91.50 sec\n",
      "[6/321] Running: 5\n",
      "âœ… Done: 5 in 126.72 sec\n",
      "[7/321] Running: 6\n",
      "âœ… Done: 6 in 73.28 sec\n",
      "[8/321] Running: 7\n",
      "âœ… Done: 7 in 137.76 sec\n",
      "[9/321] Running: 8\n",
      "âœ… Done: 8 in 120.08 sec\n",
      "[10/321] Running: 9\n",
      "âœ… Done: 9 in 123.59 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 116.29 sec\n",
      "[11/321] Running: 10\n",
      "âœ… Done: 10 in 107.47 sec\n",
      "[12/321] Running: 11\n",
      "âœ… Done: 11 in 124.29 sec\n",
      "[13/321] Running: 12\n",
      "âœ… Done: 12 in 112.05 sec\n",
      "[14/321] Running: 13\n",
      "âœ… Done: 13 in 105.26 sec\n",
      "[15/321] Running: 14\n",
      "âœ… Done: 14 in 109.00 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 111.61 sec\n",
      "[16/321] Running: 15\n",
      "âœ… Done: 15 in 148.95 sec\n",
      "[17/321] Running: 16\n",
      "âœ… Done: 16 in 74.99 sec\n",
      "[18/321] Running: 17\n",
      "âœ… Done: 17 in 106.74 sec\n",
      "[19/321] Running: 18\n",
      "âœ… Done: 18 in 137.30 sec\n",
      "[20/321] Running: 19\n",
      "âœ… Done: 19 in 106.09 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 114.81 sec\n",
      "[21/321] Running: 20\n",
      "âœ… Done: 20 in 82.64 sec\n",
      "[22/321] Running: 21\n",
      "âœ… Done: 21 in 132.80 sec\n",
      "[23/321] Running: 22\n",
      "âœ… Done: 22 in 96.09 sec\n",
      "[24/321] Running: 23\n",
      "âœ… Done: 23 in 97.60 sec\n",
      "[25/321] Running: 24\n",
      "âœ… Done: 24 in 120.24 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 105.87 sec\n",
      "[26/321] Running: 25\n",
      "âœ… Done: 25 in 121.37 sec\n",
      "[27/321] Running: 26\n",
      "âœ… Done: 26 in 127.90 sec\n",
      "[28/321] Running: 27\n",
      "âœ… Done: 27 in 87.27 sec\n",
      "[29/321] Running: 28\n",
      "âœ… Done: 28 in 130.01 sec\n",
      "[30/321] Running: 29\n",
      "âœ… Done: 29 in 120.41 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 117.39 sec\n",
      "[31/321] Running: 30\n",
      "âœ… Done: 30 in 131.36 sec\n",
      "[32/321] Running: 31\n",
      "âœ… Done: 31 in 128.87 sec\n",
      "[33/321] Running: 32\n",
      "âœ… Done: 32 in 139.99 sec\n",
      "[34/321] Running: 33\n",
      "âœ… Done: 33 in 108.37 sec\n",
      "[35/321] Running: 34\n",
      "âœ… Done: 34 in 134.55 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 128.63 sec\n",
      "[36/321] Running: 35\n",
      "âœ… Done: 35 in 106.15 sec\n",
      "[37/321] Running: 36\n",
      "âœ… Done: 36 in 134.34 sec\n",
      "[38/321] Running: 37\n",
      "âœ… Done: 37 in 121.01 sec\n",
      "[39/321] Running: 38\n",
      "âœ… Done: 38 in 106.60 sec\n",
      "[40/321] Running: 39\n",
      "âœ… Done: 39 in 89.97 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 111.61 sec\n",
      "[41/321] Running: 40\n",
      "âœ… Done: 40 in 147.68 sec\n",
      "[42/321] Running: 41\n",
      "âœ… Done: 41 in 139.55 sec\n",
      "[43/321] Running: 42\n",
      "âœ… Done: 42 in 114.48 sec\n",
      "[44/321] Running: 43\n",
      "âœ… Done: 43 in 103.86 sec\n",
      "[45/321] Running: 44\n",
      "âœ… Done: 44 in 124.77 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 126.07 sec\n",
      "[46/321] Running: 45\n",
      "âœ… Done: 45 in 119.43 sec\n",
      "[47/321] Running: 46\n",
      "âœ… Done: 46 in 114.82 sec\n",
      "[48/321] Running: 47\n",
      "âœ… Done: 47 in 117.06 sec\n",
      "[49/321] Running: 48\n",
      "âœ… Done: 48 in 117.91 sec\n",
      "[50/321] Running: 49\n",
      "âœ… Done: 49 in 144.97 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 122.84 sec\n",
      "[51/321] Running: 50\n",
      "âœ… Done: 50 in 148.82 sec\n",
      "[52/321] Running: 51\n",
      "âœ… Done: 51 in 108.69 sec\n",
      "[53/321] Running: 52\n",
      "âœ… Done: 52 in 143.73 sec\n",
      "[54/321] Running: 53\n",
      "âœ… Done: 53 in 129.31 sec\n",
      "[55/321] Running: 54\n",
      "âœ… Done: 54 in 112.65 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 128.64 sec\n",
      "[56/321] Running: 55\n",
      "âœ… Done: 55 in 151.98 sec\n",
      "[57/321] Running: 56\n",
      "âœ… Done: 56 in 126.18 sec\n",
      "[58/321] Running: 57\n",
      "âœ… Done: 57 in 112.23 sec\n",
      "[59/321] Running: 58\n",
      "âœ… Done: 58 in 145.06 sec\n",
      "[60/321] Running: 59\n",
      "âœ… Done: 59 in 168.35 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 140.76 sec\n",
      "[61/321] Running: 60\n",
      "âœ… Done: 60 in 156.86 sec\n",
      "[62/321] Running: 61\n",
      "âœ… Done: 61 in 114.80 sec\n",
      "[63/321] Running: 62\n",
      "âœ… Done: 62 in 130.29 sec\n",
      "[64/321] Running: 63\n",
      "âœ… Done: 63 in 138.12 sec\n",
      "[65/321] Running: 64\n",
      "âœ… Done: 64 in 134.72 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 134.96 sec\n",
      "[66/321] Running: 65\n",
      "âœ… Done: 65 in 138.88 sec\n",
      "[67/321] Running: 66\n",
      "âœ… Done: 66 in 119.83 sec\n",
      "[68/321] Running: 67\n",
      "âœ… Done: 67 in 130.23 sec\n",
      "[69/321] Running: 68\n",
      "âœ… Done: 68 in 125.72 sec\n",
      "[70/321] Running: 69\n",
      "âœ… Done: 69 in 113.20 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 125.57 sec\n",
      "[71/321] Running: 70\n",
      "âœ… Done: 70 in 119.97 sec\n",
      "[72/321] Running: 71\n",
      "âœ… Done: 71 in 101.51 sec\n",
      "[73/321] Running: 72\n",
      "âœ… Done: 72 in 144.03 sec\n",
      "[74/321] Running: 73\n",
      "âœ… Done: 73 in 118.65 sec\n",
      "[75/321] Running: 74\n",
      "âœ… Done: 74 in 134.06 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 123.64 sec\n",
      "[76/321] Running: 75\n",
      "âœ… Done: 75 in 164.89 sec\n",
      "[77/321] Running: 76\n",
      "âœ… Done: 76 in 151.50 sec\n",
      "[78/321] Running: 77\n",
      "âœ… Done: 77 in 143.17 sec\n",
      "[79/321] Running: 78\n",
      "âœ… Done: 78 in 146.97 sec\n",
      "[80/321] Running: 79\n",
      "âœ… Done: 79 in 141.20 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 149.55 sec\n",
      "[81/321] Running: 80\n",
      "âœ… Done: 80 in 160.99 sec\n",
      "[82/321] Running: 81\n",
      "âœ… Done: 81 in 154.51 sec\n",
      "[83/321] Running: 82\n",
      "âœ… Done: 82 in 154.82 sec\n",
      "[84/321] Running: 83\n",
      "âœ… Done: 83 in 101.92 sec\n",
      "[85/321] Running: 84\n",
      "âœ… Done: 84 in 127.44 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 139.94 sec\n",
      "[86/321] Running: 85\n",
      "âœ… Done: 85 in 106.06 sec\n",
      "[87/321] Running: 86\n",
      "âœ… Done: 86 in 104.41 sec\n",
      "[88/321] Running: 87\n",
      "âœ… Done: 87 in 166.35 sec\n",
      "[89/321] Running: 88\n",
      "âœ… Done: 88 in 211.56 sec\n",
      "[90/321] Running: 89\n",
      "âœ… Done: 89 in 231.77 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 164.03 sec\n",
      "[91/321] Running: 90\n",
      "âœ… Done: 90 in 133.39 sec\n",
      "[92/321] Running: 91\n",
      "âœ… Done: 91 in 226.08 sec\n",
      "[93/321] Running: 92\n",
      "âœ… Done: 92 in 234.42 sec\n",
      "[94/321] Running: 93\n",
      "âœ… Done: 93 in 227.17 sec\n",
      "[95/321] Running: 94\n",
      "âœ… Done: 94 in 224.07 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 209.03 sec\n",
      "[96/321] Running: 95\n",
      "âœ… Done: 95 in 177.63 sec\n",
      "[97/321] Running: 96\n",
      "âœ… Done: 96 in 172.84 sec\n",
      "[98/321] Running: 97\n",
      "âœ… Done: 97 in 148.42 sec\n",
      "[99/321] Running: 98\n",
      "âœ… Done: 98 in 140.70 sec\n",
      "[100/321] Running: 99\n",
      "âœ… Done: 99 in 134.17 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 154.75 sec\n",
      "[101/321] Running: 100\n",
      "âœ… Done: 100 in 164.41 sec\n",
      "[102/321] Running: 101\n",
      "âœ… Done: 101 in 123.17 sec\n",
      "[103/321] Running: 102\n",
      "âœ… Done: 102 in 129.78 sec\n",
      "[104/321] Running: 103\n",
      "âœ… Done: 103 in 195.50 sec\n",
      "[105/321] Running: 104\n",
      "âœ… Done: 104 in 135.81 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 149.73 sec\n",
      "[106/321] Running: 105\n",
      "âœ… Done: 105 in 52.86 sec\n",
      "[107/321] Running: 106\n",
      "âœ… Done: 106 in 84.54 sec\n",
      "[108/321] Running: 107\n",
      "âœ… Done: 107 in 60.82 sec\n",
      "[109/321] Running: 108\n",
      "âœ… Done: 108 in 109.75 sec\n",
      "[110/321] Running: 109\n",
      "âœ… Done: 109 in 144.87 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 90.57 sec\n",
      "[111/321] Running: 110\n",
      "âœ… Done: 110 in 131.60 sec\n",
      "[112/321] Running: 111\n",
      "âœ… Done: 111 in 142.36 sec\n",
      "[113/321] Running: 112\n",
      "âœ… Done: 112 in 187.79 sec\n",
      "[114/321] Running: 113\n",
      "âœ… Done: 113 in 98.49 sec\n",
      "[115/321] Running: 114\n",
      "âœ… Done: 114 in 58.21 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 123.69 sec\n",
      "[116/321] Running: 115\n",
      "âœ… Done: 115 in 114.76 sec\n",
      "[117/321] Running: 116\n",
      "âœ… Done: 116 in 145.48 sec\n",
      "[118/321] Running: 117\n",
      "âœ… Done: 117 in 97.82 sec\n",
      "[119/321] Running: 118\n",
      "âœ… Done: 118 in 97.60 sec\n",
      "[120/321] Running: 119\n",
      "âœ… Done: 119 in 182.78 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 127.69 sec\n",
      "[121/321] Running: 120\n",
      "âœ… Done: 120 in 132.04 sec\n",
      "[122/321] Running: 121\n",
      "âœ… Done: 121 in 81.66 sec\n",
      "[123/321] Running: 122\n",
      "âœ… Done: 122 in 73.99 sec\n",
      "[124/321] Running: 123\n",
      "âœ… Done: 123 in 148.98 sec\n",
      "[125/321] Running: 124\n",
      "âœ… Done: 124 in 152.35 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 117.80 sec\n",
      "[126/321] Running: 125\n",
      "âœ… Done: 125 in 84.87 sec\n",
      "[127/321] Running: 126\n",
      "âœ… Done: 126 in 156.33 sec\n",
      "[128/321] Running: 127\n",
      "âœ… Done: 127 in 125.51 sec\n",
      "[129/321] Running: 128\n",
      "âœ… Done: 128 in 243.56 sec\n",
      "[130/321] Running: 129\n",
      "âœ… Done: 129 in 148.64 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 151.78 sec\n",
      "[131/321] Running: 130\n",
      "âœ… Done: 130 in 139.20 sec\n",
      "[132/321] Running: 131\n",
      "âœ… Done: 131 in 227.34 sec\n",
      "[133/321] Running: 132\n",
      "âœ… Done: 132 in 187.51 sec\n",
      "[134/321] Running: 133\n",
      "âœ… Done: 133 in 238.06 sec\n",
      "[135/321] Running: 134\n",
      "âœ… Done: 134 in 160.85 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 190.59 sec\n",
      "[136/321] Running: 135\n",
      "âœ… Done: 135 in 147.94 sec\n",
      "[137/321] Running: 136\n",
      "âœ… Done: 136 in 142.41 sec\n",
      "[138/321] Running: 137\n",
      "âœ… Done: 137 in 114.19 sec\n",
      "[139/321] Running: 138\n",
      "âœ… Done: 138 in 143.61 sec\n",
      "[140/321] Running: 139\n",
      "âœ… Done: 139 in 147.05 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 139.04 sec\n",
      "[141/321] Running: 140\n",
      "âœ… Done: 140 in 151.85 sec\n",
      "[142/321] Running: 141\n",
      "âœ… Done: 141 in 153.53 sec\n",
      "[143/321] Running: 142\n",
      "âœ… Done: 142 in 133.16 sec\n",
      "[144/321] Running: 143\n",
      "âœ… Done: 143 in 150.38 sec\n",
      "[145/321] Running: 144\n",
      "âœ… Done: 144 in 119.45 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 141.67 sec\n",
      "[146/321] Running: 145\n",
      "âœ… Done: 145 in 136.70 sec\n",
      "[147/321] Running: 146\n",
      "âœ… Done: 146 in 108.09 sec\n",
      "[148/321] Running: 147\n",
      "âœ… Done: 147 in 107.81 sec\n",
      "[149/321] Running: 148\n",
      "âœ… Done: 148 in 188.01 sec\n",
      "[150/321] Running: 149\n",
      "âœ… Done: 149 in 179.16 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 143.95 sec\n",
      "[151/321] Running: 150\n",
      "âœ… Done: 150 in 200.87 sec\n",
      "[152/321] Running: 151\n",
      "âœ… Done: 151 in 190.24 sec\n",
      "[153/321] Running: 152\n",
      "âœ… Done: 152 in 171.38 sec\n",
      "[154/321] Running: 153\n",
      "âœ… Done: 153 in 233.91 sec\n",
      "[155/321] Running: 154\n",
      "âœ… Done: 154 in 189.16 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 197.11 sec\n",
      "[156/321] Running: 155\n",
      "âœ… Done: 155 in 235.24 sec\n",
      "[157/321] Running: 156\n",
      "âœ… Done: 156 in 152.40 sec\n",
      "[158/321] Running: 157\n",
      "âœ… Done: 157 in 172.66 sec\n",
      "[159/321] Running: 158\n",
      "âœ… Done: 158 in 190.59 sec\n",
      "[160/321] Running: 159\n",
      "âœ… Done: 159 in 218.33 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 193.84 sec\n",
      "[161/321] Running: 160\n",
      "âœ… Done: 160 in 159.61 sec\n",
      "[162/321] Running: 161\n",
      "âœ… Done: 161 in 166.52 sec\n",
      "[163/321] Running: 162\n",
      "âœ… Done: 162 in 194.19 sec\n",
      "[164/321] Running: 163\n",
      "âœ… Done: 163 in 220.73 sec\n",
      "[165/321] Running: 164\n",
      "âœ… Done: 164 in 187.26 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 185.66 sec\n",
      "[166/321] Running: 165\n",
      "âœ… Done: 165 in 161.85 sec\n",
      "[167/321] Running: 166\n",
      "âœ… Done: 166 in 214.22 sec\n",
      "[168/321] Running: 167\n",
      "âœ… Done: 167 in 245.63 sec\n",
      "[169/321] Running: 168\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m start_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     51\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_train\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    876\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[0;32m    877\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[0;32m    878\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    879\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    882\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    486\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    488\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 489\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    494\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# 1. Load and clean the data\n",
    "df = pd.read_csv('../DSS5104_TeamWork/data/electricity.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# 2. Normalize with Z-score\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# 3. Parameters\n",
    "seq_len = 336\n",
    "pred_len = 96\n",
    "train_ratio = 0.77\n",
    "metrics = []\n",
    "\n",
    "# Start total timer\n",
    "total_start = time.time()\n",
    "checkpoints = []\n",
    "\n",
    "# 4. Loop over each variable\n",
    "for i, col in enumerate(df_scaled.columns):\n",
    "    print(f\"[{i+1}/{len(df_scaled.columns)}] Running: {col}\")\n",
    "    series = df_scaled[col].values\n",
    "\n",
    "    # Create input/output samples\n",
    "    X, y = [], []\n",
    "    for j in range(len(series) - seq_len - pred_len):\n",
    "        X.append(series[j:j+seq_len])\n",
    "        y.append(series[j+seq_len:j+seq_len+pred_len])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # Split into train/test sets\n",
    "    train_size = int(len(X) * train_ratio)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model (1-step ahead)\n",
    "        start_train = time.time()\n",
    "        model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "        model.fit(X_train, y_train[:, 0])\n",
    "        train_time = time.time() - start_train\n",
    "\n",
    "        # Predict\n",
    "        start_pred = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        predict_time = time.time() - start_pred\n",
    "\n",
    "        y_true = y_test[:, 0]\n",
    "\n",
    "        # Evaluation metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        mspe = np.mean(np.square((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        metrics.append({\n",
    "            'Variable': col,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'MSPE (%)': mspe,\n",
    "            'Train Time (s)': train_time,\n",
    "            'Predict Time (s)': predict_time\n",
    "        })\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        checkpoints.append(elapsed)\n",
    "\n",
    "        # Print per-variable time\n",
    "        print(f\"âœ… Done: {col} in {elapsed:.2f} sec\")\n",
    "\n",
    "        # Checkpoint every 5 variables\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"ðŸ•’ Avg time per variable (last 5): {np.mean(checkpoints[-5:]):.2f} sec\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Skipped {col} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# End total timer\n",
    "total_elapsed = time.time() - total_start\n",
    "\n",
    "# 5. Show results\n",
    "result_df = pd.DataFrame(metrics)\n",
    "print(\"\\nâœ… Average performance across all variables:\")\n",
    "print(result_df.mean(numeric_only=True))\n",
    "\n",
    "print(f\"\\nðŸ•’ Total runtime: {total_elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1497150",
   "metadata": {},
   "source": [
    "## dataset:ILI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da86e359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] Running: % WEIGHTED ILI\n",
      "âœ… Done: % WEIGHTED ILI in 1.59 sec\n",
      "[2/7] Running: %UNWEIGHTED ILI\n",
      "âœ… Done: %UNWEIGHTED ILI in 1.54 sec\n",
      "[3/7] Running: AGE 0-4\n",
      "âœ… Done: AGE 0-4 in 1.49 sec\n",
      "[4/7] Running: AGE 5-24\n",
      "âœ… Done: AGE 5-24 in 1.65 sec\n",
      "[5/7] Running: ILITOTAL\n",
      "âœ… Done: ILITOTAL in 1.57 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 1.57 sec\n",
      "[6/7] Running: NUM. OF PROVIDERS\n",
      "âœ… Done: NUM. OF PROVIDERS in 1.37 sec\n",
      "[7/7] Running: OT\n",
      "âœ… Done: OT in 1.44 sec\n",
      "\n",
      "âœ… Average performance across all variables:\n",
      "MSE                   0.593648\n",
      "MAE                   0.385216\n",
      "RMSE                  0.732177\n",
      "MAPE (%)             55.317933\n",
      "MSPE (%)            943.581557\n",
      "Train Time (s)        1.521770\n",
      "Predict Time (s)      0.001505\n",
      "dtype: float64\n",
      "\n",
      "ðŸ•’ Total runtime: 10.68 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# 1. Load and clean the data\n",
    "df = pd.read_csv('../DSS5104_TeamWork/data/national_illness.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# 2. Normalize with Z-score\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# 3. Parameters\n",
    "seq_len = 60\n",
    "pred_len = 60\n",
    "train_ratio = 0.77\n",
    "metrics = []\n",
    "\n",
    "# Start total timer\n",
    "total_start = time.time()\n",
    "checkpoints = []\n",
    "\n",
    "# 4. Loop over each variable\n",
    "for i, col in enumerate(df_scaled.columns):\n",
    "    print(f\"[{i+1}/{len(df_scaled.columns)}] Running: {col}\")\n",
    "    series = df_scaled[col].values\n",
    "\n",
    "    # Create input/output samples\n",
    "    X, y = [], []\n",
    "    for j in range(len(series) - seq_len - pred_len):\n",
    "        X.append(series[j:j+seq_len])\n",
    "        y.append(series[j+seq_len:j+seq_len+pred_len])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # Split into train/test sets\n",
    "    train_size = int(len(X) * train_ratio)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model (1-step ahead)\n",
    "        start_train = time.time()\n",
    "        model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "        model.fit(X_train, y_train[:, 0])\n",
    "        train_time = time.time() - start_train\n",
    "\n",
    "        # Predict\n",
    "        start_pred = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        predict_time = time.time() - start_pred\n",
    "\n",
    "        y_true = y_test[:, 0]\n",
    "\n",
    "        # Evaluation metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        mspe = np.mean(np.square((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        metrics.append({\n",
    "            'Variable': col,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'MSPE (%)': mspe,\n",
    "            'Train Time (s)': train_time,\n",
    "            'Predict Time (s)': predict_time\n",
    "        })\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        checkpoints.append(elapsed)\n",
    "\n",
    "        # Print per-variable time\n",
    "        print(f\"âœ… Done: {col} in {elapsed:.2f} sec\")\n",
    "\n",
    "        # Checkpoint every 5 variables\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"ðŸ•’ Avg time per variable (last 5): {np.mean(checkpoints[-5:]):.2f} sec\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Skipped {col} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# End total timer\n",
    "total_elapsed = time.time() - total_start\n",
    "\n",
    "# 5. Show results\n",
    "result_df = pd.DataFrame(metrics)\n",
    "print(\"\\nâœ… Average performance across all variables:\")\n",
    "print(result_df.mean(numeric_only=True))\n",
    "\n",
    "print(f\"\\nðŸ•’ Total runtime: {total_elapsed:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a17bb",
   "metadata": {},
   "source": [
    "## dataset:Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f70d9b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/862] Running: 0\n",
      "âœ… Done: 0 in 31.41 sec\n",
      "[2/862] Running: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m start_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     51\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_train\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    876\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[0;32m    877\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[0;32m    878\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    879\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    882\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    486\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    488\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 489\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    494\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m \n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1404\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\keras_0310_env\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# 1. Load and clean the data\n",
    "df = pd.read_csv('../DSS5104_TeamWork/data/traffic.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# 2. Normalize with Z-score\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# 3. Parameters\n",
    "seq_len = 96\n",
    "pred_len = 96\n",
    "train_ratio = 0.77\n",
    "metrics = []\n",
    "\n",
    "# Start total timer\n",
    "total_start = time.time()\n",
    "checkpoints = []\n",
    "\n",
    "# 4. Loop over each variable\n",
    "for i, col in enumerate(df_scaled.columns):\n",
    "    print(f\"[{i+1}/{len(df_scaled.columns)}] Running: {col}\")\n",
    "    series = df_scaled[col].values\n",
    "\n",
    "    # Create input/output samples\n",
    "    X, y = [], []\n",
    "    for j in range(len(series) - seq_len - pred_len):\n",
    "        X.append(series[j:j+seq_len])\n",
    "        y.append(series[j+seq_len:j+seq_len+pred_len])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # Split into train/test sets\n",
    "    train_size = int(len(X) * train_ratio)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model (1-step ahead)\n",
    "        start_train = time.time()\n",
    "        model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "        model.fit(X_train, y_train[:, 0])\n",
    "        train_time = time.time() - start_train\n",
    "\n",
    "        # Predict\n",
    "        start_pred = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        predict_time = time.time() - start_pred\n",
    "\n",
    "        y_true = y_test[:, 0]\n",
    "\n",
    "        # Evaluation metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        mspe = np.mean(np.square((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        metrics.append({\n",
    "            'Variable': col,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'MSPE (%)': mspe,\n",
    "            'Train Time (s)': train_time,\n",
    "            'Predict Time (s)': predict_time\n",
    "        })\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        checkpoints.append(elapsed)\n",
    "\n",
    "        # Print per-variable time\n",
    "        print(f\"âœ… Done: {col} in {elapsed:.2f} sec\")\n",
    "\n",
    "        # Checkpoint every 5 variables\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"ðŸ•’ Avg time per variable (last 5): {np.mean(checkpoints[-5:]):.2f} sec\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Skipped {col} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# End total timer\n",
    "total_elapsed = time.time() - total_start\n",
    "\n",
    "# 5. Show results\n",
    "result_df = pd.DataFrame(metrics)\n",
    "print(\"\\nâœ… Average performance across all variables:\")\n",
    "print(result_df.mean(numeric_only=True))\n",
    "\n",
    "print(f\"\\nðŸ•’ Total runtime: {total_elapsed:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee67488",
   "metadata": {},
   "source": [
    "## dataset:Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16a5ef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/21] Running: p (mbar)\n",
      "âœ… Done: p (mbar) in 352.15 sec\n",
      "[2/21] Running: T (degC)\n",
      "âœ… Done: T (degC) in 349.48 sec\n",
      "[3/21] Running: Tpot (K)\n",
      "âœ… Done: Tpot (K) in 520.73 sec\n",
      "[4/21] Running: Tdew (degC)\n",
      "âœ… Done: Tdew (degC) in 469.11 sec\n",
      "[5/21] Running: rh (%)\n",
      "âœ… Done: rh (%) in 573.36 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 452.96 sec\n",
      "[6/21] Running: VPmax (mbar)\n",
      "âœ… Done: VPmax (mbar) in 524.77 sec\n",
      "[7/21] Running: VPact (mbar)\n",
      "âœ… Done: VPact (mbar) in 462.84 sec\n",
      "[8/21] Running: VPdef (mbar)\n",
      "âœ… Done: VPdef (mbar) in 492.70 sec\n",
      "[9/21] Running: sh (g/kg)\n",
      "âœ… Done: sh (g/kg) in 410.50 sec\n",
      "[10/21] Running: H2OC (mmol/mol)\n",
      "âœ… Done: H2OC (mmol/mol) in 448.48 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 467.86 sec\n",
      "[11/21] Running: rho (g/m**3)\n",
      "âœ… Done: rho (g/m**3) in 756.14 sec\n",
      "[12/21] Running: wv (m/s)\n",
      "âœ… Done: wv (m/s) in 329.11 sec\n",
      "[13/21] Running: max. wv (m/s)\n",
      "âœ… Done: max. wv (m/s) in 279.26 sec\n",
      "[14/21] Running: wd (deg)\n",
      "âœ… Done: wd (deg) in 395.38 sec\n",
      "[15/21] Running: rain (mm)\n",
      "âœ… Done: rain (mm) in 40.39 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 360.06 sec\n",
      "[16/21] Running: raining (s)\n",
      "âœ… Done: raining (s) in 46.92 sec\n",
      "[17/21] Running: SWDR (W/mï¿½)\n",
      "âœ… Done: SWDR (W/mï¿½) in 343.71 sec\n",
      "[18/21] Running: PAR (ï¿½mol/mï¿½/s)\n",
      "âœ… Done: PAR (ï¿½mol/mï¿½/s) in 385.17 sec\n",
      "[19/21] Running: max. PAR (ï¿½mol/mï¿½/s)\n",
      "âœ… Done: max. PAR (ï¿½mol/mï¿½/s) in 377.35 sec\n",
      "[20/21] Running: Tlog (degC)\n",
      "âœ… Done: Tlog (degC) in 392.67 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 309.16 sec\n",
      "[21/21] Running: OT\n",
      "âœ… Done: OT in 292.03 sec\n",
      "\n",
      "âœ… Average performance across all variables:\n",
      "MSE                     0.037870\n",
      "MAE                     0.050313\n",
      "RMSE                    0.110989\n",
      "MAPE (%)               71.616824\n",
      "MSPE (%)            85039.648419\n",
      "Train Time (s)        392.447528\n",
      "Predict Time (s)        0.033358\n",
      "dtype: float64\n",
      "\n",
      "ðŸ•’ Total runtime: 8245.41 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# 1. Load and clean the data\n",
    "df = pd.read_csv('../DSS5104_TeamWork/data/weather.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# 2. Normalize with Z-score\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# 3. Parameters\n",
    "seq_len = 336\n",
    "pred_len = 96\n",
    "train_ratio = 0.77\n",
    "metrics = []\n",
    "\n",
    "# Start total timer\n",
    "total_start = time.time()\n",
    "checkpoints = []\n",
    "\n",
    "# 4. Loop over each variable\n",
    "for i, col in enumerate(df_scaled.columns):\n",
    "    print(f\"[{i+1}/{len(df_scaled.columns)}] Running: {col}\")\n",
    "    series = df_scaled[col].values\n",
    "\n",
    "    # Create input/output samples\n",
    "    X, y = [], []\n",
    "    for j in range(len(series) - seq_len - pred_len):\n",
    "        X.append(series[j:j+seq_len])\n",
    "        y.append(series[j+seq_len:j+seq_len+pred_len])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # Split into train/test sets\n",
    "    train_size = int(len(X) * train_ratio)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model (1-step ahead)\n",
    "        start_train = time.time()\n",
    "        model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "        model.fit(X_train, y_train[:, 0])\n",
    "        train_time = time.time() - start_train\n",
    "\n",
    "        # Predict\n",
    "        start_pred = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        predict_time = time.time() - start_pred\n",
    "\n",
    "        y_true = y_test[:, 0]\n",
    "\n",
    "        # Evaluation metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        mspe = np.mean(np.square((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        metrics.append({\n",
    "            'Variable': col,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'MSPE (%)': mspe,\n",
    "            'Train Time (s)': train_time,\n",
    "            'Predict Time (s)': predict_time\n",
    "        })\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        checkpoints.append(elapsed)\n",
    "\n",
    "        # Print per-variable time\n",
    "        print(f\"âœ… Done: {col} in {elapsed:.2f} sec\")\n",
    "\n",
    "        # Checkpoint every 5 variables\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"ðŸ•’ Avg time per variable (last 5): {np.mean(checkpoints[-5:]):.2f} sec\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Skipped {col} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# End total timer\n",
    "total_elapsed = time.time() - total_start\n",
    "\n",
    "# 5. Show results\n",
    "result_df = pd.DataFrame(metrics)\n",
    "print(\"\\nâœ… Average performance across all variables:\")\n",
    "print(result_df.mean(numeric_only=True))\n",
    "\n",
    "print(f\"\\nðŸ•’ Total runtime: {total_elapsed:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393e80b",
   "metadata": {},
   "source": [
    "## dataset:Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a93b08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/8] Running: 0\n",
      "âœ… Done: 0 in 18.20 sec\n",
      "[2/8] Running: 1\n",
      "âœ… Done: 1 in 18.43 sec\n",
      "[3/8] Running: 2\n",
      "âœ… Done: 2 in 17.95 sec\n",
      "[4/8] Running: 3\n",
      "âœ… Done: 3 in 18.09 sec\n",
      "[5/8] Running: 4\n",
      "âœ… Done: 4 in 10.55 sec\n",
      "ðŸ•’ Avg time per variable (last 5): 16.64 sec\n",
      "[6/8] Running: 5\n",
      "âœ… Done: 5 in 15.72 sec\n",
      "[7/8] Running: 6\n",
      "âœ… Done: 6 in 16.51 sec\n",
      "[8/8] Running: OT\n",
      "âœ… Done: OT in 17.38 sec\n",
      "\n",
      "âœ… Average performance across all variables:\n",
      "MSE                  0.152648\n",
      "MAE                  0.253750\n",
      "RMSE                 0.319821\n",
      "MAPE (%)            22.788771\n",
      "MSPE (%)            78.128174\n",
      "Train Time (s)      16.599721\n",
      "Predict Time (s)     0.001863\n",
      "dtype: float64\n",
      "\n",
      "ðŸ•’ Total runtime: 132.99 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# 1. Load and clean the data\n",
    "df = pd.read_csv('../DSS5104_TeamWork/data/exchange_rate.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# 2. Normalize with Z-score\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "# 3. Parameters\n",
    "seq_len = 96\n",
    "pred_len = 720\n",
    "train_ratio = 0.77\n",
    "metrics = []\n",
    "\n",
    "# Start total timer\n",
    "total_start = time.time()\n",
    "checkpoints = []\n",
    "\n",
    "# 4. Loop over each variable\n",
    "for i, col in enumerate(df_scaled.columns):\n",
    "    print(f\"[{i+1}/{len(df_scaled.columns)}] Running: {col}\")\n",
    "    series = df_scaled[col].values\n",
    "\n",
    "    # Create input/output samples\n",
    "    X, y = [], []\n",
    "    for j in range(len(series) - seq_len - pred_len):\n",
    "        X.append(series[j:j+seq_len])\n",
    "        y.append(series[j+seq_len:j+seq_len+pred_len])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # Split into train/test sets\n",
    "    train_size = int(len(X) * train_ratio)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train the model (1-step ahead)\n",
    "        start_train = time.time()\n",
    "        model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "        model.fit(X_train, y_train[:, 0])\n",
    "        train_time = time.time() - start_train\n",
    "\n",
    "        # Predict\n",
    "        start_pred = time.time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        predict_time = time.time() - start_pred\n",
    "\n",
    "        y_true = y_test[:, 0]\n",
    "\n",
    "        # Evaluation metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        mspe = np.mean(np.square((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "        metrics.append({\n",
    "            'Variable': col,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE (%)': mape,\n",
    "            'MSPE (%)': mspe,\n",
    "            'Train Time (s)': train_time,\n",
    "            'Predict Time (s)': predict_time\n",
    "        })\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        checkpoints.append(elapsed)\n",
    "\n",
    "        # Print per-variable time\n",
    "        print(f\"âœ… Done: {col} in {elapsed:.2f} sec\")\n",
    "\n",
    "        # Checkpoint every 5 variables\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"ðŸ•’ Avg time per variable (last 5): {np.mean(checkpoints[-5:]):.2f} sec\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Skipped {col} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# End total timer\n",
    "total_elapsed = time.time() - total_start\n",
    "\n",
    "# 5. Show results\n",
    "result_df = pd.DataFrame(metrics)\n",
    "print(\"\\nâœ… Average performance across all variables:\")\n",
    "print(result_df.mean(numeric_only=True))\n",
    "\n",
    "print(f\"\\nðŸ•’ Total runtime: {total_elapsed:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_0310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
